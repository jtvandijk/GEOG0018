[
  {
    "objectID": "00-index.html",
    "href": "00-index.html",
    "title": "Methods in Human Geography",
    "section": "",
    "text": "Welcome to the second half of Methods in Human Geography. In this part of the module, you will delve into essential statistical analysis techniques and gain a basic foundation in creating thematic maps. We will be using R and the RStudio environment for statistical analysis, and the open-source programme QGIS for handling spatial data.\n\n\n\nMoodle is the central point of contact for GEOG0018 and it is where all important information will be communicated such as key module and assessment information. This workbook provides links to all reading materials and includes the content for all computer tutorials.\n\n\n\nThe topics covered over the next five weeks are:\n\n\n\nWeek\nSection\nTopic\n\n\n\n\n1\nGetting Started\nGetting Started\n\n\n1\nR for Data Analysis\nR for Data Analysis\n\n\n2\nStatistical Analysis\nStatistical Analysis I\n\n\n3\nStatistical Analysis\nStatistical Analysis II\n\n\n4\nSpatial Analysis\nSpatial Analysis I\n\n\n5\nSpatial Analysis\nSpatial Analysis II\n\n\n\n\n\n\n\n\n\n\n\n\nThis year’s version features the following minor updates:\n\nPartial rewrite of W04s data cleaning material.\nMinor changes, typos.\n\n\n\n\n\n\n\nThis workbook is created using the Quarto publishing system. Elements of this workbook and module material are partially based on and modified from:\n\nThe GEOG0018: Methods in Human Geography 2024-2025 workbook by Justin van Dijk\nThe GEOG0030: Geocomputation 2023-2024 workbook by Justin van Dijk\nPrevious GEOG018: Methods in Human Geography content created by Rory Coulter\nPrevious POLS0008: Introduction to Quantitative Research Methods content created by Stephen Jivraj\n\nThe datasets used in this workbook contain:\n\nData from Office for National Statistics licensed under the Open Government Licence v.3.0\nOS data © Crown copyright and database right [2024]"
  },
  {
    "objectID": "00-index.html#welcome",
    "href": "00-index.html#welcome",
    "title": "Methods in Human Geography",
    "section": "",
    "text": "Welcome to the second half of Methods in Human Geography. In this part of the module, you will delve into essential statistical analysis techniques and gain a basic foundation in creating thematic maps. We will be using R and the RStudio environment for statistical analysis, and the open-source programme QGIS for handling spatial data."
  },
  {
    "objectID": "00-index.html#moodle",
    "href": "00-index.html#moodle",
    "title": "Methods in Human Geography",
    "section": "",
    "text": "Moodle is the central point of contact for GEOG0018 and it is where all important information will be communicated such as key module and assessment information. This workbook provides links to all reading materials and includes the content for all computer tutorials."
  },
  {
    "objectID": "00-index.html#module-overview",
    "href": "00-index.html#module-overview",
    "title": "Methods in Human Geography",
    "section": "",
    "text": "The topics covered over the next five weeks are:\n\n\n\nWeek\nSection\nTopic\n\n\n\n\n1\nGetting Started\nGetting Started\n\n\n1\nR for Data Analysis\nR for Data Analysis\n\n\n2\nStatistical Analysis\nStatistical Analysis I\n\n\n3\nStatistical Analysis\nStatistical Analysis II\n\n\n4\nSpatial Analysis\nSpatial Analysis I\n\n\n5\nSpatial Analysis\nSpatial Analysis II"
  },
  {
    "objectID": "00-index.html#major-updates",
    "href": "00-index.html#major-updates",
    "title": "Methods in Human Geography",
    "section": "",
    "text": "This year’s version features the following minor updates:\n\nPartial rewrite of W04s data cleaning material.\nMinor changes, typos."
  },
  {
    "objectID": "00-index.html#acknowledgements",
    "href": "00-index.html#acknowledgements",
    "title": "Methods in Human Geography",
    "section": "",
    "text": "This workbook is created using the Quarto publishing system. Elements of this workbook and module material are partially based on and modified from:\n\nThe GEOG0018: Methods in Human Geography 2024-2025 workbook by Justin van Dijk\nThe GEOG0030: Geocomputation 2023-2024 workbook by Justin van Dijk\nPrevious GEOG018: Methods in Human Geography content created by Rory Coulter\nPrevious POLS0008: Introduction to Quantitative Research Methods content created by Stephen Jivraj\n\nThe datasets used in this workbook contain:\n\nData from Office for National Statistics licensed under the Open Government Licence v.3.0\nOS data © Crown copyright and database right [2024]"
  },
  {
    "objectID": "01-recap.html",
    "href": "01-recap.html",
    "title": "1 R for Data Analysis",
    "section": "",
    "text": "This week, we will start easy with a refresher on how to use R and RStudio for working with quantitative data. We will revisit some key concepts introduced in last year, but from next week we will build on these foundations by introducing more advanced statistical techniques.\n\n\nYou can download the slides of this week’s lecture here: [Link].\n\n\n\n\n\n\nBrundson, C. and Comber, A. 2020. Opening practice: Supporting reproducibility and critical spatial data science. Journal of Geographical Systems 23: 477–496. [Link]\nFranklin, R. 2023. Quantitative methods III: Strength in numbers? Progress in Human Geography. Online First. [Link].\n\n\n\n\n\nField, A. Discovering Statistics using R, Chapter 1: Why is my evil lecturer forcing me to learn statistics?, pp. 1-31. [Link]\nHadley, W. 2017. R for Data Science. Chapter 3: Workflow: basics. [Link]\nMiller, H. and Goodchild, M. 2015. Data-driven geography. GeoJournal 80: 449–461. [Link]\n\n\n\n\n\nIn RStudio, scripts allow us to build and save code that can be run repeatedly. We can organise these scripts into RStudio projects, which consolidate all files related to an analysis such as input data, R scripts, results, figures, and more. This organisation helps keep track of all data, input, and output, while enabling us to create standalone scripts for each part of our analysis. Additionally, it simplifies managing directories and filepaths.\nNavigate to File -&gt; New Project -&gt; New Directory. Choose a directory name, such as GEOG0018, and select the location on your computer where you want to save this project by clicking on Browse….\n\n\n\n\n\n\nEnsure you select an appropriate folder to store your GEOG0018 project. For example, you might use your Methods in Human Geography folder, if you have one, or another location within your Documents directory on your computer.\n\n\n\n\n\n\n\n\n\nPlease ensure that folder names and file names do not contain spaces or special characters such as * . \" / \\ [ ] : ; | = , &lt; ? &gt; & $ # ! ' { } ( ). Different operating systems and programming languages deal differently with spaces and special characters and as such including these in your folder names and file names can cause many problems and unexpected errors. As an alternative to using white space you can use an underscore (_) or hyphen (-) if you like.\n\n\n\nClick on Create Project. You should now see your main RStudio window switch to this new project and when you check your files pane, you should see a new R Project called GEOG0018.\nWith the basics covered, let us dive into loading a real dataset, perform some data cleaning, and conducting some exploratory data analysis on the distribution of age groups in Camden. The data covers all usual residents across London, as recorded in the 2021 Census for England and Wales, aggregated at the Lower Super Output Area (LSOA) level.\n\n\n\n\n\n\nAn LSOA is a geographic unit used in the UK for statistical analysis. It typically represents small areas with populations of around 1,000 to 3,000 people and is designed to ensure consistent data reporting. LSOAs are commonly used to report on census data, deprivation indices, and other socio-economic statistics.\n\n\n\nThe dataset has been extracted using the Custom Dataset Tool, and you can download the file via the link provided below. Save the files in your project folder under data.\n\n\n\n\n\n\nYou will have to create a folder named data inside the folder where you saved your RStudio Project.\n\n\n\n\n\n\nFile\nType\nLink\n\n\n\n\nLondon LSOA Census 2021 Age Groups\ncsv\nDownload\n\n\n\n\n\n\n\n\n\nTo download a csv file that is hosted on GitHub, click on the Download raw file button on the top right of your screen and it should download directly to your computer.\n\n\n\nTo get started, let us create our first script. File -&gt; New File -&gt; R Script. Save your script as w06-age-group-analysis.r.\nWe will start by loading the libraries that we will need:\n\n\n\nR code\n\n# load libraries\nlibrary(tidyverse)\nlibrary(janitor)\n\n\n\n\n\n\n\n\nIn RStudio, there are two primary ways to run a script: all at once or by executing individual lines or chunks of code. As a beginner, it is often beneficial to use the line-by-line approach, as it allows you to test your code interactively and catch errors early.\nTo run line-by-line:\n\nBy clicking: Highlight the line or chunk of code you want to run, then go to Code and select Run selected lines.\nBy key commands: Highlight the code, then press Ctl (or Cmd on Mac) + Return.\n\nTo run the whole script:\n\nBy clicking: In the scripting window, click Run in the top-right corner and choose Run All.\nBy key commands: Press Option + Ctrl (or Cmd) + R.\n\nIf a script gets stuck or you realise there is an error in your code, you may need to interrupt R. To do this, go to Session -&gt; Interrupt R. If the interruption does not work, you might need to terminate and restart R.\n\n\n\n\n\nNext, we can load the London-LSOA-AgeGroup.csv file into R.\n\n\n\nR code\n\n# load age data\nlsoa_age &lt;- read_csv(\"data/London-LSOA-AgeGroup.csv\")\n\n\nRows: 24970 Columns: 5\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (3): Lower layer Super Output Areas Code, Lower layer Super Output Areas...\ndbl (2): Age (5 categories) Code, Observation\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\n\n\n\n\n\nIf using a Windows machine, you may need to substitute your forward-slashes (/) with two backslashes (\\\\) whenever you are dealing with file paths.\n\n\n\nLet us have a look at the dataframe:\n\n\n\nR code\n\n# inspect number of columns\nncol(lsoa_age)\n\n\n[1] 5\n\n# inspect number of rows\nnrow(lsoa_age)\n\n[1] 24970\n\n# inspect data\nhead(lsoa_age)\n\n# A tibble: 6 × 5\n  Lower layer Super Output Areas…¹ Lower layer Super Ou…² Age (5 categories) C…³\n  &lt;chr&gt;                            &lt;chr&gt;                                   &lt;dbl&gt;\n1 E01000001                        City of London 001A                         1\n2 E01000001                        City of London 001A                         2\n3 E01000001                        City of London 001A                         3\n4 E01000001                        City of London 001A                         4\n5 E01000001                        City of London 001A                         5\n6 E01000002                        City of London 001B                         1\n# ℹ abbreviated names: ¹​`Lower layer Super Output Areas Code`,\n#   ²​`Lower layer Super Output Areas`, ³​`Age (5 categories) Code`\n# ℹ 2 more variables: `Age (5 categories)` &lt;chr&gt;, Observation &lt;dbl&gt;\n\n# inspect column names\nnames(lsoa_age)\n\n[1] \"Lower layer Super Output Areas Code\" \"Lower layer Super Output Areas\"     \n[3] \"Age (5 categories) Code\"             \"Age (5 categories)\"                 \n[5] \"Observation\"                        \n\n\n\n\n\n\n\n\nYou can further inspect the dataset using the View() function.\n\n\n\nTo access specific columns or rows in a dataframe, we can use indexing. Indexing refers to the numbering assigned to each element within a data structure, allowing us to precisely select and manipulate data.\nTo access the first row of a dataframe, you would use dataframe[1, ], and to access the first column, you would use dataframe[, 1]. The comma separates the row and column indices, with the absence of a number indicating all rows or columns.\n\n\n\n\n\n\nIn R, indexing begins at 1, meaning that the first element of any data structure is accessed with the index [1]. This is different from many other programming languages, such as Python or Java, where indexing typically starts at 0.\n\n\n\n\n\n\nR code\n\n# index columns\nlsoa_age[, 1]\n\n\n# A tibble: 24,970 × 1\n   `Lower layer Super Output Areas Code`\n   &lt;chr&gt;                                \n 1 E01000001                            \n 2 E01000001                            \n 3 E01000001                            \n 4 E01000001                            \n 5 E01000001                            \n 6 E01000002                            \n 7 E01000002                            \n 8 E01000002                            \n 9 E01000002                            \n10 E01000002                            \n# ℹ 24,960 more rows\n\n# index rows\nlsoa_age[1, ]\n\n# A tibble: 1 × 5\n  Lower layer Super Output Areas…¹ Lower layer Super Ou…² Age (5 categories) C…³\n  &lt;chr&gt;                            &lt;chr&gt;                                   &lt;dbl&gt;\n1 E01000001                        City of London 001A                         1\n# ℹ abbreviated names: ¹​`Lower layer Super Output Areas Code`,\n#   ²​`Lower layer Super Output Areas`, ³​`Age (5 categories) Code`\n# ℹ 2 more variables: `Age (5 categories)` &lt;chr&gt;, Observation &lt;dbl&gt;\n\n# index cell\nlsoa_age[1, 1]\n\n# A tibble: 1 × 1\n  `Lower layer Super Output Areas Code`\n  &lt;chr&gt;                                \n1 E01000001                            \n\n\n\n\n\n\n\n\nAlternatively, you can access the data within individual columns by referring to their names using the $ operator. This allows you to easily extract and work with a specific column without needing to know its position in the dataframe. For example, if your dataframe is named dataframe and you want to access a column named age, you would use dataframe$age. This method is especially useful when your data has many columns or when the column positions may change.\n\n\n\n\n\n\nBecause all the data are stored in long format, we need to transform the data to a wide format. This means that instead of having multiple rows for an LSOA showing counts for different age groups, all the information for each LSOA will be consolidated into a single row. We will further clean up the column names with the janitor package to make them easier to work with.\nWe can do this as follows:\n\n\n\nR code\n\n# clean names\nlsoa_age &lt;- lsoa_age |&gt;\n    clean_names()\n\n# pivot wider\nlsoa_age &lt;- lsoa_age |&gt;\n    pivot_wider(id_cols = c(\"lower_layer_super_output_areas_code\", \"lower_layer_super_output_areas\"),\n        names_from = \"age_5_categories\", values_from = \"observation\")\n\n# clean names\nlsoa_age &lt;- lsoa_age |&gt;\n    clean_names()\n\n\n\n\n\n\n\n\nIf your clean_names() function returns an error, it is likely due to a conflict with another library that also includes a clean_names() function. In such cases, R cannot determine which one to use. To resolve this, you can specify the library explicitly by using janitor::clean_names().\n\n\n\n\n\n\n\n\n\nThe code above uses a pipe function: |&gt;. The pipe operator allows you to pass the output of one function directly into the next, streamlining your code. While it might be a bit confusing at first, you will find that it makes your code faster to write and easier to read. More importantly, it reduces the need to create multiple intermediate variables to store outputs.\n\n\n\n\n\n\n\n\n\nYou can further inspect the results using the View() function.\n\n\n\nA common issue with data tied to spatial units, such as Lower Super Output Areas (LSOAs), often vary in size, meaning some areas will have more residents than others. This variation makes it challenging to compare absolute counts fairly across different areas. To address this, we can first sum the counts across all age groups to get the total number of residents in each LSOA, and then calculate the proportion of people within each age group by dividing the counts by the total number of people in each LSOA.\n\n\n\nR code\n\n# calculate total population\nlsoa_age &lt;- lsoa_age |&gt;\n    rowwise() |&gt;\n    mutate(lsoa_pop = sum(across(3:7)))\n\n# calculate proportions\nlsoa_age &lt;- lsoa_age |&gt;\n    mutate(across(3:7, ~./lsoa_pop))\n\n\n\n\n\n\n\n\nYou can further inspect the results using the View() function.\n\n\n\n\n\n\nNow that we have loaded and inspected our data, we can start by analysing its distribution. We will begin by examining the proportion of people aged 16 to 24 across London, using basic descriptive statistics such as the mean, median, and range. These measures will give us an idea of the central tendency and spread of the data.\n\n\n\nR code\n\n# mean\nmean(lsoa_age$aged_16_to_24_years)\n\n\n[1] 0.1101321\n\n# median\nmedian(lsoa_age$aged_16_to_24_years)\n\n[1] 0.1036368\n\n# range\nrange(lsoa_age$aged_16_to_24_years)\n\n[1] 0.03403141 0.59242902\n\n# summary\nsummary(lsoa_age$aged_16_to_24_years)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n0.03403 0.08679 0.10364 0.11013 0.12420 0.59243 \n\n\nFurther to these standard descriptive statistics, graphs provide a useful visual summary of the distribution of values on a variable. Graphs can help us assess, for instance, whether it our data are normally distributed, whether there are outlying values, and whether there are any missing values or errors we should deal with before running our analyses. Histograms are a good first step when it comes to looking at the distribution of values of a variable.\nWe can create a histogram as follows:\n\n\n\nR code\n\n# histogram\nhist(lsoa_age$aged_16_to_24_years, breaks = 30, xlab = \"Proportions\", main = \"Proportion of people aged between 16 and 24\")\n\n\n\n\n\nFigure 1: Quick histogram.\n\n\n\n\n\n\n\n\n\n\nA histogram is particularly useful for checking whether a variable is normally distributed because it visually displays the frequency of data points across different intervals. In a normal distribution, the histogram will show a symmetric bell-shaped curve, with most data points clustering around the mean and fewer points in the tails.\n\n\n\nBoxplots provide another method for examining the distribution of values in a continuous variable. The box itself represents the interquartile range (IQR), bounded by the lower quartile (Q1) and upper quartile (Q3), with the median displayed as a line inside the box. The whiskers extend 1.5 times the IQR above Q3 and below Q1, indicating the range within which most of the data lies, while any points outside this range are plotted as outliers.\nA boxplot can be created as follows:\n\n\n\nR code\n\n# boxplot\nboxplot(lsoa_age$aged_16_to_24_years, horizontal = TRUE, xlab = \"Proportions\", main = \"Proportion of people aged between 16 and 24\")\n\n\n\n\n\nFigure 2: Quick boxplot.\n\n\n\n\n\n\n\nSo far we have only looked at the dataset for London as a whole. So, let us zoom in to Camden by filtering our dataset:\n\n\n\nR code\n\n# filter out camden\nlsoa_age_camden &lt;- lsoa_age |&gt;\n    filter(str_detect(lower_layer_super_output_areas, \"Camden\"))\n\n# inspect\nhead(lsoa_age_camden)\n\n\n# A tibble: 6 × 8\n# Rowwise: \n  lower_layer_super_output_areas…¹ lower_layer_super_ou…² aged_15_years_and_un…³\n  &lt;chr&gt;                            &lt;chr&gt;                                   &lt;dbl&gt;\n1 E01000842                        Camden 011A                             0.147\n2 E01000843                        Camden 014A                             0.129\n3 E01000844                        Camden 011B                             0.191\n4 E01000845                        Camden 014B                             0.177\n5 E01000846                        Camden 014C                             0.209\n6 E01000847                        Camden 014D                             0.191\n# ℹ abbreviated names: ¹​lower_layer_super_output_areas_code,\n#   ²​lower_layer_super_output_areas, ³​aged_15_years_and_under\n# ℹ 5 more variables: aged_16_to_24_years &lt;dbl&gt;, aged_25_to_34_years &lt;dbl&gt;,\n#   aged_35_to_49_years &lt;dbl&gt;, aged_50_years_and_over &lt;dbl&gt;, lsoa_pop &lt;dbl&gt;\n\n\n\n\n\n\n\n\nThe str_detect() function searches for a substring within a larger string and returns a logical value indicating whether the searched text is present. In turn, te filter() function uses this to subset the dataset.\n\n\n\n\n\n\n\n\n\nYou can further inspect the results using the View() function.\n\n\n\nWe can save this dataset so that we can easily load it the next time we want to work with this by writing it to a csv file.\n\n\n\nR code\n\n# write data\nwrite_csv(x = lsoa_age_camden, file = \"data/Camden-LSOA-AgeGroup.csv\")\n\n\n\n\n\n\nThis concludes this week’s tutorial. Now complete the following homework tasks:\n\nFilter the London LSOAs to any of the London boroughs other than Camden.\nFor your selected borough, create either a histogram or a boxplot visualising the distribution of the proportion of people aged 25 to 34 years. Make sure to give your plot a clear title and label the axes appropriately.\nSave your visualisation as an image.\n\n\n\n\n\n\n\nPaste your visualisation output in the appendix of your assignment, include a few sentences interpreting the results.\n\n\n\n\n\n\nThis week, we revisited the basics of conducting quantitative analysis in R and RStudio, building on the material introduced last year. Now that everyone is back up to speed, next week we will move on to using R for statistical analysis. But for now, it is time to kick back and relax!"
  },
  {
    "objectID": "01-recap.html#lecture-slides",
    "href": "01-recap.html#lecture-slides",
    "title": "1 R for Data Analysis",
    "section": "",
    "text": "You can download the slides of this week’s lecture here: [Link]."
  },
  {
    "objectID": "01-recap.html#reading-list",
    "href": "01-recap.html#reading-list",
    "title": "1 R for Data Analysis",
    "section": "",
    "text": "Brundson, C. and Comber, A. 2020. Opening practice: Supporting reproducibility and critical spatial data science. Journal of Geographical Systems 23: 477–496. [Link]\nFranklin, R. 2023. Quantitative methods III: Strength in numbers? Progress in Human Geography. Online First. [Link].\n\n\n\n\n\nField, A. Discovering Statistics using R, Chapter 1: Why is my evil lecturer forcing me to learn statistics?, pp. 1-31. [Link]\nHadley, W. 2017. R for Data Science. Chapter 3: Workflow: basics. [Link]\nMiller, H. and Goodchild, M. 2015. Data-driven geography. GeoJournal 80: 449–461. [Link]"
  },
  {
    "objectID": "01-recap.html#age-groups-in-camden",
    "href": "01-recap.html#age-groups-in-camden",
    "title": "1 R for Data Analysis",
    "section": "",
    "text": "In RStudio, scripts allow us to build and save code that can be run repeatedly. We can organise these scripts into RStudio projects, which consolidate all files related to an analysis such as input data, R scripts, results, figures, and more. This organisation helps keep track of all data, input, and output, while enabling us to create standalone scripts for each part of our analysis. Additionally, it simplifies managing directories and filepaths.\nNavigate to File -&gt; New Project -&gt; New Directory. Choose a directory name, such as GEOG0018, and select the location on your computer where you want to save this project by clicking on Browse….\n\n\n\n\n\n\nEnsure you select an appropriate folder to store your GEOG0018 project. For example, you might use your Methods in Human Geography folder, if you have one, or another location within your Documents directory on your computer.\n\n\n\n\n\n\n\n\n\nPlease ensure that folder names and file names do not contain spaces or special characters such as * . \" / \\ [ ] : ; | = , &lt; ? &gt; & $ # ! ' { } ( ). Different operating systems and programming languages deal differently with spaces and special characters and as such including these in your folder names and file names can cause many problems and unexpected errors. As an alternative to using white space you can use an underscore (_) or hyphen (-) if you like.\n\n\n\nClick on Create Project. You should now see your main RStudio window switch to this new project and when you check your files pane, you should see a new R Project called GEOG0018.\nWith the basics covered, let us dive into loading a real dataset, perform some data cleaning, and conducting some exploratory data analysis on the distribution of age groups in Camden. The data covers all usual residents across London, as recorded in the 2021 Census for England and Wales, aggregated at the Lower Super Output Area (LSOA) level.\n\n\n\n\n\n\nAn LSOA is a geographic unit used in the UK for statistical analysis. It typically represents small areas with populations of around 1,000 to 3,000 people and is designed to ensure consistent data reporting. LSOAs are commonly used to report on census data, deprivation indices, and other socio-economic statistics.\n\n\n\nThe dataset has been extracted using the Custom Dataset Tool, and you can download the file via the link provided below. Save the files in your project folder under data.\n\n\n\n\n\n\nYou will have to create a folder named data inside the folder where you saved your RStudio Project.\n\n\n\n\n\n\nFile\nType\nLink\n\n\n\n\nLondon LSOA Census 2021 Age Groups\ncsv\nDownload\n\n\n\n\n\n\n\n\n\nTo download a csv file that is hosted on GitHub, click on the Download raw file button on the top right of your screen and it should download directly to your computer.\n\n\n\nTo get started, let us create our first script. File -&gt; New File -&gt; R Script. Save your script as w06-age-group-analysis.r.\nWe will start by loading the libraries that we will need:\n\n\n\nR code\n\n# load libraries\nlibrary(tidyverse)\nlibrary(janitor)\n\n\n\n\n\n\n\n\nIn RStudio, there are two primary ways to run a script: all at once or by executing individual lines or chunks of code. As a beginner, it is often beneficial to use the line-by-line approach, as it allows you to test your code interactively and catch errors early.\nTo run line-by-line:\n\nBy clicking: Highlight the line or chunk of code you want to run, then go to Code and select Run selected lines.\nBy key commands: Highlight the code, then press Ctl (or Cmd on Mac) + Return.\n\nTo run the whole script:\n\nBy clicking: In the scripting window, click Run in the top-right corner and choose Run All.\nBy key commands: Press Option + Ctrl (or Cmd) + R.\n\nIf a script gets stuck or you realise there is an error in your code, you may need to interrupt R. To do this, go to Session -&gt; Interrupt R. If the interruption does not work, you might need to terminate and restart R.\n\n\n\n\n\nNext, we can load the London-LSOA-AgeGroup.csv file into R.\n\n\n\nR code\n\n# load age data\nlsoa_age &lt;- read_csv(\"data/London-LSOA-AgeGroup.csv\")\n\n\nRows: 24970 Columns: 5\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (3): Lower layer Super Output Areas Code, Lower layer Super Output Areas...\ndbl (2): Age (5 categories) Code, Observation\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\n\n\n\n\n\nIf using a Windows machine, you may need to substitute your forward-slashes (/) with two backslashes (\\\\) whenever you are dealing with file paths.\n\n\n\nLet us have a look at the dataframe:\n\n\n\nR code\n\n# inspect number of columns\nncol(lsoa_age)\n\n\n[1] 5\n\n# inspect number of rows\nnrow(lsoa_age)\n\n[1] 24970\n\n# inspect data\nhead(lsoa_age)\n\n# A tibble: 6 × 5\n  Lower layer Super Output Areas…¹ Lower layer Super Ou…² Age (5 categories) C…³\n  &lt;chr&gt;                            &lt;chr&gt;                                   &lt;dbl&gt;\n1 E01000001                        City of London 001A                         1\n2 E01000001                        City of London 001A                         2\n3 E01000001                        City of London 001A                         3\n4 E01000001                        City of London 001A                         4\n5 E01000001                        City of London 001A                         5\n6 E01000002                        City of London 001B                         1\n# ℹ abbreviated names: ¹​`Lower layer Super Output Areas Code`,\n#   ²​`Lower layer Super Output Areas`, ³​`Age (5 categories) Code`\n# ℹ 2 more variables: `Age (5 categories)` &lt;chr&gt;, Observation &lt;dbl&gt;\n\n# inspect column names\nnames(lsoa_age)\n\n[1] \"Lower layer Super Output Areas Code\" \"Lower layer Super Output Areas\"     \n[3] \"Age (5 categories) Code\"             \"Age (5 categories)\"                 \n[5] \"Observation\"                        \n\n\n\n\n\n\n\n\nYou can further inspect the dataset using the View() function.\n\n\n\nTo access specific columns or rows in a dataframe, we can use indexing. Indexing refers to the numbering assigned to each element within a data structure, allowing us to precisely select and manipulate data.\nTo access the first row of a dataframe, you would use dataframe[1, ], and to access the first column, you would use dataframe[, 1]. The comma separates the row and column indices, with the absence of a number indicating all rows or columns.\n\n\n\n\n\n\nIn R, indexing begins at 1, meaning that the first element of any data structure is accessed with the index [1]. This is different from many other programming languages, such as Python or Java, where indexing typically starts at 0.\n\n\n\n\n\n\nR code\n\n# index columns\nlsoa_age[, 1]\n\n\n# A tibble: 24,970 × 1\n   `Lower layer Super Output Areas Code`\n   &lt;chr&gt;                                \n 1 E01000001                            \n 2 E01000001                            \n 3 E01000001                            \n 4 E01000001                            \n 5 E01000001                            \n 6 E01000002                            \n 7 E01000002                            \n 8 E01000002                            \n 9 E01000002                            \n10 E01000002                            \n# ℹ 24,960 more rows\n\n# index rows\nlsoa_age[1, ]\n\n# A tibble: 1 × 5\n  Lower layer Super Output Areas…¹ Lower layer Super Ou…² Age (5 categories) C…³\n  &lt;chr&gt;                            &lt;chr&gt;                                   &lt;dbl&gt;\n1 E01000001                        City of London 001A                         1\n# ℹ abbreviated names: ¹​`Lower layer Super Output Areas Code`,\n#   ²​`Lower layer Super Output Areas`, ³​`Age (5 categories) Code`\n# ℹ 2 more variables: `Age (5 categories)` &lt;chr&gt;, Observation &lt;dbl&gt;\n\n# index cell\nlsoa_age[1, 1]\n\n# A tibble: 1 × 1\n  `Lower layer Super Output Areas Code`\n  &lt;chr&gt;                                \n1 E01000001                            \n\n\n\n\n\n\n\n\nAlternatively, you can access the data within individual columns by referring to their names using the $ operator. This allows you to easily extract and work with a specific column without needing to know its position in the dataframe. For example, if your dataframe is named dataframe and you want to access a column named age, you would use dataframe$age. This method is especially useful when your data has many columns or when the column positions may change.\n\n\n\n\n\n\nBecause all the data are stored in long format, we need to transform the data to a wide format. This means that instead of having multiple rows for an LSOA showing counts for different age groups, all the information for each LSOA will be consolidated into a single row. We will further clean up the column names with the janitor package to make them easier to work with.\nWe can do this as follows:\n\n\n\nR code\n\n# clean names\nlsoa_age &lt;- lsoa_age |&gt;\n    clean_names()\n\n# pivot wider\nlsoa_age &lt;- lsoa_age |&gt;\n    pivot_wider(id_cols = c(\"lower_layer_super_output_areas_code\", \"lower_layer_super_output_areas\"),\n        names_from = \"age_5_categories\", values_from = \"observation\")\n\n# clean names\nlsoa_age &lt;- lsoa_age |&gt;\n    clean_names()\n\n\n\n\n\n\n\n\nIf your clean_names() function returns an error, it is likely due to a conflict with another library that also includes a clean_names() function. In such cases, R cannot determine which one to use. To resolve this, you can specify the library explicitly by using janitor::clean_names().\n\n\n\n\n\n\n\n\n\nThe code above uses a pipe function: |&gt;. The pipe operator allows you to pass the output of one function directly into the next, streamlining your code. While it might be a bit confusing at first, you will find that it makes your code faster to write and easier to read. More importantly, it reduces the need to create multiple intermediate variables to store outputs.\n\n\n\n\n\n\n\n\n\nYou can further inspect the results using the View() function.\n\n\n\nA common issue with data tied to spatial units, such as Lower Super Output Areas (LSOAs), often vary in size, meaning some areas will have more residents than others. This variation makes it challenging to compare absolute counts fairly across different areas. To address this, we can first sum the counts across all age groups to get the total number of residents in each LSOA, and then calculate the proportion of people within each age group by dividing the counts by the total number of people in each LSOA.\n\n\n\nR code\n\n# calculate total population\nlsoa_age &lt;- lsoa_age |&gt;\n    rowwise() |&gt;\n    mutate(lsoa_pop = sum(across(3:7)))\n\n# calculate proportions\nlsoa_age &lt;- lsoa_age |&gt;\n    mutate(across(3:7, ~./lsoa_pop))\n\n\n\n\n\n\n\n\nYou can further inspect the results using the View() function.\n\n\n\n\n\n\nNow that we have loaded and inspected our data, we can start by analysing its distribution. We will begin by examining the proportion of people aged 16 to 24 across London, using basic descriptive statistics such as the mean, median, and range. These measures will give us an idea of the central tendency and spread of the data.\n\n\n\nR code\n\n# mean\nmean(lsoa_age$aged_16_to_24_years)\n\n\n[1] 0.1101321\n\n# median\nmedian(lsoa_age$aged_16_to_24_years)\n\n[1] 0.1036368\n\n# range\nrange(lsoa_age$aged_16_to_24_years)\n\n[1] 0.03403141 0.59242902\n\n# summary\nsummary(lsoa_age$aged_16_to_24_years)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n0.03403 0.08679 0.10364 0.11013 0.12420 0.59243 \n\n\nFurther to these standard descriptive statistics, graphs provide a useful visual summary of the distribution of values on a variable. Graphs can help us assess, for instance, whether it our data are normally distributed, whether there are outlying values, and whether there are any missing values or errors we should deal with before running our analyses. Histograms are a good first step when it comes to looking at the distribution of values of a variable.\nWe can create a histogram as follows:\n\n\n\nR code\n\n# histogram\nhist(lsoa_age$aged_16_to_24_years, breaks = 30, xlab = \"Proportions\", main = \"Proportion of people aged between 16 and 24\")\n\n\n\n\n\nFigure 1: Quick histogram.\n\n\n\n\n\n\n\n\n\n\nA histogram is particularly useful for checking whether a variable is normally distributed because it visually displays the frequency of data points across different intervals. In a normal distribution, the histogram will show a symmetric bell-shaped curve, with most data points clustering around the mean and fewer points in the tails.\n\n\n\nBoxplots provide another method for examining the distribution of values in a continuous variable. The box itself represents the interquartile range (IQR), bounded by the lower quartile (Q1) and upper quartile (Q3), with the median displayed as a line inside the box. The whiskers extend 1.5 times the IQR above Q3 and below Q1, indicating the range within which most of the data lies, while any points outside this range are plotted as outliers.\nA boxplot can be created as follows:\n\n\n\nR code\n\n# boxplot\nboxplot(lsoa_age$aged_16_to_24_years, horizontal = TRUE, xlab = \"Proportions\", main = \"Proportion of people aged between 16 and 24\")\n\n\n\n\n\nFigure 2: Quick boxplot.\n\n\n\n\n\n\n\nSo far we have only looked at the dataset for London as a whole. So, let us zoom in to Camden by filtering our dataset:\n\n\n\nR code\n\n# filter out camden\nlsoa_age_camden &lt;- lsoa_age |&gt;\n    filter(str_detect(lower_layer_super_output_areas, \"Camden\"))\n\n# inspect\nhead(lsoa_age_camden)\n\n\n# A tibble: 6 × 8\n# Rowwise: \n  lower_layer_super_output_areas…¹ lower_layer_super_ou…² aged_15_years_and_un…³\n  &lt;chr&gt;                            &lt;chr&gt;                                   &lt;dbl&gt;\n1 E01000842                        Camden 011A                             0.147\n2 E01000843                        Camden 014A                             0.129\n3 E01000844                        Camden 011B                             0.191\n4 E01000845                        Camden 014B                             0.177\n5 E01000846                        Camden 014C                             0.209\n6 E01000847                        Camden 014D                             0.191\n# ℹ abbreviated names: ¹​lower_layer_super_output_areas_code,\n#   ²​lower_layer_super_output_areas, ³​aged_15_years_and_under\n# ℹ 5 more variables: aged_16_to_24_years &lt;dbl&gt;, aged_25_to_34_years &lt;dbl&gt;,\n#   aged_35_to_49_years &lt;dbl&gt;, aged_50_years_and_over &lt;dbl&gt;, lsoa_pop &lt;dbl&gt;\n\n\n\n\n\n\n\n\nThe str_detect() function searches for a substring within a larger string and returns a logical value indicating whether the searched text is present. In turn, te filter() function uses this to subset the dataset.\n\n\n\n\n\n\n\n\n\nYou can further inspect the results using the View() function.\n\n\n\nWe can save this dataset so that we can easily load it the next time we want to work with this by writing it to a csv file.\n\n\n\nR code\n\n# write data\nwrite_csv(x = lsoa_age_camden, file = \"data/Camden-LSOA-AgeGroup.csv\")"
  },
  {
    "objectID": "01-recap.html#homework-task",
    "href": "01-recap.html#homework-task",
    "title": "1 R for Data Analysis",
    "section": "",
    "text": "This concludes this week’s tutorial. Now complete the following homework tasks:\n\nFilter the London LSOAs to any of the London boroughs other than Camden.\nFor your selected borough, create either a histogram or a boxplot visualising the distribution of the proportion of people aged 25 to 34 years. Make sure to give your plot a clear title and label the axes appropriately.\nSave your visualisation as an image.\n\n\n\n\n\n\n\nPaste your visualisation output in the appendix of your assignment, include a few sentences interpreting the results."
  },
  {
    "objectID": "01-recap.html#before-you-leave",
    "href": "01-recap.html#before-you-leave",
    "title": "1 R for Data Analysis",
    "section": "",
    "text": "This week, we revisited the basics of conducting quantitative analysis in R and RStudio, building on the material introduced last year. Now that everyone is back up to speed, next week we will move on to using R for statistical analysis. But for now, it is time to kick back and relax!"
  },
  {
    "objectID": "03-statistics2.html",
    "href": "03-statistics2.html",
    "title": "1 Statistical Analysis II",
    "section": "",
    "text": "Last week, we explored methods for comparing groups. This week, our focus shifts to examining relationships between variables, aiming to assess the extent to which two variables covary — in other words, how changes in one variable are associated with changes in another. For example, what is the link between greenhouse gas emissions (x) and global mean temperature (y)? What effect does spending longer in education (x) have on earnings (y)? To what extent are house prices or local health outcomes (y) increased by proximity to urban greenspace (x)? To answer these sorts of questions we need techniques for unpacking relationships between variables.\n\n\nYou can download the slides of this week’s lecture here: [Link].\n\n\n\n\n\n\nField, A. Discovering Statistics using R, Chapter 6: Correlation, pp. 205-244. [Link]\nField, A. Discovering Statistics using R, Chapter 7: Regression, pp. 245-311. [Link]\n\n\n\n\n\nField, A. Discovering Statistics using R, Chapter 2: Everything you ever wanted to know about statistics (well, sort of), pp. 32-61. [Link]\n\n\n\n\n\nToday, we will investigate the political geography of England and Wales, focusing on the results of the July 2024 General Election, which was won by the Labour Party led by Keir Starmer. You will work with data extracted from two data sources: the constituency results from the election and socio-demographic information relating to age groups, economic status, and ethnic background from the 2021 Census, extracted using the Custom Dataset Tool. These datasets have been prepared and merged. Please download the file using the link below and save it in your project folder within the data directory.\n\n\n\nFile\nType\nLink\n\n\n\n\nEngland and Wales Parliamentary Constituencies GE2024\ncsv\nDownload\n\n\n\n\n\n\n\n\n\nTo download a csv file that is hosted on GitHub, click on the Download raw file button on the top right of your screen and it should download directly to your computer.\n\n\n\nTo get started, open your GEOG0018 R Project and create a new script: File -&gt; New File -&gt; R Script. Save your script as w08-election-analysis.r.\nWe will start by loading the libraries that we will need:\n\n\n\nR code\n\n# load libraries\nlibrary(tidyverse)\nlibrary(janitor)\nlibrary(descr)\nlibrary(ggcorrplot)\nlibrary(easystats)\n\n\n\n\nNow we have setup again, we can load the EW-GE2024-Constituency-Vars.csv file into R.\n\n\n\nR code\n\n# load election data\nelec_vars &lt;- read_csv(\"data/EW-GE2024-Constituency-Vars.csv\")\n\n\nRows: 575 Columns: 28\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (6): constituency_code, constituency_name, region_name, winning_party, ...\ndbl (22): eligible_voters, valid_votes, conservative_votes, labour_votes, li...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nAs always, we will start by inspecting the dataset to understand its structure and the variables it contains:\n\n\n\nR code\n\n# inspect\nhead(elec_vars)\n\n\n# A tibble: 6 × 28\n  constituency_code constituency_name  region_name winning_party eligible_voters\n  &lt;chr&gt;             &lt;chr&gt;              &lt;chr&gt;       &lt;chr&gt;                   &lt;dbl&gt;\n1 W07000081         Aberafan Maesteg   Wales       Lab                     72580\n2 E14001063         Aldershot          South East  Lab                     78553\n3 E14001064         Aldridge-Brownhil… West Midla… Con                     70268\n4 E14001065         Altrincham and Sa… North West  Lab                     74025\n5 W07000082         Alyn and Deeside   Wales       Lab                     75790\n6 E14001066         Amber Valley       East Midla… Lab                     71546\n# ℹ 23 more variables: valid_votes &lt;dbl&gt;, conservative_votes &lt;dbl&gt;,\n#   labour_votes &lt;dbl&gt;, libdem_votes &lt;dbl&gt;, conservative_vote_share &lt;dbl&gt;,\n#   labour_vote_share &lt;dbl&gt;, libdem_vote_share &lt;dbl&gt;,\n#   aged_15_years_and_under &lt;dbl&gt;, aged_16_to_24_years &lt;dbl&gt;,\n#   aged_25_to_34_years &lt;dbl&gt;, aged_35_to_49_years &lt;dbl&gt;,\n#   aged_50_years_and_over &lt;dbl&gt;, eco_not_applicable &lt;dbl&gt;,\n#   eco_active_employed &lt;dbl&gt;, eco_active_unemployed &lt;dbl&gt;, …\n\n# inspect column names\nnames(elec_vars)\n\n [1] \"constituency_code\"       \"constituency_name\"      \n [3] \"region_name\"             \"winning_party\"          \n [5] \"eligible_voters\"         \"valid_votes\"            \n [7] \"conservative_votes\"      \"labour_votes\"           \n [9] \"libdem_votes\"            \"conservative_vote_share\"\n[11] \"labour_vote_share\"       \"libdem_vote_share\"      \n[13] \"aged_15_years_and_under\" \"aged_16_to_24_years\"    \n[15] \"aged_25_to_34_years\"     \"aged_35_to_49_years\"    \n[17] \"aged_50_years_and_over\"  \"eco_not_applicable\"     \n[19] \"eco_active_employed\"     \"eco_active_unemployed\"  \n[21] \"eco_inactive\"            \"eth_asian\"              \n[23] \"eth_black\"               \"eth_mixed\"              \n[25] \"eth_white\"               \"eth_other\"              \n[27] \"pop_50plus_40percent\"    \"pop_white_90percent\"    \n\n\n\n\n\n\n\n\nYou can further inspect the dataset using the View() function.\n\n\n\n\n\n\nIn geography, we frequently work with categorical variables. One technique for assessing relationships between two categorical variables is crosstabulation. For example, if we hypothesise that older voters predominantly support the Conservative Party, we could crosstabulate the winning party against constituencies where more than 40% of the population is over 50 years old (pop_50plus_40percent).\n\n\n\nR code\n\n# crosstabulation\ncrosstab(elec_vars$winning_party, elec_vars$pop_50plus_40percent, prop.r = TRUE,\n    plot = FALSE)\n\n\n   Cell Contents \n|-------------------------|\n|                   Count | \n|             Row Percent | \n|-------------------------|\n\n================================================\n                           elec_vars$pop_50plus_40percent\nelec_vars$winning_party       no     yes   Total\n------------------------------------------------\nCon                          25      91     116 \n                           21.6%   78.4%   20.2%\n------------------------------------------------\nLab                         243     131     374 \n                           65.0%   35.0%   65.0%\n------------------------------------------------\nLD                           24      42      66 \n                           36.4%   63.6%   11.5%\n------------------------------------------------\nOther                         8      11      19 \n                           42.1%   57.9%    3.3%\n------------------------------------------------\nTotal                       300     275     575 \n================================================\n\n\nWe observe that in the 116 constituencies won by the Conservatives, 78.4% have more than 40% of their population aged over 50. In contrast, only 35% of the 374 constituencies won by the Labour Party have a similar demographic. This suggests an association between a large (&gt; 40%) proportion of individuals over 50 years old (x) and the election results (y).\nWe can use a Chi-square test to assess the strength of this association:\n\n\n\nR code\n\n# chi-square\ncrosstab(elec_vars$winning_party, elec_vars$pop_50plus_40percent, expected = TRUE,\n    plot = FALSE, chisq = TRUE)\n\n\n   Cell Contents \n|-------------------------|\n|                   Count | \n|         Expected Values | \n|-------------------------|\n\n================================================\n                           elec_vars$pop_50plus_40percent\nelec_vars$winning_party       no     yes   Total\n------------------------------------------------\nCon                           25      91     116\n                            60.5    55.5        \n------------------------------------------------\nLab                          243     131     374\n                           195.1   178.9        \n------------------------------------------------\nLD                            24      42      66\n                            34.4    31.6        \n------------------------------------------------\nOther                          8      11      19\n                             9.9     9.1        \n------------------------------------------------\nTotal                        300     275     575\n================================================\n\nStatistics for All Table Factors\n\nPearson's Chi-squared test \n------------------------------------------------------------\nChi^2 = 75.53043      d.f. = 3      p = 2.79e-16 \n\n        Minimum expected frequency: 9.086957 \n\n\n\n\n\n\n\n\nThe Chi-square test is a statistical method used to assess whether there is a significant association between two categorical variables.It compares the observed frequencies in each category to the expected frequencies, which are based on the assumption that there is no relationship between the variables (the null hypothesis). If the differences between observed and expected values are large enough, we reject the null hypothesis, indicating a significant association between the variables.\n\n\n\n\n\n\n\n\n\nThe \\(p\\)-value of the Chi-square test is &lt; 0.001, so the association is statistically significant.\n\n\n\n\n\n\n\n\n\nCrosstabulations are very useful if you have gathered your own survey data, where many variables will often be categorical.\n\n\n\n\n\n\nWith continuous data, we can explore associations more deeply. For example, we could investigate the relationship between the proportion of individuals over 50 years old in each parliamentary constituency (aged_50_years_and_over) and the proportion of votes cast for the Conservative Party (conservative_vote_share). A scatterplot is an ideal starting point for comparing two continuous variables.\nWe can create a scatterplot as follows:\n\n\n\nR code\n\n# scatterplot\nplot(elec_vars$aged_50_years_and_over, elec_vars$conservative_vote_share, xlab = \"Proportion of population over 50 years old\",\n    ylab = \"Proportion of votes for the Conservative party\")\n\n\n\n\n\nFigure 1: Quick scatterplot\n\n\n\n\nThe scatterplot supports our earlier hypothesis that the Conservative vote is higher in constituencies with a larger population of individuals over 50 years old. The next step is to quantify the strength and direction of this relationship. We can assess whether the variables are correlated using Pearson’s Correlation Coefficient, which ranges from -1 (perfect negative correlation) to 0 (no correlation) to +1 (perfect positive correlation).\n\n\n\n\n\n\nPearson’s Correlation Coefficient measures the linear relationship between two continuous variables. A coefficient close to 1 indicates a strong positive correlation, meaning that as one variable increases, the other tends to increase as well. Conversely, a coefficient close to -1 signifies a strong negative correlation, where an increase in one variable is associated with a decrease in the other. A coefficient near 0 suggests little to no linear relationship between the variables.\n\n\n\nWe can run a correlation as follows:\n\n\n\nR code\n\n# correlation\ncor.test(elec_vars$aged_50_years_and_over, elec_vars$conservative_vote_share, method = \"pearson\")\n\n\n\n    Pearson's product-moment correlation\n\ndata:  elec_vars$aged_50_years_and_over and elec_vars$conservative_vote_share\nt = 16.472, df = 573, p-value &lt; 2.2e-16\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.5086971 0.6199173\nsample estimates:\n      cor \n0.5668849 \n\n\n\n\n\n\n\n\nThe Pearson test is parametric and requires interval or ratio variables with a linear relationship. If the data are ordinal or skewed, Spearman’s rank correlation (method = 'spearman') is a better option, as it provides a non-parametric alternative.\n\n\n\nIt can be useful to look at a matrix of correlation coefficients between a selection of variables within your dataset. To generate this, we can use the cor() function and subsequently visualise the results:\n\n\n\nR code\n\n# select variables\nelec_vars_sel &lt;- elec_vars |&gt;\n    select(\"conservative_vote_share\", \"labour_vote_share\", \"libdem_vote_share\", starts_with(\"aged\"),\n        starts_with(\"eco\"), starts_with(\"eth\"))\n\n# correlation matrix\ncor_mat &lt;- cor(elec_vars_sel)\n\n# correlation plot\nggcorrplot(cor_mat, outline.col = \"#ffffff\", tl.cex = 8, legend.title = \"Correlation\")\n\n\n\n\n\nFigure 2: Correlation plot of selected variables.\n\n\n\n\n\n\n\n\n\nA Pearson’s correlation describes the strength and direction of a linear relationship, but we can also quantify the change in the dependent variable (y) for a one-unit change in the independent variable (x) with an Ordinary Least Squares (OLS) regression.\nLet us run a model with the proportion of the population that is over 50 years old as x and the Conservative vote share as y:\n\n\n\nR code\n\n# regression\nlm_voteshare &lt;- lm(conservative_vote_share ~ aged_50_years_and_over, data = elec_vars)\n\n# summary\nsummary(lm_voteshare)\n\n\n\nCall:\nlm(formula = conservative_vote_share ~ aged_50_years_and_over, \n    data = elec_vars)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.26274 -0.05384  0.00359  0.05877  0.31701 \n\nCoefficients:\n                       Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)            -0.03884    0.01772  -2.192   0.0288 *  \naged_50_years_and_over  0.74008    0.04493  16.472   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.08345 on 573 degrees of freedom\nMultiple R-squared:  0.3214,    Adjusted R-squared:  0.3202 \nF-statistic: 271.3 on 1 and 573 DF,  p-value: &lt; 2.2e-16\n\n\nThe Estimates column shows the regression coefficients: -0.039 for the intercept (where the line crosses the y-axis when x = 0) and 0.740 for aged_50_years_and_over (the slope of the line, indicating how y changes on average as x increases by 1). This means that for every percentage point increase in the share of people over 50 in a constituency, the Conservative vote share increases by 0.740.\n\n\n\n\n\n\nThe \\(R^2\\) (R-squared) of a regression model shows how well the independent variables explain the variation in the dependent variable. It ranges from 0 to 1, where 0 means the model explains none of the variation, and 1 means it explains all of it. Our model suggests that 32% of variation in the dependent variable is explained by the model. The remaining 68% is due to factors not captured by the model or random error.\n\n\n\n\n\n\nNow, let us include additional predictors in our regression model to see if we can better explain the variation in the Conservative vote share. We will include eco_active_employed (the proportion of economically active employed individuals) and eth_white (the proportion of the population that identifies as white):\n\n\n\nR code\n\n# regression\nlm_voteshare &lt;- lm(conservative_vote_share ~ aged_50_years_and_over + eco_active_employed +\n    eth_white, data = elec_vars)\n\n# summary\nsummary(lm_voteshare)\n\n\n\nCall:\nlm(formula = conservative_vote_share ~ aged_50_years_and_over + \n    eco_active_employed + eth_white, data = elec_vars)\n\nResiduals:\n      Min        1Q    Median        3Q       Max \n-0.271771 -0.045878  0.006598  0.053589  0.218631 \n\nCoefficients:\n                       Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)            -0.50862    0.04462 -11.399  &lt; 2e-16 ***\naged_50_years_and_over  1.20724    0.06972  17.315  &lt; 2e-16 ***\neco_active_employed     1.01807    0.08674  11.737  &lt; 2e-16 ***\neth_white              -0.22281    0.03096  -7.197 1.95e-12 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.07408 on 571 degrees of freedom\nMultiple R-squared:  0.4671,    Adjusted R-squared:  0.4643 \nF-statistic: 166.8 on 3 and 571 DF,  p-value: &lt; 2.2e-16\n\n\n\n\n\n\n\n\nThe model \\(R^2\\) has increased to 46%, indicating that adding both eco_active_employed and eth_white has improved our ability to predict Conservative vote share in 2024.\n\n\n\n\n\n\n\n\n\nThe coefficients in a multivariate regression model represent the predicted change in the dependent variable (y) with a one-unit increase in each independent variable (x), while holding all other variables in the model constant. This makes the coefficients partial regression coefficients, meaning they show the effect of each predictor after accounting for the influence of the others.\n\n\n\nThis is a good start, but as geographers, we should also consider whether regional patterns influence voter share. We can use a boxplot to split out the vote shares by region to see whether there is any evidence of regional differentiation:\n\n\n\nR code\n\n# change region variable order\nelec_vars &lt;- elec_vars |&gt;\n    mutate(region_name = factor(region_name, levels = c(\"London\", \"South East\", \"South West\",\n        \"North East\", \"North West\", \"East Midlands\", \"West Midlands\", \"East of England\",\n        \"Yorkshire and The Humber\", \"Wales\")))\n\n# boxplot\nboxplot(elec_vars$conservative_vote_share ~ elec_vars$region_name, xlab = \"Region\",\n    ylab = \"Conservative vote share\")\n\n\n\n\n\nFigure 3: Boxplot on the Conservative voter share by region.\n\n\n\n\nThe boxplots suggest some regional variation and so we will add the region_name variable to our regression model. To do this, we need to transform the categorical variable into dummy variables, one for each region, resulting in 10 new variables. Each observation will be coded as 1 for its corresponding region and 0 for all other regions.\n\n\n\n\n\n\nBy including all but one of the regional dummy variables in the regression model, we control for regional differences, with the omitted category serving as the reference category. R will detect that you have a categorical variable if it is stored as a character or factor and will do the dummy generation for you. The coefficients for the included dummy variables indicate how much the predicted value of the dependent variable differs for constituencies in that region compared to the reference region.\n\n\n\n\n\n\nR code\n\n# regression\nlm_voteshare &lt;- lm(conservative_vote_share ~ aged_50_years_and_over + eco_active_employed +\n    eth_white + region_name, data = elec_vars)\n\n# summary\nsummary(lm_voteshare)\n\n\n\nCall:\nlm(formula = conservative_vote_share ~ aged_50_years_and_over + \n    eco_active_employed + eth_white + region_name, data = elec_vars)\n\nResiduals:\n      Min        1Q    Median        3Q       Max \n-0.212739 -0.040914 -0.000956  0.036534  0.251386 \n\nCoefficients:\n                                     Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)                         -0.387395   0.049488  -7.828 2.48e-14 ***\naged_50_years_and_over               1.096071   0.065739  16.673  &lt; 2e-16 ***\neco_active_employed                  0.747208   0.101295   7.377 5.86e-13 ***\neth_white                           -0.183626   0.037209  -4.935 1.06e-06 ***\nregion_nameSouth East                0.056109   0.013387   4.191 3.22e-05 ***\nregion_nameSouth West                0.013635   0.015542   0.877  0.38067    \nregion_nameNorth East               -0.020671   0.020219  -1.022  0.30705    \nregion_nameNorth West               -0.039032   0.015201  -2.568  0.01049 *  \nregion_nameEast Midlands             0.046112   0.015404   2.993  0.00288 ** \nregion_nameWest Midlands             0.042555   0.014559   2.923  0.00361 ** \nregion_nameEast of England           0.059563   0.014387   4.140 4.00e-05 ***\nregion_nameYorkshire and The Humber -0.002996   0.015842  -0.189  0.85006    \nregion_nameWales                    -0.053131   0.019101  -2.782  0.00559 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.06529 on 562 degrees of freedom\nMultiple R-squared:  0.5925,    Adjusted R-squared:  0.5838 \nF-statistic:  68.1 on 12 and 562 DF,  p-value: &lt; 2.2e-16\n\n\n\n\n\n\n\n\nThe coefficient for each region represents the average difference in the predicted Conservative vote share compared to the reference region, which is London in this case.\n\n\n\n\n\n\nThe validity of a regression relies on several assumptions being met. One of the most important assumptions is that the residuals (the differences between predicted and observed values) should be independent and normally distributed. The easystats package provides tools to quickly generate relevant figures and diagnostics to help your assess your mode.\nCheck for normality of the residuals:\n\n\n\nR code\n\n# run diagnostics\nperformance::check_model(lm_voteshare, check = c(\"qq\", \"normality\"))\n\n\n\n\n\nFigure 4: Check for normality of residuals.\n\n\n\n\nAnother important assumption is homoscedasticity, which occurs when the residuals exhibit constant variance across all levels of the independent variable(s). In contrast, heteroscedasticity arises when the variance of the residuals is not constant. Linearity, of course, is another key assumption in linear regression, signifying that a straight-line relationship exists between the independent and dependent variables.\nCheck for linearity and heteroscedasticity:\n\n\n\nR code\n\n# run diagnostics\nperformance::check_model(lm_voteshare, check = c(\"linearity\", \"homogeneity\"))\n\n\n\n\n\nFigure 5: Check for linearity and homogeneity\n\n\n\n\n\n\n\n\n\n\nThe diagnostics indicate that the relationship between the independent and dependent variables might not be linear. This suggests the possibility of a curvilinear relationship, where the regression line can bend rather than being constrained to a straight line.\n\n\n\nMulticollinearity is another potential issue. Multicollinearity arises when independent variables are highly correlated with each other. This situation can complicate regression analysis, as it makes it challenging to isolate the individual effects of each correlated variable on the dependent variable. Finally, outliers can skew the results and affect the overall model fit.\n\n\n\n\n\n\nMulticollinearity can be assessed using Variance Inflation Factors (VIF). VIF measures how much the variance of an estimated regression coefficient increases when your predictors are correlated. It is calculated by fitting a regression model for each independent variable against all the other independent variables and then assessing how much the R-squared value increases. A VIF value greater than 10 typically indicates problematic multicollinearity.\n\n\n\nWe can check for influential observations and multicollinearity as follows:\n\n\n\nR code\n\n# run diagnostics\nperformance::check_model(lm_voteshare, check = c(\"vif\", \"outliers\"))\n\n\n\n\n\nFigure 6: Check for influential observations and multicollinearity.\n\n\n\n\n\n\n\n\n\n\nIf you encounter multicollinearity issues, you have several options to address them. One straightforward approach is to remove one of the correlated predictors from your model. Alternatively, you could recode one or more of the collinear variables into ordinal or categorical forms. For example, converting a continuous variable into categories such as High and Low percentages of individuals over 50 may reduce correlation and improve model performance.\n\n\n\n\n\n\n\n\nThis concludes this week’s tutorial. Now complete the following homework tasks making use of elec_vars dataset:\n\nAnalyse the association between the pop_white_90percent and winning_party variable.\nBuild a multiple regression model with the labour_vote_share as dependent variable for constituencies in England.\n\n\n\n\n\n\n\nPaste the test outputs in the appendix of your assignment, include a few sentences summarising what your model shows.\n\n\n\n\n\n\nThis week, we focused on statistically assessing associations between different variables.Because this week’s material was a little technical, we will be shifting our focus away from R next week and turn our attention to QGIS. Time to sit down and process this weeks material before some more relaxed content next week!"
  },
  {
    "objectID": "03-statistics2.html#lecture-slides",
    "href": "03-statistics2.html#lecture-slides",
    "title": "1 Statistical Analysis II",
    "section": "",
    "text": "You can download the slides of this week’s lecture here: [Link]."
  },
  {
    "objectID": "03-statistics2.html#reading-list",
    "href": "03-statistics2.html#reading-list",
    "title": "1 Statistical Analysis II",
    "section": "",
    "text": "Field, A. Discovering Statistics using R, Chapter 6: Correlation, pp. 205-244. [Link]\nField, A. Discovering Statistics using R, Chapter 7: Regression, pp. 245-311. [Link]\n\n\n\n\n\nField, A. Discovering Statistics using R, Chapter 2: Everything you ever wanted to know about statistics (well, sort of), pp. 32-61. [Link]"
  },
  {
    "objectID": "03-statistics2.html#elections-results-in-england-and-wales",
    "href": "03-statistics2.html#elections-results-in-england-and-wales",
    "title": "1 Statistical Analysis II",
    "section": "",
    "text": "Today, we will investigate the political geography of England and Wales, focusing on the results of the July 2024 General Election, which was won by the Labour Party led by Keir Starmer. You will work with data extracted from two data sources: the constituency results from the election and socio-demographic information relating to age groups, economic status, and ethnic background from the 2021 Census, extracted using the Custom Dataset Tool. These datasets have been prepared and merged. Please download the file using the link below and save it in your project folder within the data directory.\n\n\n\nFile\nType\nLink\n\n\n\n\nEngland and Wales Parliamentary Constituencies GE2024\ncsv\nDownload\n\n\n\n\n\n\n\n\n\nTo download a csv file that is hosted on GitHub, click on the Download raw file button on the top right of your screen and it should download directly to your computer.\n\n\n\nTo get started, open your GEOG0018 R Project and create a new script: File -&gt; New File -&gt; R Script. Save your script as w08-election-analysis.r.\nWe will start by loading the libraries that we will need:\n\n\n\nR code\n\n# load libraries\nlibrary(tidyverse)\nlibrary(janitor)\nlibrary(descr)\nlibrary(ggcorrplot)\nlibrary(easystats)\n\n\n\n\nNow we have setup again, we can load the EW-GE2024-Constituency-Vars.csv file into R.\n\n\n\nR code\n\n# load election data\nelec_vars &lt;- read_csv(\"data/EW-GE2024-Constituency-Vars.csv\")\n\n\nRows: 575 Columns: 28\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (6): constituency_code, constituency_name, region_name, winning_party, ...\ndbl (22): eligible_voters, valid_votes, conservative_votes, labour_votes, li...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nAs always, we will start by inspecting the dataset to understand its structure and the variables it contains:\n\n\n\nR code\n\n# inspect\nhead(elec_vars)\n\n\n# A tibble: 6 × 28\n  constituency_code constituency_name  region_name winning_party eligible_voters\n  &lt;chr&gt;             &lt;chr&gt;              &lt;chr&gt;       &lt;chr&gt;                   &lt;dbl&gt;\n1 W07000081         Aberafan Maesteg   Wales       Lab                     72580\n2 E14001063         Aldershot          South East  Lab                     78553\n3 E14001064         Aldridge-Brownhil… West Midla… Con                     70268\n4 E14001065         Altrincham and Sa… North West  Lab                     74025\n5 W07000082         Alyn and Deeside   Wales       Lab                     75790\n6 E14001066         Amber Valley       East Midla… Lab                     71546\n# ℹ 23 more variables: valid_votes &lt;dbl&gt;, conservative_votes &lt;dbl&gt;,\n#   labour_votes &lt;dbl&gt;, libdem_votes &lt;dbl&gt;, conservative_vote_share &lt;dbl&gt;,\n#   labour_vote_share &lt;dbl&gt;, libdem_vote_share &lt;dbl&gt;,\n#   aged_15_years_and_under &lt;dbl&gt;, aged_16_to_24_years &lt;dbl&gt;,\n#   aged_25_to_34_years &lt;dbl&gt;, aged_35_to_49_years &lt;dbl&gt;,\n#   aged_50_years_and_over &lt;dbl&gt;, eco_not_applicable &lt;dbl&gt;,\n#   eco_active_employed &lt;dbl&gt;, eco_active_unemployed &lt;dbl&gt;, …\n\n# inspect column names\nnames(elec_vars)\n\n [1] \"constituency_code\"       \"constituency_name\"      \n [3] \"region_name\"             \"winning_party\"          \n [5] \"eligible_voters\"         \"valid_votes\"            \n [7] \"conservative_votes\"      \"labour_votes\"           \n [9] \"libdem_votes\"            \"conservative_vote_share\"\n[11] \"labour_vote_share\"       \"libdem_vote_share\"      \n[13] \"aged_15_years_and_under\" \"aged_16_to_24_years\"    \n[15] \"aged_25_to_34_years\"     \"aged_35_to_49_years\"    \n[17] \"aged_50_years_and_over\"  \"eco_not_applicable\"     \n[19] \"eco_active_employed\"     \"eco_active_unemployed\"  \n[21] \"eco_inactive\"            \"eth_asian\"              \n[23] \"eth_black\"               \"eth_mixed\"              \n[25] \"eth_white\"               \"eth_other\"              \n[27] \"pop_50plus_40percent\"    \"pop_white_90percent\"    \n\n\n\n\n\n\n\n\nYou can further inspect the dataset using the View() function.\n\n\n\n\n\n\nIn geography, we frequently work with categorical variables. One technique for assessing relationships between two categorical variables is crosstabulation. For example, if we hypothesise that older voters predominantly support the Conservative Party, we could crosstabulate the winning party against constituencies where more than 40% of the population is over 50 years old (pop_50plus_40percent).\n\n\n\nR code\n\n# crosstabulation\ncrosstab(elec_vars$winning_party, elec_vars$pop_50plus_40percent, prop.r = TRUE,\n    plot = FALSE)\n\n\n   Cell Contents \n|-------------------------|\n|                   Count | \n|             Row Percent | \n|-------------------------|\n\n================================================\n                           elec_vars$pop_50plus_40percent\nelec_vars$winning_party       no     yes   Total\n------------------------------------------------\nCon                          25      91     116 \n                           21.6%   78.4%   20.2%\n------------------------------------------------\nLab                         243     131     374 \n                           65.0%   35.0%   65.0%\n------------------------------------------------\nLD                           24      42      66 \n                           36.4%   63.6%   11.5%\n------------------------------------------------\nOther                         8      11      19 \n                           42.1%   57.9%    3.3%\n------------------------------------------------\nTotal                       300     275     575 \n================================================\n\n\nWe observe that in the 116 constituencies won by the Conservatives, 78.4% have more than 40% of their population aged over 50. In contrast, only 35% of the 374 constituencies won by the Labour Party have a similar demographic. This suggests an association between a large (&gt; 40%) proportion of individuals over 50 years old (x) and the election results (y).\nWe can use a Chi-square test to assess the strength of this association:\n\n\n\nR code\n\n# chi-square\ncrosstab(elec_vars$winning_party, elec_vars$pop_50plus_40percent, expected = TRUE,\n    plot = FALSE, chisq = TRUE)\n\n\n   Cell Contents \n|-------------------------|\n|                   Count | \n|         Expected Values | \n|-------------------------|\n\n================================================\n                           elec_vars$pop_50plus_40percent\nelec_vars$winning_party       no     yes   Total\n------------------------------------------------\nCon                           25      91     116\n                            60.5    55.5        \n------------------------------------------------\nLab                          243     131     374\n                           195.1   178.9        \n------------------------------------------------\nLD                            24      42      66\n                            34.4    31.6        \n------------------------------------------------\nOther                          8      11      19\n                             9.9     9.1        \n------------------------------------------------\nTotal                        300     275     575\n================================================\n\nStatistics for All Table Factors\n\nPearson's Chi-squared test \n------------------------------------------------------------\nChi^2 = 75.53043      d.f. = 3      p = 2.79e-16 \n\n        Minimum expected frequency: 9.086957 \n\n\n\n\n\n\n\n\nThe Chi-square test is a statistical method used to assess whether there is a significant association between two categorical variables.It compares the observed frequencies in each category to the expected frequencies, which are based on the assumption that there is no relationship between the variables (the null hypothesis). If the differences between observed and expected values are large enough, we reject the null hypothesis, indicating a significant association between the variables.\n\n\n\n\n\n\n\n\n\nThe \\(p\\)-value of the Chi-square test is &lt; 0.001, so the association is statistically significant.\n\n\n\n\n\n\n\n\n\nCrosstabulations are very useful if you have gathered your own survey data, where many variables will often be categorical.\n\n\n\n\n\n\nWith continuous data, we can explore associations more deeply. For example, we could investigate the relationship between the proportion of individuals over 50 years old in each parliamentary constituency (aged_50_years_and_over) and the proportion of votes cast for the Conservative Party (conservative_vote_share). A scatterplot is an ideal starting point for comparing two continuous variables.\nWe can create a scatterplot as follows:\n\n\n\nR code\n\n# scatterplot\nplot(elec_vars$aged_50_years_and_over, elec_vars$conservative_vote_share, xlab = \"Proportion of population over 50 years old\",\n    ylab = \"Proportion of votes for the Conservative party\")\n\n\n\n\n\nFigure 1: Quick scatterplot\n\n\n\n\nThe scatterplot supports our earlier hypothesis that the Conservative vote is higher in constituencies with a larger population of individuals over 50 years old. The next step is to quantify the strength and direction of this relationship. We can assess whether the variables are correlated using Pearson’s Correlation Coefficient, which ranges from -1 (perfect negative correlation) to 0 (no correlation) to +1 (perfect positive correlation).\n\n\n\n\n\n\nPearson’s Correlation Coefficient measures the linear relationship between two continuous variables. A coefficient close to 1 indicates a strong positive correlation, meaning that as one variable increases, the other tends to increase as well. Conversely, a coefficient close to -1 signifies a strong negative correlation, where an increase in one variable is associated with a decrease in the other. A coefficient near 0 suggests little to no linear relationship between the variables.\n\n\n\nWe can run a correlation as follows:\n\n\n\nR code\n\n# correlation\ncor.test(elec_vars$aged_50_years_and_over, elec_vars$conservative_vote_share, method = \"pearson\")\n\n\n\n    Pearson's product-moment correlation\n\ndata:  elec_vars$aged_50_years_and_over and elec_vars$conservative_vote_share\nt = 16.472, df = 573, p-value &lt; 2.2e-16\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.5086971 0.6199173\nsample estimates:\n      cor \n0.5668849 \n\n\n\n\n\n\n\n\nThe Pearson test is parametric and requires interval or ratio variables with a linear relationship. If the data are ordinal or skewed, Spearman’s rank correlation (method = 'spearman') is a better option, as it provides a non-parametric alternative.\n\n\n\nIt can be useful to look at a matrix of correlation coefficients between a selection of variables within your dataset. To generate this, we can use the cor() function and subsequently visualise the results:\n\n\n\nR code\n\n# select variables\nelec_vars_sel &lt;- elec_vars |&gt;\n    select(\"conservative_vote_share\", \"labour_vote_share\", \"libdem_vote_share\", starts_with(\"aged\"),\n        starts_with(\"eco\"), starts_with(\"eth\"))\n\n# correlation matrix\ncor_mat &lt;- cor(elec_vars_sel)\n\n# correlation plot\nggcorrplot(cor_mat, outline.col = \"#ffffff\", tl.cex = 8, legend.title = \"Correlation\")\n\n\n\n\n\nFigure 2: Correlation plot of selected variables.\n\n\n\n\n\n\n\n\n\nA Pearson’s correlation describes the strength and direction of a linear relationship, but we can also quantify the change in the dependent variable (y) for a one-unit change in the independent variable (x) with an Ordinary Least Squares (OLS) regression.\nLet us run a model with the proportion of the population that is over 50 years old as x and the Conservative vote share as y:\n\n\n\nR code\n\n# regression\nlm_voteshare &lt;- lm(conservative_vote_share ~ aged_50_years_and_over, data = elec_vars)\n\n# summary\nsummary(lm_voteshare)\n\n\n\nCall:\nlm(formula = conservative_vote_share ~ aged_50_years_and_over, \n    data = elec_vars)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.26274 -0.05384  0.00359  0.05877  0.31701 \n\nCoefficients:\n                       Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)            -0.03884    0.01772  -2.192   0.0288 *  \naged_50_years_and_over  0.74008    0.04493  16.472   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.08345 on 573 degrees of freedom\nMultiple R-squared:  0.3214,    Adjusted R-squared:  0.3202 \nF-statistic: 271.3 on 1 and 573 DF,  p-value: &lt; 2.2e-16\n\n\nThe Estimates column shows the regression coefficients: -0.039 for the intercept (where the line crosses the y-axis when x = 0) and 0.740 for aged_50_years_and_over (the slope of the line, indicating how y changes on average as x increases by 1). This means that for every percentage point increase in the share of people over 50 in a constituency, the Conservative vote share increases by 0.740.\n\n\n\n\n\n\nThe \\(R^2\\) (R-squared) of a regression model shows how well the independent variables explain the variation in the dependent variable. It ranges from 0 to 1, where 0 means the model explains none of the variation, and 1 means it explains all of it. Our model suggests that 32% of variation in the dependent variable is explained by the model. The remaining 68% is due to factors not captured by the model or random error.\n\n\n\n\n\n\nNow, let us include additional predictors in our regression model to see if we can better explain the variation in the Conservative vote share. We will include eco_active_employed (the proportion of economically active employed individuals) and eth_white (the proportion of the population that identifies as white):\n\n\n\nR code\n\n# regression\nlm_voteshare &lt;- lm(conservative_vote_share ~ aged_50_years_and_over + eco_active_employed +\n    eth_white, data = elec_vars)\n\n# summary\nsummary(lm_voteshare)\n\n\n\nCall:\nlm(formula = conservative_vote_share ~ aged_50_years_and_over + \n    eco_active_employed + eth_white, data = elec_vars)\n\nResiduals:\n      Min        1Q    Median        3Q       Max \n-0.271771 -0.045878  0.006598  0.053589  0.218631 \n\nCoefficients:\n                       Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)            -0.50862    0.04462 -11.399  &lt; 2e-16 ***\naged_50_years_and_over  1.20724    0.06972  17.315  &lt; 2e-16 ***\neco_active_employed     1.01807    0.08674  11.737  &lt; 2e-16 ***\neth_white              -0.22281    0.03096  -7.197 1.95e-12 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.07408 on 571 degrees of freedom\nMultiple R-squared:  0.4671,    Adjusted R-squared:  0.4643 \nF-statistic: 166.8 on 3 and 571 DF,  p-value: &lt; 2.2e-16\n\n\n\n\n\n\n\n\nThe model \\(R^2\\) has increased to 46%, indicating that adding both eco_active_employed and eth_white has improved our ability to predict Conservative vote share in 2024.\n\n\n\n\n\n\n\n\n\nThe coefficients in a multivariate regression model represent the predicted change in the dependent variable (y) with a one-unit increase in each independent variable (x), while holding all other variables in the model constant. This makes the coefficients partial regression coefficients, meaning they show the effect of each predictor after accounting for the influence of the others.\n\n\n\nThis is a good start, but as geographers, we should also consider whether regional patterns influence voter share. We can use a boxplot to split out the vote shares by region to see whether there is any evidence of regional differentiation:\n\n\n\nR code\n\n# change region variable order\nelec_vars &lt;- elec_vars |&gt;\n    mutate(region_name = factor(region_name, levels = c(\"London\", \"South East\", \"South West\",\n        \"North East\", \"North West\", \"East Midlands\", \"West Midlands\", \"East of England\",\n        \"Yorkshire and The Humber\", \"Wales\")))\n\n# boxplot\nboxplot(elec_vars$conservative_vote_share ~ elec_vars$region_name, xlab = \"Region\",\n    ylab = \"Conservative vote share\")\n\n\n\n\n\nFigure 3: Boxplot on the Conservative voter share by region.\n\n\n\n\nThe boxplots suggest some regional variation and so we will add the region_name variable to our regression model. To do this, we need to transform the categorical variable into dummy variables, one for each region, resulting in 10 new variables. Each observation will be coded as 1 for its corresponding region and 0 for all other regions.\n\n\n\n\n\n\nBy including all but one of the regional dummy variables in the regression model, we control for regional differences, with the omitted category serving as the reference category. R will detect that you have a categorical variable if it is stored as a character or factor and will do the dummy generation for you. The coefficients for the included dummy variables indicate how much the predicted value of the dependent variable differs for constituencies in that region compared to the reference region.\n\n\n\n\n\n\nR code\n\n# regression\nlm_voteshare &lt;- lm(conservative_vote_share ~ aged_50_years_and_over + eco_active_employed +\n    eth_white + region_name, data = elec_vars)\n\n# summary\nsummary(lm_voteshare)\n\n\n\nCall:\nlm(formula = conservative_vote_share ~ aged_50_years_and_over + \n    eco_active_employed + eth_white + region_name, data = elec_vars)\n\nResiduals:\n      Min        1Q    Median        3Q       Max \n-0.212739 -0.040914 -0.000956  0.036534  0.251386 \n\nCoefficients:\n                                     Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)                         -0.387395   0.049488  -7.828 2.48e-14 ***\naged_50_years_and_over               1.096071   0.065739  16.673  &lt; 2e-16 ***\neco_active_employed                  0.747208   0.101295   7.377 5.86e-13 ***\neth_white                           -0.183626   0.037209  -4.935 1.06e-06 ***\nregion_nameSouth East                0.056109   0.013387   4.191 3.22e-05 ***\nregion_nameSouth West                0.013635   0.015542   0.877  0.38067    \nregion_nameNorth East               -0.020671   0.020219  -1.022  0.30705    \nregion_nameNorth West               -0.039032   0.015201  -2.568  0.01049 *  \nregion_nameEast Midlands             0.046112   0.015404   2.993  0.00288 ** \nregion_nameWest Midlands             0.042555   0.014559   2.923  0.00361 ** \nregion_nameEast of England           0.059563   0.014387   4.140 4.00e-05 ***\nregion_nameYorkshire and The Humber -0.002996   0.015842  -0.189  0.85006    \nregion_nameWales                    -0.053131   0.019101  -2.782  0.00559 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.06529 on 562 degrees of freedom\nMultiple R-squared:  0.5925,    Adjusted R-squared:  0.5838 \nF-statistic:  68.1 on 12 and 562 DF,  p-value: &lt; 2.2e-16\n\n\n\n\n\n\n\n\nThe coefficient for each region represents the average difference in the predicted Conservative vote share compared to the reference region, which is London in this case.\n\n\n\n\n\n\nThe validity of a regression relies on several assumptions being met. One of the most important assumptions is that the residuals (the differences between predicted and observed values) should be independent and normally distributed. The easystats package provides tools to quickly generate relevant figures and diagnostics to help your assess your mode.\nCheck for normality of the residuals:\n\n\n\nR code\n\n# run diagnostics\nperformance::check_model(lm_voteshare, check = c(\"qq\", \"normality\"))\n\n\n\n\n\nFigure 4: Check for normality of residuals.\n\n\n\n\nAnother important assumption is homoscedasticity, which occurs when the residuals exhibit constant variance across all levels of the independent variable(s). In contrast, heteroscedasticity arises when the variance of the residuals is not constant. Linearity, of course, is another key assumption in linear regression, signifying that a straight-line relationship exists between the independent and dependent variables.\nCheck for linearity and heteroscedasticity:\n\n\n\nR code\n\n# run diagnostics\nperformance::check_model(lm_voteshare, check = c(\"linearity\", \"homogeneity\"))\n\n\n\n\n\nFigure 5: Check for linearity and homogeneity\n\n\n\n\n\n\n\n\n\n\nThe diagnostics indicate that the relationship between the independent and dependent variables might not be linear. This suggests the possibility of a curvilinear relationship, where the regression line can bend rather than being constrained to a straight line.\n\n\n\nMulticollinearity is another potential issue. Multicollinearity arises when independent variables are highly correlated with each other. This situation can complicate regression analysis, as it makes it challenging to isolate the individual effects of each correlated variable on the dependent variable. Finally, outliers can skew the results and affect the overall model fit.\n\n\n\n\n\n\nMulticollinearity can be assessed using Variance Inflation Factors (VIF). VIF measures how much the variance of an estimated regression coefficient increases when your predictors are correlated. It is calculated by fitting a regression model for each independent variable against all the other independent variables and then assessing how much the R-squared value increases. A VIF value greater than 10 typically indicates problematic multicollinearity.\n\n\n\nWe can check for influential observations and multicollinearity as follows:\n\n\n\nR code\n\n# run diagnostics\nperformance::check_model(lm_voteshare, check = c(\"vif\", \"outliers\"))\n\n\n\n\n\nFigure 6: Check for influential observations and multicollinearity.\n\n\n\n\n\n\n\n\n\n\nIf you encounter multicollinearity issues, you have several options to address them. One straightforward approach is to remove one of the correlated predictors from your model. Alternatively, you could recode one or more of the collinear variables into ordinal or categorical forms. For example, converting a continuous variable into categories such as High and Low percentages of individuals over 50 may reduce correlation and improve model performance."
  },
  {
    "objectID": "03-statistics2.html#homework-task",
    "href": "03-statistics2.html#homework-task",
    "title": "1 Statistical Analysis II",
    "section": "",
    "text": "This concludes this week’s tutorial. Now complete the following homework tasks making use of elec_vars dataset:\n\nAnalyse the association between the pop_white_90percent and winning_party variable.\nBuild a multiple regression model with the labour_vote_share as dependent variable for constituencies in England.\n\n\n\n\n\n\n\nPaste the test outputs in the appendix of your assignment, include a few sentences summarising what your model shows."
  },
  {
    "objectID": "03-statistics2.html#before-you-leave",
    "href": "03-statistics2.html#before-you-leave",
    "title": "1 Statistical Analysis II",
    "section": "",
    "text": "This week, we focused on statistically assessing associations between different variables.Because this week’s material was a little technical, we will be shifting our focus away from R next week and turn our attention to QGIS. Time to sit down and process this weeks material before some more relaxed content next week!"
  },
  {
    "objectID": "00-installation.html",
    "href": "00-installation.html",
    "title": "Getting started",
    "section": "",
    "text": "R is a programming language originally designed for conducting statistical analysis and creating graphics. The major advantage of using R is that it can be used on any computer operating system, and is free for anyone to use and contribute to. Because of this, it has rapidly become the statistical language of choice for many academics and has a large user community with people constantly contributing new packages to carry out all manner of statistical, graphical, and geographical tasks. In this tutorial, we will guide you through installing R on your computer and review the basics of interacting with the language and its functions.\n\n\nInstalling R takes a few relatively simple steps involving two pieces of software. First there is the R programming language itself. Follow these steps to get it installed on your computer:\n\nNavigate in your browser to the download page: [Link]\nIf you use a Windows computer, click on Download R for Windows. Then click on base. Download and install R 4.5.x for Windows. If you use a Mac computer, click on Download R for macOS and download and install R-4.5.x.arm64.pkg for Apple silicon Macs or R-4.5.x.x86_64.pkg for older Intel-based Macs.\n\nThat is it! You now have successfully installed R onto your computer. To make working with the R language a little bit easier we also need to install something called an Integrated Development Environment (IDE). We will use RStudio Desktop:\n\nNavigate to the official webpage of RStudio: [Link]\nDownload and install RStudio on your computer.\n\n\n\n\n\n\n\nIn case you do not have access to a laptop or encounter issues with the installation process, RStudio is also available through Desktop@UCL Anywhere as well as that it can be accessed on UCL computers across campus.\n\n\n\nAfter this, start RStudio to see if the installation was successful. Your screen should look something like what is shown in Figure 1.\n\n\n\n\n\nFigure 1: The RStudio interface.\n\n\n\n\nThe main panes that we will be using are:\n\n\n\n\n\n\n\nWindow\nPurpose\n\n\n\n\nConsole\nWhere we write one-off code such as installing packages.\n\n\nFiles\nWhere we can see where our files are stored on our computer system.\n\n\nEnvironment\nWhere our variables or objects are kept in memory.\n\n\nPlots\nWhere the outputs of our graphs, charts and maps are shown.\n\n\n\n\n\n\nNow we have installed R and RStudio, we need to customise R. Many useful R functions come in packages, these are free libraries of code written and made available by other R users. This includes packages specifically developed for data cleaning, data wrangling, visualisation, mapping, and spatial analysis. To save us some time later on, we will install the R packages that we will need during the module in one go.\nStart RStudio, and copy and paste the following code into the console window. You can execute the code by pressing the Return button on your keyboard. Depending on your computer’s specifications and the internet connection, this may take a short while.\n\n\n\nR code\n\n# install packages\ninstall.packages(c(\"tidyverse\", \"janitor\", \"descr\", \"ggcorrplot\", \"patchwork\", \"easystats\"))\n\n\n\n\n\n\n\n\nR libraries installed through CRAN are typically pre-compiled, meaning that these packages can be easily installed on your machine without additional steps. However, when attempting to install an R library that requires compilation, additional software must be installed. For Windows users, RTools provides the necessary tools for building R packages from source. MacOS users should install Xcode as well as the GNU Fortran compiler.\n\n\n\nOnce you have installed the packages, we need to check whether we can in fact load them into R. Copy and paste the following code into the console, and execute by pressing Return on your keyboard.\n\n\n\nR code\n\n# load packages\nlibrary(tidyverse)\nlibrary(janitor)\nlibrary(descr)\nlibrary(ggcorrplot)\nlibrary(patchwork)\nlibrary(easystats)\n\n\nYou will see some information printed to your console but as long as you do not get any of the messages below, the installation was successful. If you do get any of the messages below it means that the package was not properly installed, so try to install the package in question again.\n\nError: package or namespace load failed for &lt;packagename&gt;\nError: package '&lt;packagename&gt;' could not be loaded\nError in library(&lt;packagename&gt;) : there is no package called '&lt;packagename&gt;'\n\n\n\n\n\n\n\nMany packages require additional software components, known as dependencies, to function properly. Occasionally, when you install a package, some of these dependencies are not installed automatically. When you then try to load a package, you might encounter error messages that relate to a package that you did not explicitly loaded or installed. If this is the case, it is likely due to a missing dependency. To resolve this, identify the missing dependency and install it using the command install.packages('&lt;dependencyname&gt;'). Afterwards, try loading your packages again.\n\n\n\n\n\n\nUnlike traditional statistical analysis software like Microsoft Excel or IBM SPSS Statistics, which often rely on point-and-click interfaces, R requires users to input commands to perform tasks such as loading datasets and fitting models. This command-based approach is typically done by writing scripts, which not only document your workflow but also allow for easy repetition of tasks.\nLet us begin by exploring some of R’s built-in functionality through a simple exercise: creating a few variables and performing basic mathematical operations.\n\n\n\n\n\n\nIn your RStudio console, you will notice a prompt sign &gt; on the left-hand side. This is where you can directly interact with R. If any text appears in red, it indicates an error or warning. When you see the &gt;, it means R is ready for your next command. However, if you see a +, it means you have not completed the previous line of code. This typically happens when brackets are left open or a command is not properly finished in the way R expected.\n\n\n\nAt its core, every programming language can be used as a powerful calculator. Type in 10 * 12 into the console and execute.\n\n\n\nR code\n\n# multiplication\n10 * 12\n\n\n[1] 120\n\n\nOnce you press return, you should see the answer of 120 returned below.\n\n\nInstead of using raw numbers or standalone values, it is more effective to store these values in variables, which allows for easy reference later. In R, this process is known as creating an object, and the object is stored as a variable. To assign a value to a variable, use the &lt;- syntax. Let us create two variables to experiment with this\nType in ten &lt;- 10 into the console and execute.\n\n\n\nR code\n\n# store a variable\nten &lt;- 10\n\n\nYou will see nothing is returned to the console. However, if you check your environment window, you will see that a new variable has appeared, containing the value you assigned.\nType in twelve &lt;- 12 into the console and execute.\n\n\n\nR code\n\n# store a variable\ntwelve &lt;- 12\n\n\nAgain, nothing will be returned to the console, but be sure to check your environment window for the newly created variable. We have now stored two numbers in our environment and assigned them variable names for easy reference. R stores these objects as variables in your computer’s RAM memory, allowing for quick processing. Keep in mind that without saving your environment, these variables will be lost when you close R and you would need to run your code again.\nNow that we have our variables, let us proceed with a simple multiplication. Type in ten * twelve into the console and execute.\n\n\n\nR code\n\n# using variables\nten * twelve\n\n\n[1] 120\n\n\nYou should see the output in the console of 120. While this calculation may seem trivial, it demonstrates a powerful concept: these variables can be treated just like the values they contain.\nNext, type in ten * twelve * 8 into the console and execute.\n\n\n\nR code\n\n# using variables and values\nten * twelve * 8\n\n\n[1] 960\n\n\nYou should get an answer of 960. As you can see, we can mix variables with raw values without any problems. We can also store the output of variable calculations as a new variable.\nType output &lt;- ten * twelve * 8 into the console and execute.\n\n\n\nR code\n\n# store output\noutput &lt;- ten * twelve * 8\n\n\nBecause we are storing the output of our calculation to a new variable, the answer is not returned to the screen but is kept in memory.\n\n\n\nWe can ask R to return the value of the output variable by simply typing its name into the console. You should see that it returns the same value as the earlier calculation.\n\n\n\nR code\n\n# return value\noutput\n\n\n[1] 960\n\n\n\n\n\nWe can also store variables of different data types, not just numbers but text as well.\nType in str_variable &lt;- \"Hello GEOG0018\" into the console and execute.\n\n\n\nR code\n\n# store a variable\nstr_variable &lt;- \"Hello GEOG0018\"\n\n\nWe have just stored our sentence made from a combination of characters. A variable that stores text is known as a string or character variable. These variables are always denoted by the use of single ('') or double (\"\") quotation marks.\nType in str_variable into the console and execute.\n\n\n\nR code\n\n# return variable\nstr_variable\n\n\n[1] \"Hello GEOG0018\"\n\n\nYou should see our entire sentence returned, enclosed in quotation marks (\"\").\n\n\n\nWe can call functions on our variable. For example, we can ask R to print our variable, which will give us the same output as accessing it directly via the console.\nType in print(str_variable) into the console and execute.\n\n\n\nR code\n\n# printing a variable\nprint(str_variable)\n\n\n[1] \"Hello GEOG0018\"\n\n\n\n\n\n\n\n\nYou can type ?print into the console to learn more about the print() function. This method works with any function, giving you access to its documentation. Understanding this documentation is crucial for using the function correctly and interpreting its output.\n\n\n\n\n\n\nR code\n\n# open documentation of the print function\n?print\n\n\n\n\n\n\n\n\nIn many cases, a function will take more than one argument or parameter, so it is important to know what you need to provide the function with in order for it to work. For now, we are using functions that only need one required argument although most functions will also have several optional or default parameters.\n\n\n\n\n\n\nWithin the base R language, there are various functions that have been written to help us examine and find out information about our variables. For example, we can use the typeof() function to check what data type our variable is.\nType in typeof(str_variable) into the console and execute.\n\n\n\nR code\n\n# call the typeof() function\ntypeof(str_variable)\n\n\n[1] \"character\"\n\n\nYou should see the answer: character. As evident, our str_variable is a character data type. We can try testing this out on one of our earlier variables too.\nType in typeof(ten) into the console and execute.\n\n\n\nR code\n\n# call the typeof() function\ntypeof(ten)\n\n\n[1] \"double\"\n\n\nYou should see the answer: double. Alternatively, we can check the class of a variable by using the class() function.\nType in class(str_variable) into the console and execute.\n\n\n\nR code\n\n# call the class() function\nclass(str_variable)\n\n\n[1] \"character\"\n\n\nIn this case, you will see the same result as before because, in R, both the class and type of a string are character. Other programming languages might use the term string instead, but it essentially means the same thing.\nType in class(ten) into the console and execute.\n\n\n\nR code\n\n# call the class() function\nclass(ten)\n\n\n[1] \"numeric\"\n\n\nIn this case, you will get a different result because the class of this variable is numeric. Numeric objects in R can be either doubles (decimals) or integers (whole numbers). You can test whether the ten variable is an integer by using specific functions designed for this purpose.\nType in is.integer(ten) into the console and execute.\n\n\n\nR code\n\n# call the integer() function\nis.integer(ten)\n\n\n[1] FALSE\n\n\nYou should see the result FALSE. As we know from the typeof() function, the ten variable is stored as a double, so it cannot be an integer.\n\n\n\n\n\n\nWhilst knowing how to distinguish between different data types might not seem important now, the difference betwee a double and an integer can quite easily lead to unexpected errors.\n\n\n\nWe can also check the length of our a variable.\nType in length(str_variable) into the console and execute.\n\n\n\nR code\n\n# call the length() function\nlength(str_variable)\n\n\n[1] 1\n\n\nYou should get the answer 1 because we only have one set of characters. We can also determine the length of each set of characters, which tells us the length of the string contained in the variable.\nType in nchar(str_variable) into the console and execute.\n\n\n\nR code\n\n# call the nchar() function\nnchar(str_variable)\n\n\n[1] 14\n\n\nYou should get an answer of 14.\n\n\n\nVariables are not constricted to one value, but can be combined to create larger objects. Type in two_str_variable &lt;- c(\"This is our second variable\", \"It has two parts to it\") into the console and execute.\n\n\n\nR code\n\n# store a new variable\ntwo_str_variable &lt;- c(\"This is our second string variable\", \"It has two parts to it\")\n\n\nIn this code, we have created a new variable using the c() function, which combines values into a vector or list. We provided the c() function with two sets of strings, separated by a comma and enclosed within the function’s parentheses.\nLet us now try both our length() and nchar() on our new variable and see what the results are:\n\n\n\nR code\n\n# call the length() function\nlength(two_str_variable)\n\n\n[1] 2\n\n# call the nchar() function\nnchar(two_str_variable)\n\n[1] 34 22\n\n\nYou should notice that the length() function now returned a 2 and the nchar() function returned two values: 34 and 22.\n\n\n\n\n\n\nYou may have noticed that each line of code in the examples includes a comment explaining its purpose. In R, comments are created using the hash symbol #. This symbol instructs R to ignore the commented line when executing the code. Comments are useful for understanding your code when you revisit it later or when sharing it with others.\n\n\n\nExtending the concept of multi-value objects to two dimensions results in a dataframe. A dataframe is the de facto data structure for most tabular data. We will use functions from the tidyverse library, a suite of packages to load tabular data and conduct data analysis.\nWithin the tidyverse, dataframes are referred to as tibbles. Some of the most important and useful functions come from the tidyr and dplyr packages, including:\n\n\n\n\n\n\n\n\nPackage\nFunction\nUse to\n\n\n\n\ndplyr\nselect()\nselect columns\n\n\ndplyr\nfilter()\nselect rows\n\n\ndplyr\nmutate()\ncreate, transform, or recode variables\n\n\ndplyr\nsummarise()\nsummarise data\n\n\ndplyr\ngroup_by()\ngroup data into subgroups for further processing\n\n\ntidyr\npivot_longer()\nconvert data from wide format to long format\n\n\ntidyr\npivot_wider()\nconvert long format dataset to wide format\n\n\n\n\n\n\n\n\n\nFor more information on the tidyverse you can refer to www.tidyverse.org.\n\n\n\n\n\n\n\n\n\nWe will be using these functions in the coming weeks, so ensure you are familiar with what they do and how they work.\n\n\n\n\n\n\n\nThis introductory tutorial did not cover anything new that you have not encountered in last year’s modules. However, since it has been some time, you should now have R and RStudio installed (again?) and working on your computer, or at the very least, be able to access RStudio via Desktop@UCL Anywhere with the necessary libraries installed. Now that you are up to speed, time for some real action by using R for some data analysis"
  },
  {
    "objectID": "00-installation.html#installation-of-r",
    "href": "00-installation.html#installation-of-r",
    "title": "Getting started",
    "section": "",
    "text": "Installing R takes a few relatively simple steps involving two pieces of software. First there is the R programming language itself. Follow these steps to get it installed on your computer:\n\nNavigate in your browser to the download page: [Link]\nIf you use a Windows computer, click on Download R for Windows. Then click on base. Download and install R 4.5.x for Windows. If you use a Mac computer, click on Download R for macOS and download and install R-4.5.x.arm64.pkg for Apple silicon Macs or R-4.5.x.x86_64.pkg for older Intel-based Macs.\n\nThat is it! You now have successfully installed R onto your computer. To make working with the R language a little bit easier we also need to install something called an Integrated Development Environment (IDE). We will use RStudio Desktop:\n\nNavigate to the official webpage of RStudio: [Link]\nDownload and install RStudio on your computer.\n\n\n\n\n\n\n\nIn case you do not have access to a laptop or encounter issues with the installation process, RStudio is also available through Desktop@UCL Anywhere as well as that it can be accessed on UCL computers across campus.\n\n\n\nAfter this, start RStudio to see if the installation was successful. Your screen should look something like what is shown in Figure 1.\n\n\n\n\n\nFigure 1: The RStudio interface.\n\n\n\n\nThe main panes that we will be using are:\n\n\n\n\n\n\n\nWindow\nPurpose\n\n\n\n\nConsole\nWhere we write one-off code such as installing packages.\n\n\nFiles\nWhere we can see where our files are stored on our computer system.\n\n\nEnvironment\nWhere our variables or objects are kept in memory.\n\n\nPlots\nWhere the outputs of our graphs, charts and maps are shown."
  },
  {
    "objectID": "00-installation.html#customisation-of-r",
    "href": "00-installation.html#customisation-of-r",
    "title": "Getting started",
    "section": "",
    "text": "Now we have installed R and RStudio, we need to customise R. Many useful R functions come in packages, these are free libraries of code written and made available by other R users. This includes packages specifically developed for data cleaning, data wrangling, visualisation, mapping, and spatial analysis. To save us some time later on, we will install the R packages that we will need during the module in one go.\nStart RStudio, and copy and paste the following code into the console window. You can execute the code by pressing the Return button on your keyboard. Depending on your computer’s specifications and the internet connection, this may take a short while.\n\n\n\nR code\n\n# install packages\ninstall.packages(c(\"tidyverse\", \"janitor\", \"descr\", \"ggcorrplot\", \"patchwork\", \"easystats\"))\n\n\n\n\n\n\n\n\nR libraries installed through CRAN are typically pre-compiled, meaning that these packages can be easily installed on your machine without additional steps. However, when attempting to install an R library that requires compilation, additional software must be installed. For Windows users, RTools provides the necessary tools for building R packages from source. MacOS users should install Xcode as well as the GNU Fortran compiler.\n\n\n\nOnce you have installed the packages, we need to check whether we can in fact load them into R. Copy and paste the following code into the console, and execute by pressing Return on your keyboard.\n\n\n\nR code\n\n# load packages\nlibrary(tidyverse)\nlibrary(janitor)\nlibrary(descr)\nlibrary(ggcorrplot)\nlibrary(patchwork)\nlibrary(easystats)\n\n\nYou will see some information printed to your console but as long as you do not get any of the messages below, the installation was successful. If you do get any of the messages below it means that the package was not properly installed, so try to install the package in question again.\n\nError: package or namespace load failed for &lt;packagename&gt;\nError: package '&lt;packagename&gt;' could not be loaded\nError in library(&lt;packagename&gt;) : there is no package called '&lt;packagename&gt;'\n\n\n\n\n\n\n\nMany packages require additional software components, known as dependencies, to function properly. Occasionally, when you install a package, some of these dependencies are not installed automatically. When you then try to load a package, you might encounter error messages that relate to a package that you did not explicitly loaded or installed. If this is the case, it is likely due to a missing dependency. To resolve this, identify the missing dependency and install it using the command install.packages('&lt;dependencyname&gt;'). Afterwards, try loading your packages again."
  },
  {
    "objectID": "00-installation.html#getting-started-with-r",
    "href": "00-installation.html#getting-started-with-r",
    "title": "Getting started",
    "section": "",
    "text": "Unlike traditional statistical analysis software like Microsoft Excel or IBM SPSS Statistics, which often rely on point-and-click interfaces, R requires users to input commands to perform tasks such as loading datasets and fitting models. This command-based approach is typically done by writing scripts, which not only document your workflow but also allow for easy repetition of tasks.\nLet us begin by exploring some of R’s built-in functionality through a simple exercise: creating a few variables and performing basic mathematical operations.\n\n\n\n\n\n\nIn your RStudio console, you will notice a prompt sign &gt; on the left-hand side. This is where you can directly interact with R. If any text appears in red, it indicates an error or warning. When you see the &gt;, it means R is ready for your next command. However, if you see a +, it means you have not completed the previous line of code. This typically happens when brackets are left open or a command is not properly finished in the way R expected.\n\n\n\nAt its core, every programming language can be used as a powerful calculator. Type in 10 * 12 into the console and execute.\n\n\n\nR code\n\n# multiplication\n10 * 12\n\n\n[1] 120\n\n\nOnce you press return, you should see the answer of 120 returned below.\n\n\nInstead of using raw numbers or standalone values, it is more effective to store these values in variables, which allows for easy reference later. In R, this process is known as creating an object, and the object is stored as a variable. To assign a value to a variable, use the &lt;- syntax. Let us create two variables to experiment with this\nType in ten &lt;- 10 into the console and execute.\n\n\n\nR code\n\n# store a variable\nten &lt;- 10\n\n\nYou will see nothing is returned to the console. However, if you check your environment window, you will see that a new variable has appeared, containing the value you assigned.\nType in twelve &lt;- 12 into the console and execute.\n\n\n\nR code\n\n# store a variable\ntwelve &lt;- 12\n\n\nAgain, nothing will be returned to the console, but be sure to check your environment window for the newly created variable. We have now stored two numbers in our environment and assigned them variable names for easy reference. R stores these objects as variables in your computer’s RAM memory, allowing for quick processing. Keep in mind that without saving your environment, these variables will be lost when you close R and you would need to run your code again.\nNow that we have our variables, let us proceed with a simple multiplication. Type in ten * twelve into the console and execute.\n\n\n\nR code\n\n# using variables\nten * twelve\n\n\n[1] 120\n\n\nYou should see the output in the console of 120. While this calculation may seem trivial, it demonstrates a powerful concept: these variables can be treated just like the values they contain.\nNext, type in ten * twelve * 8 into the console and execute.\n\n\n\nR code\n\n# using variables and values\nten * twelve * 8\n\n\n[1] 960\n\n\nYou should get an answer of 960. As you can see, we can mix variables with raw values without any problems. We can also store the output of variable calculations as a new variable.\nType output &lt;- ten * twelve * 8 into the console and execute.\n\n\n\nR code\n\n# store output\noutput &lt;- ten * twelve * 8\n\n\nBecause we are storing the output of our calculation to a new variable, the answer is not returned to the screen but is kept in memory.\n\n\n\nWe can ask R to return the value of the output variable by simply typing its name into the console. You should see that it returns the same value as the earlier calculation.\n\n\n\nR code\n\n# return value\noutput\n\n\n[1] 960\n\n\n\n\n\nWe can also store variables of different data types, not just numbers but text as well.\nType in str_variable &lt;- \"Hello GEOG0018\" into the console and execute.\n\n\n\nR code\n\n# store a variable\nstr_variable &lt;- \"Hello GEOG0018\"\n\n\nWe have just stored our sentence made from a combination of characters. A variable that stores text is known as a string or character variable. These variables are always denoted by the use of single ('') or double (\"\") quotation marks.\nType in str_variable into the console and execute.\n\n\n\nR code\n\n# return variable\nstr_variable\n\n\n[1] \"Hello GEOG0018\"\n\n\nYou should see our entire sentence returned, enclosed in quotation marks (\"\").\n\n\n\nWe can call functions on our variable. For example, we can ask R to print our variable, which will give us the same output as accessing it directly via the console.\nType in print(str_variable) into the console and execute.\n\n\n\nR code\n\n# printing a variable\nprint(str_variable)\n\n\n[1] \"Hello GEOG0018\"\n\n\n\n\n\n\n\n\nYou can type ?print into the console to learn more about the print() function. This method works with any function, giving you access to its documentation. Understanding this documentation is crucial for using the function correctly and interpreting its output.\n\n\n\n\n\n\nR code\n\n# open documentation of the print function\n?print\n\n\n\n\n\n\n\n\nIn many cases, a function will take more than one argument or parameter, so it is important to know what you need to provide the function with in order for it to work. For now, we are using functions that only need one required argument although most functions will also have several optional or default parameters.\n\n\n\n\n\n\nWithin the base R language, there are various functions that have been written to help us examine and find out information about our variables. For example, we can use the typeof() function to check what data type our variable is.\nType in typeof(str_variable) into the console and execute.\n\n\n\nR code\n\n# call the typeof() function\ntypeof(str_variable)\n\n\n[1] \"character\"\n\n\nYou should see the answer: character. As evident, our str_variable is a character data type. We can try testing this out on one of our earlier variables too.\nType in typeof(ten) into the console and execute.\n\n\n\nR code\n\n# call the typeof() function\ntypeof(ten)\n\n\n[1] \"double\"\n\n\nYou should see the answer: double. Alternatively, we can check the class of a variable by using the class() function.\nType in class(str_variable) into the console and execute.\n\n\n\nR code\n\n# call the class() function\nclass(str_variable)\n\n\n[1] \"character\"\n\n\nIn this case, you will see the same result as before because, in R, both the class and type of a string are character. Other programming languages might use the term string instead, but it essentially means the same thing.\nType in class(ten) into the console and execute.\n\n\n\nR code\n\n# call the class() function\nclass(ten)\n\n\n[1] \"numeric\"\n\n\nIn this case, you will get a different result because the class of this variable is numeric. Numeric objects in R can be either doubles (decimals) or integers (whole numbers). You can test whether the ten variable is an integer by using specific functions designed for this purpose.\nType in is.integer(ten) into the console and execute.\n\n\n\nR code\n\n# call the integer() function\nis.integer(ten)\n\n\n[1] FALSE\n\n\nYou should see the result FALSE. As we know from the typeof() function, the ten variable is stored as a double, so it cannot be an integer.\n\n\n\n\n\n\nWhilst knowing how to distinguish between different data types might not seem important now, the difference betwee a double and an integer can quite easily lead to unexpected errors.\n\n\n\nWe can also check the length of our a variable.\nType in length(str_variable) into the console and execute.\n\n\n\nR code\n\n# call the length() function\nlength(str_variable)\n\n\n[1] 1\n\n\nYou should get the answer 1 because we only have one set of characters. We can also determine the length of each set of characters, which tells us the length of the string contained in the variable.\nType in nchar(str_variable) into the console and execute.\n\n\n\nR code\n\n# call the nchar() function\nnchar(str_variable)\n\n\n[1] 14\n\n\nYou should get an answer of 14.\n\n\n\nVariables are not constricted to one value, but can be combined to create larger objects. Type in two_str_variable &lt;- c(\"This is our second variable\", \"It has two parts to it\") into the console and execute.\n\n\n\nR code\n\n# store a new variable\ntwo_str_variable &lt;- c(\"This is our second string variable\", \"It has two parts to it\")\n\n\nIn this code, we have created a new variable using the c() function, which combines values into a vector or list. We provided the c() function with two sets of strings, separated by a comma and enclosed within the function’s parentheses.\nLet us now try both our length() and nchar() on our new variable and see what the results are:\n\n\n\nR code\n\n# call the length() function\nlength(two_str_variable)\n\n\n[1] 2\n\n# call the nchar() function\nnchar(two_str_variable)\n\n[1] 34 22\n\n\nYou should notice that the length() function now returned a 2 and the nchar() function returned two values: 34 and 22.\n\n\n\n\n\n\nYou may have noticed that each line of code in the examples includes a comment explaining its purpose. In R, comments are created using the hash symbol #. This symbol instructs R to ignore the commented line when executing the code. Comments are useful for understanding your code when you revisit it later or when sharing it with others.\n\n\n\nExtending the concept of multi-value objects to two dimensions results in a dataframe. A dataframe is the de facto data structure for most tabular data. We will use functions from the tidyverse library, a suite of packages to load tabular data and conduct data analysis.\nWithin the tidyverse, dataframes are referred to as tibbles. Some of the most important and useful functions come from the tidyr and dplyr packages, including:\n\n\n\n\n\n\n\n\nPackage\nFunction\nUse to\n\n\n\n\ndplyr\nselect()\nselect columns\n\n\ndplyr\nfilter()\nselect rows\n\n\ndplyr\nmutate()\ncreate, transform, or recode variables\n\n\ndplyr\nsummarise()\nsummarise data\n\n\ndplyr\ngroup_by()\ngroup data into subgroups for further processing\n\n\ntidyr\npivot_longer()\nconvert data from wide format to long format\n\n\ntidyr\npivot_wider()\nconvert long format dataset to wide format\n\n\n\n\n\n\n\n\n\nFor more information on the tidyverse you can refer to www.tidyverse.org.\n\n\n\n\n\n\n\n\n\nWe will be using these functions in the coming weeks, so ensure you are familiar with what they do and how they work."
  },
  {
    "objectID": "00-installation.html#before-you-leave",
    "href": "00-installation.html#before-you-leave",
    "title": "Getting started",
    "section": "",
    "text": "This introductory tutorial did not cover anything new that you have not encountered in last year’s modules. However, since it has been some time, you should now have R and RStudio installed (again?) and working on your computer, or at the very least, be able to access RStudio via Desktop@UCL Anywhere with the necessary libraries installed. Now that you are up to speed, time for some real action by using R for some data analysis"
  },
  {
    "objectID": "02-statistics1.html",
    "href": "02-statistics1.html",
    "title": "1 Statistical Analysis I",
    "section": "",
    "text": "Geographers are often interested not only in a specific variable within an area but also in how that variable varies between different areas. Building on last week’s refresher on using R and RStudio for quantitative data analysis, this week we will focus on making inferences about different groups.\n\n\nYou can download the slides of this week’s lecture here: [Link].\n\n\n\n\n\n\nField, A. Discovering Statistics using R, Chapter 9: Comparing two means, pp. 359-397. [Link]\nField, A. Discovering Statistics using R, Chapter 10: Comparing several means: ANOVA (GLM1), pp. 398-461. [Link]\n\n\n\n\n\nField, A. Discovering Statistics using R, Chapter 15: Non-parametric tests, pp. 653-695. [Link]\n\n\n\n\n\nIn this week’s tutorial, we will explore unemployment rates in London as reported in the 2021 Census. In addition, we will use of last week’s dataset on age groups in London for the homework task. You can download both files using the links provided below. Make sure to save the files in your project folder in the data directory.\n\n\n\nFile\nType\nLink\n\n\n\n\nLondon LSOA Census 2021 Age Groups\ncsv\nDownload\n\n\nLondon LSOA Census 2021 Economic Status\ncsv\nDownload\n\n\n\n\n\n\n\n\n\nTo download a csv file that is hosted on GitHub, click on the Download raw file button on the top right of your screen and it should download directly to your computer.\n\n\n\nTo get started, open your GEOG0018 R Project and create a new script: File -&gt; New File -&gt; R Script. Save your script as w07-employment-age-analysis.r.\nWe will start by loading the libraries that we will need:\n\n\n\nR code\n\n# load libraries\nlibrary(tidyverse)\nlibrary(janitor)\n\n\n\n\nNext, we can load both the London-LSOA-AgeGroup.csv and London-LSOA-EconomicStatus.csv file into R.\n\n\n\nR code\n\n# load age data\nlsoa_age &lt;- read_csv(\"data/London-LSOA-AgeGroup.csv\")\n\n\nRows: 24970 Columns: 5\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (3): Lower layer Super Output Areas Code, Lower layer Super Output Areas...\ndbl (2): Age (5 categories) Code, Observation\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n# load eployment data\nlsoa_emp &lt;- read_csv(\"data/London-LSOA-EconomicStatus.csv\")\n\nRows: 19976 Columns: 5\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (3): Lower layer Super Output Areas Code, Lower layer Super Output Areas...\ndbl (2): Economic activity status (4 categories) Code, Observation\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nBecause we already worked with the lsoa_age dataset last week, we will shift our focus to the lsoa_emp dataset. Now we have loaded the data into R, we will start by inspecting the dataset to understand its structure and the variables it contains:\n\n\n\nR code\n\n# inspect\nhead(lsoa_emp)\n\n\n# A tibble: 6 × 5\n  Lower layer Super Output Areas…¹ Lower layer Super Ou…² Economic activity st…³\n  &lt;chr&gt;                            &lt;chr&gt;                                   &lt;dbl&gt;\n1 E01000001                        City of London 001A                        -8\n2 E01000001                        City of London 001A                         1\n3 E01000001                        City of London 001A                         2\n4 E01000001                        City of London 001A                         3\n5 E01000002                        City of London 001B                        -8\n6 E01000002                        City of London 001B                         1\n# ℹ abbreviated names: ¹​`Lower layer Super Output Areas Code`,\n#   ²​`Lower layer Super Output Areas`,\n#   ³​`Economic activity status (4 categories) Code`\n# ℹ 2 more variables: `Economic activity status (4 categories)` &lt;chr&gt;,\n#   Observation &lt;dbl&gt;\n\n# inspect column names\nnames(lsoa_emp)\n\n[1] \"Lower layer Super Output Areas Code\"         \n[2] \"Lower layer Super Output Areas\"              \n[3] \"Economic activity status (4 categories) Code\"\n[4] \"Economic activity status (4 categories)\"     \n[5] \"Observation\"                                 \n\n# inspect economic status categories\nunique(lsoa_emp$`Economic activity status (4 categories)`)\n\n[1] \"Does not apply\"                                                   \n[2] \"Economically active: In employment (including full-time students)\"\n[3] \"Economically active: Unemployed (including full-time students)\"   \n[4] \"Economically inactive\"                                            \n\n\n\n\n\n\n\n\nThe economic status categories in the lsoa_emp dataset represent the number of individuals and their economic activity status as reported in the 2021 Census. The Office for National Statistics (ONS) provides definitions for each of these statuses in their Census 2021 data dictionary.\n\n\n\n\n\n\n\n\n\nYou can further inspect the dataset using the View() function.\n\n\n\n\n\n\nBecause the lsoa_emp dataset was extracted using the Custom Dataset Tool, we need to convert the data from long to wide format. Like last week, we can do this by pivoting the table:\n\n\n\nR code\n\n# clean names\nlsoa_emp &lt;- lsoa_emp |&gt;\n    clean_names()\n\n# pivot wider\nlsoa_emp &lt;- lsoa_emp |&gt;\n    pivot_wider(id_cols = c(\"lower_layer_super_output_areas_code\", \"lower_layer_super_output_areas\"),\n        names_from = \"economic_activity_status_4_categories\", values_from = \"observation\")\n\n# clean names\nlsoa_emp &lt;- lsoa_emp |&gt;\n    clean_names()\n\n\n\n\n\n\n\n\nIf your clean_names() function returns an error, it is likely due to a conflict with another library that also includes a clean_names() function. In such cases, R cannot determine which one to use. To resolve this, you can specify the library explicitly by using janitor::clean_names().\n\n\n\nThe new column names of the lsoa_emp data are quite lengthy, so let us manually assign more concise column names for ease of use. This will help simplify our code and make it easier to reference the columns in subsequent analyses. Here is how we can do that:\n\n\n\nR code\n\n# names\nnames(lsoa_emp)[3:6] &lt;- c(\"not_applicable\", \"active_employed\", \"active_unemployed\",\n    \"inactive\")\n\n\nTo account for the non-uniformity of the areal units, we further need to convert the observations to proportions again:\n\n\n\nR code\n\n# calculate total population\nlsoa_emp &lt;- lsoa_emp |&gt;\n    rowwise() |&gt;\n    mutate(lsoa_pop = sum(across(3:6)))\n\n# calculate proportions\nlsoa_emp_prop &lt;- lsoa_emp |&gt;\n    mutate(across(3:6, ~./lsoa_pop))\n\n\n\n\n\n\n\n\nYou can further inspect the results using the View() function.\n\n\n\n\n\n\nNow that we have our data prepared, we can begin comparing different groups within it. Various statistical methods exist for this purpose, depending on the nature of the data and the number of groups involved.\nFor comparing two groups with continuous data, we can use the t-test, which assesses whether the means of the two groups are significantly different from each other. If the data does not meet the assumptions of normality, the Mann-Whitney U test serves as a non-parametric alternative, comparing the ranks of the values instead.\n\n\n\n\n\n\nParametric tests assume that the data follows a specific distribution, usually normal, and rely on parameters such as mean and variance. In contrast, non-parametric tests do not assume a particular distribution and are used for ordinal data or when assumptions of parametric tests are violated.\n\n\n\nWhen we want to compare more than two groups, we can use the Analysis of Variance (ANOVA) test. An ANOVA evaluates whether there are any statistically significant differences between the means of multiple groups. If the assumptions of ANOVA are violated, we can use the Kruskal-Wallis test, which is a non-parametric method that compares the ranks across the groups.\n\n\nTwo compare two groups with continuous data, we can run an independent samples t-test, which allows us to statistically assess whether the means of the two groups differ significantly. For instance, we might be interested in whether the proportion of the population that is unemployed in Lower Super Output Areas (LSOAs) in the London Borough of Camden is different, on average, from the proportion of the population that is unemployed in the London Borough of Sutton.\nLet us begin by creating two separate datasets to facilitate our comparison: one containing the LSOAs for Camden and containing the LSOAs for Sutton. We can do this by filtering the lsoa_emp dataset:\n\n\n\nR code\n\n# filter out camden lsoas\nlsoa_emp_camden &lt;- lsoa_emp_prop |&gt;\n    filter(str_detect(lower_layer_super_output_areas, \"Camden\"))\n\n# filter out sutton lsoas\nlsoa_emp_sutton &lt;- lsoa_emp_prop |&gt;\n    filter(str_detect(lower_layer_super_output_areas, \"Sutton\"))\n\n\nWe can use the t.test() function in R to conduct a two-tailed test, assessing whether the mean proportion of unemployed people in Camden differs from that in Sutton The null hypothesis for this test states that there is no difference in the mean proportions of unemployed individuals between the two areas. We can run the test as follows:\n\n\n\nR code\n\n# run t-test\nt.test(lsoa_emp_camden$active_unemployed, lsoa_emp_sutton$active_unemployed)\n\n\n\n    Welch Two Sample t-test\n\ndata:  lsoa_emp_camden$active_unemployed and lsoa_emp_sutton$active_unemployed\nt = 11.279, df = 188.26, p-value &lt; 2.2e-16\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n 0.01303670 0.01856346\nsample estimates:\n mean of x  mean of y \n0.04220920 0.02640912 \n\n\n\n\n\n\n\n\nWe can see that the mean in Sutton LSOAs (2.64%) is lower than in Camden LSOAs (4.22%). Since the \\(p\\)-value is &lt; 0.05 we can conclude reject the null hypothesis that means for Sutton and Camden are the same, and accept the alternative hypothesis.\n\n\n\nThe non-parametric alternative to the t-test is the Mann-Whitney U test (also known as the Wilcoxon rank-sum test). This test is used to compare differences between two independent groups when the assumptions of the t-test (such as normality of the data) are not met.\n\n\n\n\n\n\nThe Mann-Whitney U test evaluates whether the distributions of the two groups differ by ranking all the data points and then comparing the sums of these ranks, making it suitable for ordinal data or non-normally distributed continuous data.\n\n\n\nIf we think our data violates any of the assumption underlying the t-test, we can run the Mann-Whitney U (Wilcoxon rank-sum) test instead:\n\n\n\nR code\n\n# run mann-whitney\nwilcox.test(lsoa_emp_camden$active_unemployed, lsoa_emp_sutton$active_unemployed)\n\n\n\n    Wilcoxon rank sum test with continuity correction\n\ndata:  lsoa_emp_camden$active_unemployed and lsoa_emp_sutton$active_unemployed\nW = 13295, p-value &lt; 2.2e-16\nalternative hypothesis: true location shift is not equal to 0\n\n\nAlso the results of the Mann-Whitney U test suggest that the difference in means in these two London boroughs is statistically significant.\n\n\n\n\n\n\nSince the \\(p\\)-value is &lt; 0.05 we can reject the null hypothesis that means for Sutton and Camden are the same, and accept the alternative hypothesis.\n\n\n\n\n\n\n\n\n\nTo determine if a variable is normally distributed, visual inspection can be helpful, but the Shapiro-Wilk test (shapiro.test()) offers a more formal statistical assessment. This test compares the distribution of your variable to a normal distribution, with the null hypothesis stating that they are the same. If the test returns a small \\(p\\)-value (typically &lt; 0.05), you can conclude that the variable is not normally distributed. However, it is important to note that the test is significantly influenced by sample size; in large datasets, even minor deviations from normality can yield a low \\(p\\)-value. The Shapiro-Wilk test should therefore not be considered definitive and should be used alongside visual assessment of the data.\n\n\n\n\n\n\nIf we want to compare more than two groups, we can use an Analysis of Variance (ANOVA) test. An ANOVA is a statistical method used to compare the means of three or more groups to determine if at least one group mean significantly differs from the others. For instance, if we extend our analysis from Camden and Sutton to include the London Borough of Hammersmith and Fulham, we can test for differences in means among these three areas.\n\n\n\n\n\n\nThe null hypothesis of the ANOVA is that all group means are equal, while the alternative hypothesis posits that at least one group mean is different. ANOVA assesses the variance within groups and between groups to calculate the F-statistic, which is the ratio of the variance between the groups to the variance within the groups. A higher F-statistic indicates that group means are not all the same. The \\(p\\)-value derived from the F-statistic tells us whether we can reject the null hypothesis.\n\n\n\nTo run an ANOVA in R, we first need to ensure that all the groups we are comparing are contained within the same dataframe. We need a dataframe where one column represents the dependent variable (e.g. unemployment rates) and another column represents the independent grouping variable (e.g. borough names).\nWe can do this by filtering our LSOA dataframe to include only those LSOAs that fall within Camden, Sutton, and Hammersmith and Fulham. To create the grouping variable, we need to extract the borough name from the lower_layer_super_output_areas column. We can do this by creating a new variable that excludes the last five characters of the lower_layer_super_output_areas variable.\n\n\n\nR code\n\n# filter\nlsoa_subset &lt;- lsoa_emp_prop |&gt;\n    filter(str_detect(lower_layer_super_output_areas, \"Camden\") | str_detect(lower_layer_super_output_areas,\n        \"Sutton\") | str_detect(lower_layer_super_output_areas, \"Hammersmith and Fulham\"))\n\n# extract borough\nlsoa_subset &lt;- lsoa_subset |&gt;\n    mutate(borough_name = substr(lower_layer_super_output_areas, 1, nchar(lower_layer_super_output_areas) -\n        5))\n\n\n\n\n\n\n\n\nYou can further inspect the results using the View() function.\n\n\n\nWe can now run the ANOVA:\n\n\n\nR code\n\n# run anova\nanova_result &lt;- aov(active_unemployed ~ borough_name, data = lsoa_subset)\n\n# summary\nsummary(anova_result)\n\n\n              Df  Sum Sq  Mean Sq F value Pr(&gt;F)    \nborough_name   2 0.01968 0.009842   75.31 &lt;2e-16 ***\nResiduals    365 0.04770 0.000131                   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\n\n\n\n\nThe ANOVA output shows the F-statistic with a value 75.31, with a \\(p\\)-value &lt; 0.001. We can conclude that there are significant differences in unemployment rates between the boroughs. This means we reject the null hypothesis that all group means are equal.\n\n\n\nWhat this does not tell us is which group are significantly different from another. We can conduct a post hoc Tukey test to identify which specific groups differ from another:\n\n\n\nR code\n\n# run post hoc tukey test\ntukey_result &lt;- TukeyHSD(anova_result)\n\n# summary\ntukey_result\n\n\n  Tukey multiple comparisons of means\n    95% family-wise confidence level\n\nFit: aov(formula = active_unemployed ~ borough_name, data = lsoa_subset)\n\n$borough_name\n                                       diff          lwr          upr     p adj\nHammersmith and Fulham-Camden -0.0006520551 -0.004096128  0.002792018 0.8964024\nSutton-Camden                 -0.0158000776 -0.019184199 -0.012415956 0.0000000\nSutton-Hammersmith and Fulham -0.0151480225 -0.018637794 -0.011658251 0.0000000\n\n\nThe output presents pairwise comparisons between the groups, with the null hypothesis stating that the group means are equal. Our analysis indicates significant differences in unemployment rates between Sutton and Camden, as well as between Sutton and Hammersmith and Fulham. However, there is no evidence of a difference in unemployment rates between Camden and Hammersmith and Fulham.\nIf we think our data violates any of the assumption underlying the ANOVA, we can run the non-parametric Kruskal-Wallis test instead.\n\n\n\n\n\n\nThe Kruskal-Wallis test is a non-parametric statistical method used to compare the medians of three or more independent groups. It is an alternative to one-way ANOVA when the assumptions of normality and homogeneity of variances are not met. The test ranks all the data points and evaluates whether the rank sums of the groups differ significantly.\n\n\n\n\n\n\nR code\n\n# run kruskal-wallis\nkruskal.test(active_unemployed ~ borough_name, data = lsoa_subset)\n\n\n\n    Kruskal-Wallis rank sum test\n\ndata:  active_unemployed by borough_name\nKruskal-Wallis chi-squared = 117.37, df = 2, p-value &lt; 2.2e-16\n\n\nThe results of the Kruskal-Wallis test suggest that the differences in medians among these three London boroughs are statistically significant.\n\n\n\n\n\n\nTo further investigate which specific groups differ, we can conduct a series of pairwise Mann-Whitney U tests for comparisons between each pair of boroughs.\n\n\n\n\n\n\n\n\nThis concludes this week’s tutorial. Now complete the following homework tasks making use of both the lsoa_emp and lsoa_age datasets:\n\nUse an appropriate statistical test to determine whether the average proportion of the employed population differs between LSOAs in the London Borough of Bexley and the London Borough of Harrow.\nApply the relevant test to compare the proportion of people aged 50 years and over across the London Boroughs of Lambeth, Southwark, and Westminster.\n\n\n\n\n\n\n\nPaste the test outputs in the appendix of your assignment, include a few sentences interpreting the results.\n\n\n\n\n\n\nThis week, we explored how to statistically compare different groups to assess whether they are different, using both parametric and non-parametric methods. Next week, we will shift our focus from group comparisons to analysing associations and relationships between variables. Looking forward? Good. For now, take some time to unwind and get ready for what is next!"
  },
  {
    "objectID": "02-statistics1.html#lecture-slides",
    "href": "02-statistics1.html#lecture-slides",
    "title": "1 Statistical Analysis I",
    "section": "",
    "text": "You can download the slides of this week’s lecture here: [Link]."
  },
  {
    "objectID": "02-statistics1.html#reading-list",
    "href": "02-statistics1.html#reading-list",
    "title": "1 Statistical Analysis I",
    "section": "",
    "text": "Field, A. Discovering Statistics using R, Chapter 9: Comparing two means, pp. 359-397. [Link]\nField, A. Discovering Statistics using R, Chapter 10: Comparing several means: ANOVA (GLM1), pp. 398-461. [Link]\n\n\n\n\n\nField, A. Discovering Statistics using R, Chapter 15: Non-parametric tests, pp. 653-695. [Link]"
  },
  {
    "objectID": "02-statistics1.html#unemployment-in-london",
    "href": "02-statistics1.html#unemployment-in-london",
    "title": "1 Statistical Analysis I",
    "section": "",
    "text": "In this week’s tutorial, we will explore unemployment rates in London as reported in the 2021 Census. In addition, we will use of last week’s dataset on age groups in London for the homework task. You can download both files using the links provided below. Make sure to save the files in your project folder in the data directory.\n\n\n\nFile\nType\nLink\n\n\n\n\nLondon LSOA Census 2021 Age Groups\ncsv\nDownload\n\n\nLondon LSOA Census 2021 Economic Status\ncsv\nDownload\n\n\n\n\n\n\n\n\n\nTo download a csv file that is hosted on GitHub, click on the Download raw file button on the top right of your screen and it should download directly to your computer.\n\n\n\nTo get started, open your GEOG0018 R Project and create a new script: File -&gt; New File -&gt; R Script. Save your script as w07-employment-age-analysis.r.\nWe will start by loading the libraries that we will need:\n\n\n\nR code\n\n# load libraries\nlibrary(tidyverse)\nlibrary(janitor)\n\n\n\n\nNext, we can load both the London-LSOA-AgeGroup.csv and London-LSOA-EconomicStatus.csv file into R.\n\n\n\nR code\n\n# load age data\nlsoa_age &lt;- read_csv(\"data/London-LSOA-AgeGroup.csv\")\n\n\nRows: 24970 Columns: 5\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (3): Lower layer Super Output Areas Code, Lower layer Super Output Areas...\ndbl (2): Age (5 categories) Code, Observation\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n# load eployment data\nlsoa_emp &lt;- read_csv(\"data/London-LSOA-EconomicStatus.csv\")\n\nRows: 19976 Columns: 5\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (3): Lower layer Super Output Areas Code, Lower layer Super Output Areas...\ndbl (2): Economic activity status (4 categories) Code, Observation\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nBecause we already worked with the lsoa_age dataset last week, we will shift our focus to the lsoa_emp dataset. Now we have loaded the data into R, we will start by inspecting the dataset to understand its structure and the variables it contains:\n\n\n\nR code\n\n# inspect\nhead(lsoa_emp)\n\n\n# A tibble: 6 × 5\n  Lower layer Super Output Areas…¹ Lower layer Super Ou…² Economic activity st…³\n  &lt;chr&gt;                            &lt;chr&gt;                                   &lt;dbl&gt;\n1 E01000001                        City of London 001A                        -8\n2 E01000001                        City of London 001A                         1\n3 E01000001                        City of London 001A                         2\n4 E01000001                        City of London 001A                         3\n5 E01000002                        City of London 001B                        -8\n6 E01000002                        City of London 001B                         1\n# ℹ abbreviated names: ¹​`Lower layer Super Output Areas Code`,\n#   ²​`Lower layer Super Output Areas`,\n#   ³​`Economic activity status (4 categories) Code`\n# ℹ 2 more variables: `Economic activity status (4 categories)` &lt;chr&gt;,\n#   Observation &lt;dbl&gt;\n\n# inspect column names\nnames(lsoa_emp)\n\n[1] \"Lower layer Super Output Areas Code\"         \n[2] \"Lower layer Super Output Areas\"              \n[3] \"Economic activity status (4 categories) Code\"\n[4] \"Economic activity status (4 categories)\"     \n[5] \"Observation\"                                 \n\n# inspect economic status categories\nunique(lsoa_emp$`Economic activity status (4 categories)`)\n\n[1] \"Does not apply\"                                                   \n[2] \"Economically active: In employment (including full-time students)\"\n[3] \"Economically active: Unemployed (including full-time students)\"   \n[4] \"Economically inactive\"                                            \n\n\n\n\n\n\n\n\nThe economic status categories in the lsoa_emp dataset represent the number of individuals and their economic activity status as reported in the 2021 Census. The Office for National Statistics (ONS) provides definitions for each of these statuses in their Census 2021 data dictionary.\n\n\n\n\n\n\n\n\n\nYou can further inspect the dataset using the View() function.\n\n\n\n\n\n\nBecause the lsoa_emp dataset was extracted using the Custom Dataset Tool, we need to convert the data from long to wide format. Like last week, we can do this by pivoting the table:\n\n\n\nR code\n\n# clean names\nlsoa_emp &lt;- lsoa_emp |&gt;\n    clean_names()\n\n# pivot wider\nlsoa_emp &lt;- lsoa_emp |&gt;\n    pivot_wider(id_cols = c(\"lower_layer_super_output_areas_code\", \"lower_layer_super_output_areas\"),\n        names_from = \"economic_activity_status_4_categories\", values_from = \"observation\")\n\n# clean names\nlsoa_emp &lt;- lsoa_emp |&gt;\n    clean_names()\n\n\n\n\n\n\n\n\nIf your clean_names() function returns an error, it is likely due to a conflict with another library that also includes a clean_names() function. In such cases, R cannot determine which one to use. To resolve this, you can specify the library explicitly by using janitor::clean_names().\n\n\n\nThe new column names of the lsoa_emp data are quite lengthy, so let us manually assign more concise column names for ease of use. This will help simplify our code and make it easier to reference the columns in subsequent analyses. Here is how we can do that:\n\n\n\nR code\n\n# names\nnames(lsoa_emp)[3:6] &lt;- c(\"not_applicable\", \"active_employed\", \"active_unemployed\",\n    \"inactive\")\n\n\nTo account for the non-uniformity of the areal units, we further need to convert the observations to proportions again:\n\n\n\nR code\n\n# calculate total population\nlsoa_emp &lt;- lsoa_emp |&gt;\n    rowwise() |&gt;\n    mutate(lsoa_pop = sum(across(3:6)))\n\n# calculate proportions\nlsoa_emp_prop &lt;- lsoa_emp |&gt;\n    mutate(across(3:6, ~./lsoa_pop))\n\n\n\n\n\n\n\n\nYou can further inspect the results using the View() function.\n\n\n\n\n\n\nNow that we have our data prepared, we can begin comparing different groups within it. Various statistical methods exist for this purpose, depending on the nature of the data and the number of groups involved.\nFor comparing two groups with continuous data, we can use the t-test, which assesses whether the means of the two groups are significantly different from each other. If the data does not meet the assumptions of normality, the Mann-Whitney U test serves as a non-parametric alternative, comparing the ranks of the values instead.\n\n\n\n\n\n\nParametric tests assume that the data follows a specific distribution, usually normal, and rely on parameters such as mean and variance. In contrast, non-parametric tests do not assume a particular distribution and are used for ordinal data or when assumptions of parametric tests are violated.\n\n\n\nWhen we want to compare more than two groups, we can use the Analysis of Variance (ANOVA) test. An ANOVA evaluates whether there are any statistically significant differences between the means of multiple groups. If the assumptions of ANOVA are violated, we can use the Kruskal-Wallis test, which is a non-parametric method that compares the ranks across the groups.\n\n\nTwo compare two groups with continuous data, we can run an independent samples t-test, which allows us to statistically assess whether the means of the two groups differ significantly. For instance, we might be interested in whether the proportion of the population that is unemployed in Lower Super Output Areas (LSOAs) in the London Borough of Camden is different, on average, from the proportion of the population that is unemployed in the London Borough of Sutton.\nLet us begin by creating two separate datasets to facilitate our comparison: one containing the LSOAs for Camden and containing the LSOAs for Sutton. We can do this by filtering the lsoa_emp dataset:\n\n\n\nR code\n\n# filter out camden lsoas\nlsoa_emp_camden &lt;- lsoa_emp_prop |&gt;\n    filter(str_detect(lower_layer_super_output_areas, \"Camden\"))\n\n# filter out sutton lsoas\nlsoa_emp_sutton &lt;- lsoa_emp_prop |&gt;\n    filter(str_detect(lower_layer_super_output_areas, \"Sutton\"))\n\n\nWe can use the t.test() function in R to conduct a two-tailed test, assessing whether the mean proportion of unemployed people in Camden differs from that in Sutton The null hypothesis for this test states that there is no difference in the mean proportions of unemployed individuals between the two areas. We can run the test as follows:\n\n\n\nR code\n\n# run t-test\nt.test(lsoa_emp_camden$active_unemployed, lsoa_emp_sutton$active_unemployed)\n\n\n\n    Welch Two Sample t-test\n\ndata:  lsoa_emp_camden$active_unemployed and lsoa_emp_sutton$active_unemployed\nt = 11.279, df = 188.26, p-value &lt; 2.2e-16\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n 0.01303670 0.01856346\nsample estimates:\n mean of x  mean of y \n0.04220920 0.02640912 \n\n\n\n\n\n\n\n\nWe can see that the mean in Sutton LSOAs (2.64%) is lower than in Camden LSOAs (4.22%). Since the \\(p\\)-value is &lt; 0.05 we can conclude reject the null hypothesis that means for Sutton and Camden are the same, and accept the alternative hypothesis.\n\n\n\nThe non-parametric alternative to the t-test is the Mann-Whitney U test (also known as the Wilcoxon rank-sum test). This test is used to compare differences between two independent groups when the assumptions of the t-test (such as normality of the data) are not met.\n\n\n\n\n\n\nThe Mann-Whitney U test evaluates whether the distributions of the two groups differ by ranking all the data points and then comparing the sums of these ranks, making it suitable for ordinal data or non-normally distributed continuous data.\n\n\n\nIf we think our data violates any of the assumption underlying the t-test, we can run the Mann-Whitney U (Wilcoxon rank-sum) test instead:\n\n\n\nR code\n\n# run mann-whitney\nwilcox.test(lsoa_emp_camden$active_unemployed, lsoa_emp_sutton$active_unemployed)\n\n\n\n    Wilcoxon rank sum test with continuity correction\n\ndata:  lsoa_emp_camden$active_unemployed and lsoa_emp_sutton$active_unemployed\nW = 13295, p-value &lt; 2.2e-16\nalternative hypothesis: true location shift is not equal to 0\n\n\nAlso the results of the Mann-Whitney U test suggest that the difference in means in these two London boroughs is statistically significant.\n\n\n\n\n\n\nSince the \\(p\\)-value is &lt; 0.05 we can reject the null hypothesis that means for Sutton and Camden are the same, and accept the alternative hypothesis.\n\n\n\n\n\n\n\n\n\nTo determine if a variable is normally distributed, visual inspection can be helpful, but the Shapiro-Wilk test (shapiro.test()) offers a more formal statistical assessment. This test compares the distribution of your variable to a normal distribution, with the null hypothesis stating that they are the same. If the test returns a small \\(p\\)-value (typically &lt; 0.05), you can conclude that the variable is not normally distributed. However, it is important to note that the test is significantly influenced by sample size; in large datasets, even minor deviations from normality can yield a low \\(p\\)-value. The Shapiro-Wilk test should therefore not be considered definitive and should be used alongside visual assessment of the data.\n\n\n\n\n\n\nIf we want to compare more than two groups, we can use an Analysis of Variance (ANOVA) test. An ANOVA is a statistical method used to compare the means of three or more groups to determine if at least one group mean significantly differs from the others. For instance, if we extend our analysis from Camden and Sutton to include the London Borough of Hammersmith and Fulham, we can test for differences in means among these three areas.\n\n\n\n\n\n\nThe null hypothesis of the ANOVA is that all group means are equal, while the alternative hypothesis posits that at least one group mean is different. ANOVA assesses the variance within groups and between groups to calculate the F-statistic, which is the ratio of the variance between the groups to the variance within the groups. A higher F-statistic indicates that group means are not all the same. The \\(p\\)-value derived from the F-statistic tells us whether we can reject the null hypothesis.\n\n\n\nTo run an ANOVA in R, we first need to ensure that all the groups we are comparing are contained within the same dataframe. We need a dataframe where one column represents the dependent variable (e.g. unemployment rates) and another column represents the independent grouping variable (e.g. borough names).\nWe can do this by filtering our LSOA dataframe to include only those LSOAs that fall within Camden, Sutton, and Hammersmith and Fulham. To create the grouping variable, we need to extract the borough name from the lower_layer_super_output_areas column. We can do this by creating a new variable that excludes the last five characters of the lower_layer_super_output_areas variable.\n\n\n\nR code\n\n# filter\nlsoa_subset &lt;- lsoa_emp_prop |&gt;\n    filter(str_detect(lower_layer_super_output_areas, \"Camden\") | str_detect(lower_layer_super_output_areas,\n        \"Sutton\") | str_detect(lower_layer_super_output_areas, \"Hammersmith and Fulham\"))\n\n# extract borough\nlsoa_subset &lt;- lsoa_subset |&gt;\n    mutate(borough_name = substr(lower_layer_super_output_areas, 1, nchar(lower_layer_super_output_areas) -\n        5))\n\n\n\n\n\n\n\n\nYou can further inspect the results using the View() function.\n\n\n\nWe can now run the ANOVA:\n\n\n\nR code\n\n# run anova\nanova_result &lt;- aov(active_unemployed ~ borough_name, data = lsoa_subset)\n\n# summary\nsummary(anova_result)\n\n\n              Df  Sum Sq  Mean Sq F value Pr(&gt;F)    \nborough_name   2 0.01968 0.009842   75.31 &lt;2e-16 ***\nResiduals    365 0.04770 0.000131                   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\n\n\n\n\nThe ANOVA output shows the F-statistic with a value 75.31, with a \\(p\\)-value &lt; 0.001. We can conclude that there are significant differences in unemployment rates between the boroughs. This means we reject the null hypothesis that all group means are equal.\n\n\n\nWhat this does not tell us is which group are significantly different from another. We can conduct a post hoc Tukey test to identify which specific groups differ from another:\n\n\n\nR code\n\n# run post hoc tukey test\ntukey_result &lt;- TukeyHSD(anova_result)\n\n# summary\ntukey_result\n\n\n  Tukey multiple comparisons of means\n    95% family-wise confidence level\n\nFit: aov(formula = active_unemployed ~ borough_name, data = lsoa_subset)\n\n$borough_name\n                                       diff          lwr          upr     p adj\nHammersmith and Fulham-Camden -0.0006520551 -0.004096128  0.002792018 0.8964024\nSutton-Camden                 -0.0158000776 -0.019184199 -0.012415956 0.0000000\nSutton-Hammersmith and Fulham -0.0151480225 -0.018637794 -0.011658251 0.0000000\n\n\nThe output presents pairwise comparisons between the groups, with the null hypothesis stating that the group means are equal. Our analysis indicates significant differences in unemployment rates between Sutton and Camden, as well as between Sutton and Hammersmith and Fulham. However, there is no evidence of a difference in unemployment rates between Camden and Hammersmith and Fulham.\nIf we think our data violates any of the assumption underlying the ANOVA, we can run the non-parametric Kruskal-Wallis test instead.\n\n\n\n\n\n\nThe Kruskal-Wallis test is a non-parametric statistical method used to compare the medians of three or more independent groups. It is an alternative to one-way ANOVA when the assumptions of normality and homogeneity of variances are not met. The test ranks all the data points and evaluates whether the rank sums of the groups differ significantly.\n\n\n\n\n\n\nR code\n\n# run kruskal-wallis\nkruskal.test(active_unemployed ~ borough_name, data = lsoa_subset)\n\n\n\n    Kruskal-Wallis rank sum test\n\ndata:  active_unemployed by borough_name\nKruskal-Wallis chi-squared = 117.37, df = 2, p-value &lt; 2.2e-16\n\n\nThe results of the Kruskal-Wallis test suggest that the differences in medians among these three London boroughs are statistically significant.\n\n\n\n\n\n\nTo further investigate which specific groups differ, we can conduct a series of pairwise Mann-Whitney U tests for comparisons between each pair of boroughs."
  },
  {
    "objectID": "02-statistics1.html#homework-task",
    "href": "02-statistics1.html#homework-task",
    "title": "1 Statistical Analysis I",
    "section": "",
    "text": "This concludes this week’s tutorial. Now complete the following homework tasks making use of both the lsoa_emp and lsoa_age datasets:\n\nUse an appropriate statistical test to determine whether the average proportion of the employed population differs between LSOAs in the London Borough of Bexley and the London Borough of Harrow.\nApply the relevant test to compare the proportion of people aged 50 years and over across the London Boroughs of Lambeth, Southwark, and Westminster.\n\n\n\n\n\n\n\nPaste the test outputs in the appendix of your assignment, include a few sentences interpreting the results."
  },
  {
    "objectID": "02-statistics1.html#before-you-leave",
    "href": "02-statistics1.html#before-you-leave",
    "title": "1 Statistical Analysis I",
    "section": "",
    "text": "This week, we explored how to statistically compare different groups to assess whether they are different, using both parametric and non-parametric methods. Next week, we will shift our focus from group comparisons to analysing associations and relationships between variables. Looking forward? Good. For now, take some time to unwind and get ready for what is next!"
  },
  {
    "objectID": "04-spatial1.html",
    "href": "04-spatial1.html",
    "title": "1 Spatial Analysis I",
    "section": "",
    "text": "This week’s lecture introduced foundational concepts in GIScience and GIS software, with a focus on how spatial data is represented. Building on these ideas, the practical component allows you to apply this knowledge by creating thematic maps using QGIS.\n\n\nYou can download the slides of this week’s lecture here: [Link].\n\n\n\n\n\n\nLongley, P. et al. 2015. Geographic Information Science & Systems, Chapter 2: The Nature of Geographic Data, pp. 33-54. [Link]\nLongley, P. et al. 2015. Geographic Information Science & Systems, Chapter 3: Representing Geography, pp. 55-76. [Link]\n\n\n\n\n\nYuan, M. 2001. Representing complex geographic phenomena in GIS. Cartography and Geographic Information Science 28(2): 83-96. [Link]\n\n\n\n\n\nQGIS is an open-source graphic user interface GIS with many community developed add-on packages that provide additional functionality to the software. You can download and install QGIS on your personal machine by going to the QGIS website: [Link].\n\n\n\n\n\n\nQGIS is available through Desktop@UCL Anywhere as well as that it can be accessed on UCL computers across campus. Instead of installing the software, you may want to consider using this version.\n\n\n\n\n\n\n\n\n\nIf you want to install QGIS on your personal computer, we recommend installing the Long Term Release (QGIS 3.34 LTR), as this version is the most current and stable. For Windows users, please note that the QGIS installation process may take some time, so be patient during the setup.\n\n\n\nAfter installation, launch QGIS to confirm that the installation was successful.\n\n\n\nIn this tutorial, we will map the population distribution in London using Census estimates from 2011 and 2021. When working with quantitative data, much of the effort goes into data preparation rather than analysis. It is often said that 80% of the time is spent on tasks like finding, retrieving, managing, and processing data. This is because the data you need is rarely in the format required for analysis. Before we can create our maps, we therefore have to go through several data preparation and cleaning steps.\nTo get started, download a copy of the 2011 and 2021 London LSOAs spatial boundaries via the links provided below. Save these files in your project folder under data.\n\n\n\nFile\nType\nLink\n\n\n\n\nLondon LSOA 2011 Spatial Boundaries\nGeoPackage\nDownload\n\n\nLondon LSOA 2021 Spatial Boundaries\nGeoPackage\nDownload\n\n\n\n\n\n\n\n\n\nAdministrative geographies, such as LSOAs, are typically updated as populations change, meaning their boundaries are subject to periodic or occasional adjustments. For this reason, we must use the 2011 LSOA boundaries to map the 2011 Census data and the 2021 LSOA boundaries for the 2021 Census data.\n\n\n\n\n\n\n\n\n\nFor the spatial boundaries of the London LSOAs, you may have noticed that, instead of providing a collection of files known as a shapefile, we have supplied a GeoPackage. While shapefiles remain in use, GeoPackage is a more modern and portable file format. Have a look at this article on towardsdatascience.com for an excellent explanation on why one should use GeoPackage files over shapefiles, where possible: [Link]\n\n\n\nFor our population data, we will use the 2011 and 2021 Census population counts that are made available by the Office for National Statistics through their Nomis portal.\nTo get the 2011 Census population counts, you should:\n\nNavigate to the Nomis portal: [Link]\nClick on Query data in the Data Downloads panel.\nClick on Census 2011 -&gt; Key Statistics.\nClick on KS101EW - Usual resident population.\nSelect Geography and set 2011 super output areas - lower layer to All.\nClick on Download data at the left hand side of the screen.\nOnce the data is ready for download, save it to your computer in your data folder as LSOA2011-population.xlsx.\n\nTo get the 2021 Census population counts, you should:\n\nNavigate to the Nomis portal: [Link]\nClick on Query data in the Data Downloads panel.\nClick on Census 2021 -&gt; Topic Summaries.\nClick on TS007A - Age by five-year age bands.\nSelect Geography and set 2021 super output areas - lower layer to All.\nClick on Download data at the left hand side of the screen.\nOnce the data is ready for download, save it to your computer in your data folder as LSOA2021-population.xlsx.\n\n\n\nWhen opening the LSOA spreadsheets in Excel, you will notice additional rows of information at the top. This extra data can cause issues in QGIS, so we need to clean the dataset. To avoid errors later on, we will also save the cleaned data in csv format.\n\n\nOpen the LSOA2011-population.xlsx file in Excel. We have two main columns: one with identifying information that distinguishes each area from one another and one with population counts. In addition, there are some less informative rows at the top of the spreadsheet. To prepare the data we need to take several steps.\n\nOpen a new, empty Excel spreadsheet.\nFrom the LSOA2011-population.xlsx spreadsheet, select and delete row 1 to 8: highlight the rows, then click on Edit in the menu bar and click on Delete.\n\n\n\n\n\n\n\nNote that the Edit menu bar option might be named differently, depending your version of Excel and/or your operating system.\n\n\n\n\nRename the columns in your spreadsheet to lsoa and pop2011, respectively.\nCut all the data from the pop2011 column that is stored in column B and paste these into column C. You now should have a column without any data sat between the lsoa and pop2011 columns.\nHighlight the entire lsoa column and in the Data menu click on the Text to Columns menu button.\n\n\n\n\n\n\nFigure 1: Highlight the lsoa columns and find the Text to Columns menu option. Note that column B does not contain any data.\n\n\n\n\n\n\n\n\n\n\nNote that the Text to Columns function might be elsewhere in the menu depending on your version of Excel and/or your operating system.\n\n\n\n\nIn the Text to Columns menu, select the Delimited radio button and click Next.\nUncheck the checkbox for Tab and put : (colon) into the Other box. Click Finish.\nChange the name of the lsoa column to lsoa11_code and change the name of the now populated column B to lsoa11_name.\n\n\n\n\n\n\nFigure 2: Your spreadsheet should now look something like this.\n\n\n\n\n\nSelect all three columns, click on Data in the menu bar and. select Sort. In the window that opens, select lsoa11_name as column to sort your data on. Click OK.\nNow we need to identify the LSOAs pertaining to London. Unfortunately, we can only do this manually by searching for the name of each of the London Boroughs and subsequently cutting (Edit -&gt; Cut) the associated rows of data from the spreadsheet and pasting these into the second, empty spreadsheet that you created in Step 1.\nGo to Edit -&gt; Find -&gt; Find. Type in City of London. Cut (Edit -&gt; Cut) the six rows of data (City of London 001A, City of London 001B, etc.) and paste these into the second, empty spreadsheet. Make sure to give your columns appropriate names.\nRepeat this process to find all London LSOAs by searching for the names of all remaining 32 London Boroughs (see Table 1 for a list).\n\n\n\n\n\n\n\nYou can make this process a bit less tedious by using keyboard shortcuts. On MacOS you can use: cmd + f to open up the find menu, cmd + x to cut data, and cmd + v to paste data. On Windows you can use: ctrl + f to open up the find menu, ctrl + x to cut data, and ctrl + v to paste date.\n\n\n\n\n\n\n\n\n\nOnce you are done cutting and pasting, you should have 4,836 rows of data in your second spreadsheet; this count includes your column names.\n\n\n\n\n\nTable 1: The names of all London Boroughs.\n\n\nLondon Boroughs\n\n\n\n\n\nWestminster\nSutton\n\n\nKensington and Chelsea\nCrodyon\n\n\nHammersmith and Fulham\nBromley\n\n\nWandsworth\nLewisham\n\n\nLambeth\nGreenwich\n\n\nSouthwark\nBexley\n\n\nTower Hamlets\nHavering\n\n\nHackney\nBarking and Dagenham\n\n\nIslington\nRedbridge\n\n\nCamden\nNewham\n\n\nBrent\nWaltham Forest\n\n\nEaling\nHaringey\n\n\nHounslow\nEnfield\n\n\nRichmond upon Thames\nBarnet\n\n\nKingston upon Thames\nHarrow\n\n\nMerton\nHillingdon\n\n\n\n\n\nOne potential issue when linking the population data to the spatial data is the presence of trailing spaces in the lsoa11_code column. These spaces are not immediately noticeable, as they appear at the end of the LSOA codes without any visible characters following them. Although this may seem minor, it can cause problems when using the data. To fix this, highlight the entire lsoa11_code column in your new spreadsheet, go to the Home menu, and select Replace under the Find & Select menu.\n\n\n\n\n\n\nFigure 3: Accessing the Replace option.\n\n\n\n\n\nIn the Find what box put in a singular whitespace, using the spacebar on your keyboard, keep the Replace with box empty, and click on Replace all. You should get a message that 4,835 replacements have been made.\n\nBefore saving your data, one final bit of formatting is needed for the population field. Currently, commas are used to separate the thousands in the values. If we leave these commas, QGIS will interpret them as decimal points, resulting in incorrect population figures.\n\nTo format the pop2011 column, highlight the entire column and right-click on the C cell. Click on Format Cells and set the cells to Number with 0 decimal places. You should see that the commas are now removed from your population values.\nSave your spreadsheet as a csv file into your data folder as LSOA2011-population.csv.\n\n\n\n\n\n\n\nAfter saving the file, Excel may prompt a warning about possible data loss. You can safely ignore this message, as it typically relates to lost markup information (e.g. fonts, colours, bold items) or Excel formulas (e.g. means, medians). If prompted, choose to save as CSV UTF-8 (Comma-delimited) (.csv).\n\n\n\n\n\n\n\n\n\nBe aware that depending on your operating system’s language settings (e.g. Windows, macOS, Linux), csv files might use different characters instead of commas. While this may seem trivial, it can lead to issues when importing data into another program. Therefore, it is wise to check your csv file in a plain text editor (e.g. TextEdit on macOS or Notepad on Windows). If you see semicolons (;) instead of commas (,), you can quickly fix this by finding and replacing every semicolon with a comma, just as we did with the whitespace characters in Excel.\n\n\n\n\n\n\nNow the 2011 data is prepared, we can move on to the 2021 data. Open the LSOA2021-population.xlsx in Excel. You will notice that the file is formatted largely the same as the LSOA2011-population.xlsx file. However, this time the data for all Local Authority Districts are grouped together, with all data also grouped by region. This arrangement simplifies our task, as we can easily cut the data for each of the 32 Boroughs and the City of London in one go.\n\nOpen a new Excel spreadsheet.\nFrom the LSOA2021-population.xlsx spreadsheet, cut (Edit -&gt; Cut) all cells from columns A to B and rows 19,790 to 24,783 and paste these into this new spreadsheet.\nTake the remaining steps to prepare the 2021 population steps: split the lsoa column, remove the trailing whitespace characters from the LSOA code column, and remove the decimal commas in the population count column.\nSave the file as csv into your data folder as LSOA2021-population.csv with the following column names: lsoa21_code, lsoa21_name, and pop2021.\n\n\n\n\n\n\n\nAgain make sure you did not miss any LSOAs. You should end up with 4,995 rows of data in this second spreadsheet.\n\n\n\n\n\n\n\nWe will now use QGIS to map our prepared population data. To achieve this, we first need to join our table data to our spatial datasets.\n\nStart QGIS.\nClick on Project -&gt; New. Save your project as w09-population-analysis. Remember to save your work throughout the practical.\nBefore we add our data, we will first set the Coordinate Reference System (CRS) of our Project. Click on Project -&gt; Properties – CRS. In the Filter box, type British National Grid. Select OSGB 1936 / British National Grid - EPSG:27700 and click Apply. Click OK.\n\n\n\n\n\n\n\nA CRS is a framework that defines how spatial data is represented in relation to the Earth’s surface. It includes both a coordinate system (which specifies how points in space are represented using numbers) and a set of parameters that define the relationship between those coordinates and actual locations on the Earth.\n\n\n\nNow the project is set up, we can start by loading our 2011 spatial layer.\n\nClick on Layer -&gt; Add Layer -&gt; Add Vector Layer.\nWith File as your source type, click on the small three dots button and navigate to your data\nHighlight the London-LSOA2011.gpkg file and click Open. Then click Add. You may need to close the box after adding the layer.\n\nNext, we will load our 2011 population csv.\n\nClick on Layer -&gt; Add Layer -&gt; Add Delimited Text Layer.\nClick on the three dots button again and navigate to the London-LSOA-2011.csv file in your data folder. Your file format should be set to csv. You should further have the following boxes ticked under the Record and Field options menu: Decimal separator is comma, First record has field names, Detect field types and Discard empty fields.\nSet the Geometry to No geometry (attribute only table) under the Geometry Definition menu. Then click Add and Close. You should now see a table added to your Layers pane.\n\n\n\n\n\n\nFigure 4: The 2011 London LSOAs.\n\n\n\n\nWe can now join this table data to our spatial data using an attribute join.\n\n\n\n\n\n\nAn atribute join allows you to link two datasets together based on a common attribute that facilitates the matching of rows. To perform an attribute join, you need a single unique identifying field for your records in both datasets. This can be a code, a name, or any other string of information. For the join to work, it is essential that the ID field is consistent across both datasets, meaning there should be no typos or spelling mistakes.\n\n\n\n\n\nFigure 5: Attribute Joins.\n\n\n\n\n\n\n\nBecause both our datasets contain both LSOA names and LSOA codes, we will use the LSOA codes as basis for our join. Unlike names, codes reduce the likelihood of errors and mismatches since they are not dependent on spelling.\n\nOpen up the Attribute Tables of each layer to confirm the columnnames that we can use for the join.\nRight-click on your LSOA2011 spatial layer, click on Properties and then click on the Joins tab.\n\nClick on the + button. Make sure the Join Layer is set to LSOA2011-population.\nSet the Join field to lsoa11_code.\nSet the Target field to lsoa11cd.\nClick the Joined Fields box and click to only select the pop2011 field.\nClick on the Custom Field Name Prefix and remove the pre-entered text to leave it blank.\nClick on OK.\nClick on Apply in the main Join tab and then click OK to return to the main QGIS window.\n\n\nWe can now check to see if our join has worked by opening up the Attribute Table of our LSOA2011 spatial layer. We should see that the spatial layer has a new Population field attached to it.\n\n\n\nWe can now finally map the 2011 population distribution of London.\n\nRight-click on the London-LSOA-2011 layer and click on Properties -&gt; Symbology.\n\nIn the dropdown menu at the top of the window, select Graduated as symbology.\nUnder Value choose pop2011 as your column.\nWe can then change the color ramp to suit our aesthetic preferences. In the Colour ramp dropdown menu select Magma.\nThe final step is to classify our data, which involves deciding how to group the values in our dataset to create a graduated representation. Today we will use the Natural Breaks option. Open the drop-down menu next to Mode, select Natural Breaks, change it to 7 classes and then click Classify.\nFinally click Apply to style your dataset.\n\n\nYou should now be looking at something like this:\n\n\n\n\n\nFigure 6: The population distribution of London in 2011.\n\n\n\n\n\n\n\n\n\n\nWhile the map above is acceptable for today, it is technically incorrect because it displays absolute numbers on a choropleth. You should never do this unless the spatial units are identical in size, as larger areas will draw disproportionate attention and skew the visualisation.\n\n\n\nTo export your map to an image, click on Project -&gt; Import/Export -&gt; Export to Image.\n\n\n\n\nThis concludes this week’s tutorial. Now complete the following homework tasks:\n\nLoad the London-LSOA2021.gpkg into QGIS.\nLoad the LSOA2021-population.csv into QGIS.\nJoin the two datasets together using an Attribute Join.\nStyle your data appropriately.\nExport your map as an image.\n\n\n\n\n\n\n\nPaste the exported map into the appendix of your assignment, include a few sentences interpreting the results. Consider whether there are any noticeable changes in the population distribution between 2011 and 2021.\n\n\n\n\n\n\nThis week we spent a lot of time preparing and cleaning our datasets, followed by a simple visualisation of the population in London in 2011 and 2021. Next week, we will move forward with working in QGIS and conduct some point data aggregation. For today, however, we are done!"
  },
  {
    "objectID": "04-spatial1.html#lecture-slides",
    "href": "04-spatial1.html#lecture-slides",
    "title": "1 Spatial Analysis I",
    "section": "",
    "text": "You can download the slides of this week’s lecture here: [Link]."
  },
  {
    "objectID": "04-spatial1.html#reading-list",
    "href": "04-spatial1.html#reading-list",
    "title": "1 Spatial Analysis I",
    "section": "",
    "text": "Longley, P. et al. 2015. Geographic Information Science & Systems, Chapter 2: The Nature of Geographic Data, pp. 33-54. [Link]\nLongley, P. et al. 2015. Geographic Information Science & Systems, Chapter 3: Representing Geography, pp. 55-76. [Link]\n\n\n\n\n\nYuan, M. 2001. Representing complex geographic phenomena in GIS. Cartography and Geographic Information Science 28(2): 83-96. [Link]"
  },
  {
    "objectID": "04-spatial1.html#installation-of-qgis",
    "href": "04-spatial1.html#installation-of-qgis",
    "title": "1 Spatial Analysis I",
    "section": "",
    "text": "QGIS is an open-source graphic user interface GIS with many community developed add-on packages that provide additional functionality to the software. You can download and install QGIS on your personal machine by going to the QGIS website: [Link].\n\n\n\n\n\n\nQGIS is available through Desktop@UCL Anywhere as well as that it can be accessed on UCL computers across campus. Instead of installing the software, you may want to consider using this version.\n\n\n\n\n\n\n\n\n\nIf you want to install QGIS on your personal computer, we recommend installing the Long Term Release (QGIS 3.34 LTR), as this version is the most current and stable. For Windows users, please note that the QGIS installation process may take some time, so be patient during the setup.\n\n\n\nAfter installation, launch QGIS to confirm that the installation was successful."
  },
  {
    "objectID": "04-spatial1.html#population-in-london",
    "href": "04-spatial1.html#population-in-london",
    "title": "1 Spatial Analysis I",
    "section": "",
    "text": "In this tutorial, we will map the population distribution in London using Census estimates from 2011 and 2021. When working with quantitative data, much of the effort goes into data preparation rather than analysis. It is often said that 80% of the time is spent on tasks like finding, retrieving, managing, and processing data. This is because the data you need is rarely in the format required for analysis. Before we can create our maps, we therefore have to go through several data preparation and cleaning steps.\nTo get started, download a copy of the 2011 and 2021 London LSOAs spatial boundaries via the links provided below. Save these files in your project folder under data.\n\n\n\nFile\nType\nLink\n\n\n\n\nLondon LSOA 2011 Spatial Boundaries\nGeoPackage\nDownload\n\n\nLondon LSOA 2021 Spatial Boundaries\nGeoPackage\nDownload\n\n\n\n\n\n\n\n\n\nAdministrative geographies, such as LSOAs, are typically updated as populations change, meaning their boundaries are subject to periodic or occasional adjustments. For this reason, we must use the 2011 LSOA boundaries to map the 2011 Census data and the 2021 LSOA boundaries for the 2021 Census data.\n\n\n\n\n\n\n\n\n\nFor the spatial boundaries of the London LSOAs, you may have noticed that, instead of providing a collection of files known as a shapefile, we have supplied a GeoPackage. While shapefiles remain in use, GeoPackage is a more modern and portable file format. Have a look at this article on towardsdatascience.com for an excellent explanation on why one should use GeoPackage files over shapefiles, where possible: [Link]\n\n\n\nFor our population data, we will use the 2011 and 2021 Census population counts that are made available by the Office for National Statistics through their Nomis portal.\nTo get the 2011 Census population counts, you should:\n\nNavigate to the Nomis portal: [Link]\nClick on Query data in the Data Downloads panel.\nClick on Census 2011 -&gt; Key Statistics.\nClick on KS101EW - Usual resident population.\nSelect Geography and set 2011 super output areas - lower layer to All.\nClick on Download data at the left hand side of the screen.\nOnce the data is ready for download, save it to your computer in your data folder as LSOA2011-population.xlsx.\n\nTo get the 2021 Census population counts, you should:\n\nNavigate to the Nomis portal: [Link]\nClick on Query data in the Data Downloads panel.\nClick on Census 2021 -&gt; Topic Summaries.\nClick on TS007A - Age by five-year age bands.\nSelect Geography and set 2021 super output areas - lower layer to All.\nClick on Download data at the left hand side of the screen.\nOnce the data is ready for download, save it to your computer in your data folder as LSOA2021-population.xlsx.\n\n\n\nWhen opening the LSOA spreadsheets in Excel, you will notice additional rows of information at the top. This extra data can cause issues in QGIS, so we need to clean the dataset. To avoid errors later on, we will also save the cleaned data in csv format.\n\n\nOpen the LSOA2011-population.xlsx file in Excel. We have two main columns: one with identifying information that distinguishes each area from one another and one with population counts. In addition, there are some less informative rows at the top of the spreadsheet. To prepare the data we need to take several steps.\n\nOpen a new, empty Excel spreadsheet.\nFrom the LSOA2011-population.xlsx spreadsheet, select and delete row 1 to 8: highlight the rows, then click on Edit in the menu bar and click on Delete.\n\n\n\n\n\n\n\nNote that the Edit menu bar option might be named differently, depending your version of Excel and/or your operating system.\n\n\n\n\nRename the columns in your spreadsheet to lsoa and pop2011, respectively.\nCut all the data from the pop2011 column that is stored in column B and paste these into column C. You now should have a column without any data sat between the lsoa and pop2011 columns.\nHighlight the entire lsoa column and in the Data menu click on the Text to Columns menu button.\n\n\n\n\n\n\nFigure 1: Highlight the lsoa columns and find the Text to Columns menu option. Note that column B does not contain any data.\n\n\n\n\n\n\n\n\n\n\nNote that the Text to Columns function might be elsewhere in the menu depending on your version of Excel and/or your operating system.\n\n\n\n\nIn the Text to Columns menu, select the Delimited radio button and click Next.\nUncheck the checkbox for Tab and put : (colon) into the Other box. Click Finish.\nChange the name of the lsoa column to lsoa11_code and change the name of the now populated column B to lsoa11_name.\n\n\n\n\n\n\nFigure 2: Your spreadsheet should now look something like this.\n\n\n\n\n\nSelect all three columns, click on Data in the menu bar and. select Sort. In the window that opens, select lsoa11_name as column to sort your data on. Click OK.\nNow we need to identify the LSOAs pertaining to London. Unfortunately, we can only do this manually by searching for the name of each of the London Boroughs and subsequently cutting (Edit -&gt; Cut) the associated rows of data from the spreadsheet and pasting these into the second, empty spreadsheet that you created in Step 1.\nGo to Edit -&gt; Find -&gt; Find. Type in City of London. Cut (Edit -&gt; Cut) the six rows of data (City of London 001A, City of London 001B, etc.) and paste these into the second, empty spreadsheet. Make sure to give your columns appropriate names.\nRepeat this process to find all London LSOAs by searching for the names of all remaining 32 London Boroughs (see Table 1 for a list).\n\n\n\n\n\n\n\nYou can make this process a bit less tedious by using keyboard shortcuts. On MacOS you can use: cmd + f to open up the find menu, cmd + x to cut data, and cmd + v to paste data. On Windows you can use: ctrl + f to open up the find menu, ctrl + x to cut data, and ctrl + v to paste date.\n\n\n\n\n\n\n\n\n\nOnce you are done cutting and pasting, you should have 4,836 rows of data in your second spreadsheet; this count includes your column names.\n\n\n\n\n\nTable 1: The names of all London Boroughs.\n\n\nLondon Boroughs\n\n\n\n\n\nWestminster\nSutton\n\n\nKensington and Chelsea\nCrodyon\n\n\nHammersmith and Fulham\nBromley\n\n\nWandsworth\nLewisham\n\n\nLambeth\nGreenwich\n\n\nSouthwark\nBexley\n\n\nTower Hamlets\nHavering\n\n\nHackney\nBarking and Dagenham\n\n\nIslington\nRedbridge\n\n\nCamden\nNewham\n\n\nBrent\nWaltham Forest\n\n\nEaling\nHaringey\n\n\nHounslow\nEnfield\n\n\nRichmond upon Thames\nBarnet\n\n\nKingston upon Thames\nHarrow\n\n\nMerton\nHillingdon\n\n\n\n\n\nOne potential issue when linking the population data to the spatial data is the presence of trailing spaces in the lsoa11_code column. These spaces are not immediately noticeable, as they appear at the end of the LSOA codes without any visible characters following them. Although this may seem minor, it can cause problems when using the data. To fix this, highlight the entire lsoa11_code column in your new spreadsheet, go to the Home menu, and select Replace under the Find & Select menu.\n\n\n\n\n\n\nFigure 3: Accessing the Replace option.\n\n\n\n\n\nIn the Find what box put in a singular whitespace, using the spacebar on your keyboard, keep the Replace with box empty, and click on Replace all. You should get a message that 4,835 replacements have been made.\n\nBefore saving your data, one final bit of formatting is needed for the population field. Currently, commas are used to separate the thousands in the values. If we leave these commas, QGIS will interpret them as decimal points, resulting in incorrect population figures.\n\nTo format the pop2011 column, highlight the entire column and right-click on the C cell. Click on Format Cells and set the cells to Number with 0 decimal places. You should see that the commas are now removed from your population values.\nSave your spreadsheet as a csv file into your data folder as LSOA2011-population.csv.\n\n\n\n\n\n\n\nAfter saving the file, Excel may prompt a warning about possible data loss. You can safely ignore this message, as it typically relates to lost markup information (e.g. fonts, colours, bold items) or Excel formulas (e.g. means, medians). If prompted, choose to save as CSV UTF-8 (Comma-delimited) (.csv).\n\n\n\n\n\n\n\n\n\nBe aware that depending on your operating system’s language settings (e.g. Windows, macOS, Linux), csv files might use different characters instead of commas. While this may seem trivial, it can lead to issues when importing data into another program. Therefore, it is wise to check your csv file in a plain text editor (e.g. TextEdit on macOS or Notepad on Windows). If you see semicolons (;) instead of commas (,), you can quickly fix this by finding and replacing every semicolon with a comma, just as we did with the whitespace characters in Excel.\n\n\n\n\n\n\nNow the 2011 data is prepared, we can move on to the 2021 data. Open the LSOA2021-population.xlsx in Excel. You will notice that the file is formatted largely the same as the LSOA2011-population.xlsx file. However, this time the data for all Local Authority Districts are grouped together, with all data also grouped by region. This arrangement simplifies our task, as we can easily cut the data for each of the 32 Boroughs and the City of London in one go.\n\nOpen a new Excel spreadsheet.\nFrom the LSOA2021-population.xlsx spreadsheet, cut (Edit -&gt; Cut) all cells from columns A to B and rows 19,790 to 24,783 and paste these into this new spreadsheet.\nTake the remaining steps to prepare the 2021 population steps: split the lsoa column, remove the trailing whitespace characters from the LSOA code column, and remove the decimal commas in the population count column.\nSave the file as csv into your data folder as LSOA2021-population.csv with the following column names: lsoa21_code, lsoa21_name, and pop2021.\n\n\n\n\n\n\n\nAgain make sure you did not miss any LSOAs. You should end up with 4,995 rows of data in this second spreadsheet.\n\n\n\n\n\n\n\nWe will now use QGIS to map our prepared population data. To achieve this, we first need to join our table data to our spatial datasets.\n\nStart QGIS.\nClick on Project -&gt; New. Save your project as w09-population-analysis. Remember to save your work throughout the practical.\nBefore we add our data, we will first set the Coordinate Reference System (CRS) of our Project. Click on Project -&gt; Properties – CRS. In the Filter box, type British National Grid. Select OSGB 1936 / British National Grid - EPSG:27700 and click Apply. Click OK.\n\n\n\n\n\n\n\nA CRS is a framework that defines how spatial data is represented in relation to the Earth’s surface. It includes both a coordinate system (which specifies how points in space are represented using numbers) and a set of parameters that define the relationship between those coordinates and actual locations on the Earth.\n\n\n\nNow the project is set up, we can start by loading our 2011 spatial layer.\n\nClick on Layer -&gt; Add Layer -&gt; Add Vector Layer.\nWith File as your source type, click on the small three dots button and navigate to your data\nHighlight the London-LSOA2011.gpkg file and click Open. Then click Add. You may need to close the box after adding the layer.\n\nNext, we will load our 2011 population csv.\n\nClick on Layer -&gt; Add Layer -&gt; Add Delimited Text Layer.\nClick on the three dots button again and navigate to the London-LSOA-2011.csv file in your data folder. Your file format should be set to csv. You should further have the following boxes ticked under the Record and Field options menu: Decimal separator is comma, First record has field names, Detect field types and Discard empty fields.\nSet the Geometry to No geometry (attribute only table) under the Geometry Definition menu. Then click Add and Close. You should now see a table added to your Layers pane.\n\n\n\n\n\n\nFigure 4: The 2011 London LSOAs.\n\n\n\n\nWe can now join this table data to our spatial data using an attribute join.\n\n\n\n\n\n\nAn atribute join allows you to link two datasets together based on a common attribute that facilitates the matching of rows. To perform an attribute join, you need a single unique identifying field for your records in both datasets. This can be a code, a name, or any other string of information. For the join to work, it is essential that the ID field is consistent across both datasets, meaning there should be no typos or spelling mistakes.\n\n\n\n\n\nFigure 5: Attribute Joins.\n\n\n\n\n\n\n\nBecause both our datasets contain both LSOA names and LSOA codes, we will use the LSOA codes as basis for our join. Unlike names, codes reduce the likelihood of errors and mismatches since they are not dependent on spelling.\n\nOpen up the Attribute Tables of each layer to confirm the columnnames that we can use for the join.\nRight-click on your LSOA2011 spatial layer, click on Properties and then click on the Joins tab.\n\nClick on the + button. Make sure the Join Layer is set to LSOA2011-population.\nSet the Join field to lsoa11_code.\nSet the Target field to lsoa11cd.\nClick the Joined Fields box and click to only select the pop2011 field.\nClick on the Custom Field Name Prefix and remove the pre-entered text to leave it blank.\nClick on OK.\nClick on Apply in the main Join tab and then click OK to return to the main QGIS window.\n\n\nWe can now check to see if our join has worked by opening up the Attribute Table of our LSOA2011 spatial layer. We should see that the spatial layer has a new Population field attached to it.\n\n\n\nWe can now finally map the 2011 population distribution of London.\n\nRight-click on the London-LSOA-2011 layer and click on Properties -&gt; Symbology.\n\nIn the dropdown menu at the top of the window, select Graduated as symbology.\nUnder Value choose pop2011 as your column.\nWe can then change the color ramp to suit our aesthetic preferences. In the Colour ramp dropdown menu select Magma.\nThe final step is to classify our data, which involves deciding how to group the values in our dataset to create a graduated representation. Today we will use the Natural Breaks option. Open the drop-down menu next to Mode, select Natural Breaks, change it to 7 classes and then click Classify.\nFinally click Apply to style your dataset.\n\n\nYou should now be looking at something like this:\n\n\n\n\n\nFigure 6: The population distribution of London in 2011.\n\n\n\n\n\n\n\n\n\n\nWhile the map above is acceptable for today, it is technically incorrect because it displays absolute numbers on a choropleth. You should never do this unless the spatial units are identical in size, as larger areas will draw disproportionate attention and skew the visualisation.\n\n\n\nTo export your map to an image, click on Project -&gt; Import/Export -&gt; Export to Image."
  },
  {
    "objectID": "04-spatial1.html#homework-task",
    "href": "04-spatial1.html#homework-task",
    "title": "1 Spatial Analysis I",
    "section": "",
    "text": "This concludes this week’s tutorial. Now complete the following homework tasks:\n\nLoad the London-LSOA2021.gpkg into QGIS.\nLoad the LSOA2021-population.csv into QGIS.\nJoin the two datasets together using an Attribute Join.\nStyle your data appropriately.\nExport your map as an image.\n\n\n\n\n\n\n\nPaste the exported map into the appendix of your assignment, include a few sentences interpreting the results. Consider whether there are any noticeable changes in the population distribution between 2011 and 2021."
  },
  {
    "objectID": "04-spatial1.html#before-you-leave",
    "href": "04-spatial1.html#before-you-leave",
    "title": "1 Spatial Analysis I",
    "section": "",
    "text": "This week we spent a lot of time preparing and cleaning our datasets, followed by a simple visualisation of the population in London in 2011 and 2021. Next week, we will move forward with working in QGIS and conduct some point data aggregation. For today, however, we are done!"
  },
  {
    "objectID": "05-spatial2.html",
    "href": "05-spatial2.html",
    "title": "1 Spatial Analysis II",
    "section": "",
    "text": "Building on last week’s introduction to creating thematic maps in QGIS, this week we take it a step further by conducting some basic spatial analysis. Our focus will be on aggregating point event data (such as crime incidents) to administrative geographies and then mapping the aggregated results.\n\n\nYou can download the slides of this week’s lecture here: [Link].\n\n\n\n\n\n\nLongley, P. et al. 2015. Geographic Information Science & Systems, Chapter 4: Georeferencing, pp. 77-98. [Link]\nLongley, P. et al. 2015. Geographic Information Science & Systems, Chapter 11: Cartography and Map Production, pp. 237-252. [Link]\n\n\n\n\n\nLongley, P. et al. 2015. Geographic Information Science & systems, Chapter 12: Geovisualization, pp. 266-289. [Link]\n\n\n\n\n\nIn this week’s practical we will explore the spatial patterns of crime, specifically theft from persons across London boroughs in 2021. For our crime data, we will use data extracted from the Police Data Portal, which provides access to tabular data for crimes recorded by various UK police forces since 2017. You can download the relevant dataset via the link provided below.\nSave the file inside your data folder.\n\n\n\nFile\nType\nLink\n\n\n\n\nTheft from persons in 2021\ncsv\nDownload\n\n\n\n\n\n\n\n\n\nYou may remember that we used this data portal last year during Geography in the Field II.\n\n\n\nIn addition to the crime data, we will also need the spatial boundaries of the London boroughs. To access the spatial boundaries of the London boroughs, you should:\n\nNavigate to the London Datastore: [Link].\nClick on Data in the navigation menu.\nType London Boroughs into the search field.\nDownload the GeoPackage containing the boundaries of each of London’s 33 boroughs.\nRename the file to London-Borough.gpkg and save it in your data folder.\n\n\n\n\n\n\n\nA lot of data about London is collated by the Greater London Authority and made available through the London Datastore. Whereas some of the data is a bit outdated, it is a good place to get some data specific to London.\n\n\n\nWhen studying aggregated point event data, it is important to normalise the data using an appropriate denominator. We therefore also need the population figures for each borough. To get the 2021 borough population counts, you should:\n\nNavigate to the Nomis portal: [Link]\nClick on Query data in the Data Downloads panel.\nClick on Census 2021 -&gt; Topic Summaries.\nClick on TS007A - Age by five-year age bands.\nSelect Geography and set Local authorities: district / unitary (as of April 2023) to Some.\nUnder List areas within select London. Click on Tick all.\nClick on Download data at the left hand side of the screen.\nOnce the data is ready for download, save it to your computer in your data folder as London-Borough-Population.xlsx.\n\n\n\n\n\n\nFigure 1: The downloaded London Borough population dataset.\n\n\n\n\n\n\nTo prepare the borough population data for use in QGIS, you should:\n\nOpen the dataset in your spreadsheet editing software. Locate the columns containing the borough names and their associated population counts. Copy these two columns into a new spreadsheet, and rename the columns to BoroughNames and pop2021 respectively.\nFormat the pop2021 column so that it is recognised as being a numeric column.\nSave the file as a new csv in your data folder as London-Borough-Population.csv.\n\n\n\n\n\n\nFigure 2: The prepared London Borough population dataset.\n\n\n\n\n\n\n\n\n\n\nIf you are struggling with the data preparation of the borough population, refer back to last week’s data cleaning section.\n\n\n\nNow we have our datasets sorted out, we can move to QGIS to analyse our crime dataset.\n\nStart QGIS\nClick on Project -&gt; New. Save your project as w10-crime-analysis. Remember to save your work throughout the practical.\nBefore we get started with adding data, we will first set the Coordinate Reference System of our Project. Click on Project -&gt; Properties – CRS. In the Filter box, type British National Grid. Select OSGB 1936 / British National Grid - EPSG:27700 and click Apply. Click OK.\n\nWe will start by adding the spatial layer containing the London boroughs.\n\nClick on Layer -&gt; Add Layer -&gt; Add Vector Layer.\nWith File select as your source type, click on the small three dots button and navigate to your London-Borough.gpkg file in your data folder. Select the file, then click Add. You may need to close the box after adding the layer.\n\n\n\n\n\n\nFigure 3: The London boroughs.\n\n\n\n\nTo link the spatial data to the population data, we need to join the datasets. Since we covered the steps for joining datasets last week, we will only provide a broad overview of the process:\n\nLoad the London-Borough-Population.csv dataset you just created as a Delimited Text File Layer in QGIS.\nJoin the two datasets together using the Joins tab in the Properties box of the borough layer. Use the BoroughNames field as the unique identifier for the join.\n\n\n\n\n\n\n\nBecause the population file does not include borough codes, we have to use borough names to join the datasets together. While this approach works here (and is easy to manually verify since there are only 33 records), using codes is generally preferred to reduce the risk of errors or incomplete joins. It us often worth taking the time to find these codes, as they ensure more reliable and accurate data linking.\n\n\n\nWe will now load and map our crime data. This will be done using the Delimited Text File Layer option, similar to how you previously loaded the borough population data, but this time we will be adding point coordinates to visualise the crime data as points on the map.\n\nClick on Layer -&gt; Add Layer -&gt; Add Delimited Text File Layer.\n\nClick on the three dots button next to File Name and navigate to your London-Crime-2021 in your data folder.\nYour file format should be set to csv. In Record and Fields Options tick Decimal separator is comma, First record has field names, Detect field types and Discard empty fields.\nUnder Geometry Definition, select Point coordinates and set the X field to Longitude and the Y field to Latitude. The Geometry CRS should be: EPSG:4326 - WGS84. Click Add.\n\n\n\n\n\n\n\n\nAfter clicking Add, you will receive a pop-up in QGIS regarding transformations. Transformations are algorithms used to convert data from one Coordinate Reference System (CRS) to another. In this case, QGIS recognises that the Project CRS is set to the British National Grid, while the Layer you are adding uses the WGS84 CRS. QGIS is asking which transformation it should apply to align the Layer with the Project CRS. One of the key strengths of QGIS is its ability to project data on the fly. This means that once QGIS knows which transformation to use, it will automatically convert all Layers to match the Project CRS, ensuring they are rendered accurately in relation to one another.\n\n\n\n\nClick OK to accept QGIS’ suggested on-the-fly projection. You should now see your crime dataset displayed as points, layered on top of the London borough polygons.\n\n\n\n\n\n\nFigure 4: The London boroughs together with the crime dataset.\n\n\n\n\n\nWe can confirm the temporary nature of the on-the-fly projection by checking the CRS of the London-Crime-2021 layer. To do this, right-click on the layer, then select Properties -&gt; Information. In the information panel, you should see that the CRS of the layer remains as WGS84.\n\nTo ensure that our analysis is both accurate and efficient, it is best to reproject the crime data to the same CRS as our administrative datasets.\n\nBack in the main QGIS window, click on Vector -&gt; Data Management Tools -&gt; Reproject Layer. Fill in the parameters as follows:\n\nInput Layer: London-Crime-2021\nTarget CRS: Project CRS: EPSG: 27700\nReprojected: Click on the three buttons and Save to GeoPackage to create a new data file.\nSave it in your data folder as London-Crime-2021-BNG.gpkg, using London-Crime-2021-BNG as Layer Name.\nClick Run. You should now see the new data layer added to your project. You can now close the Reproject Layer tool.\n\nYou can now also remove the original London-Crime-2021 dataset, only keeping the reprojected version.\n\n\n\n\nThe next step of our analysis involves aggregating the crime events to our administrative geography. We will use the Count Points in Polygons in the Analysis toolset to count how many crimes have occurred in each borough. This will provide us with a count statistic that we can subsequently normalise using population data to create a crime rate statistic.\n\nClick on Vector -&gt; Analysis Tools -&gt; Count Points in Polygons.\nWithin the toolbox, select the parameters as follows:\n\nPolygons: London-Borough\nPoints: London-Crime-2021-BNG [Note how both our data layers state the same CRS.]\nWeight field: leave blank\nClass field: leave blank\nCount field names: theft2021\nClick on the three dot button and select Save to GeoPackage:.\nSave the output as London-Borough-Crime.gpkg, with London-Borough-Crime as Layer Name.\n\nClick Run and Close the dialogue box. You should now see the London-Borough-Crime layer added to your Layers panel\nRight-click on the London-Borough-Crime layer and open the Attribute Table. If all went well, you should see a theft2021 column next to your pop2021 column.\n\nWe can now calculate the crime rate:\n\nWith the Attribute Table of your London-Borough-Crime layer still open, click on the pencil icon in the top left corner to enable Editing mode.\n\n\n\n\n\n\n\nEditing mode allows you to modify both the Attribute Table values and the geometry of your data.\n\n\n\n\nWhilst in the Editing mode, click on New Field button (hotkeys: ctrl + w or cmd + w and fill in the Field Parameters as follows:\n\nName: crime-rate\nComment: leave blank\nType: Decimal number\n\nClick OK. You should now see a new field added to the Attribute Table.\n\nThe new field contains NULL values for each row, so we need to populate the column using the Field Calculator.\n\nWhilst still in the Editing mode, click on the Abacus button (ctrl + i or cmd + i), which is known as the Field Calculator.\nIn the Field Calculator window:\n\nCheck the Update existing field box.\nUse the drop-down to select the crime-rate field.\nIn the Expression editor, add the following expression: ( \"theft2021\" / \"pop2021\" ) * 10000 and click OK.\n\nClick on the Save button to save these edits. Click again on the Pencil button to exit Editing mode.\n\n\n\n\n\n\nFigure 5: The attribute table of the London-Borough-Crime layer after the crime rate calculation.\n\n\n\n\n\n\n\n\n\n\nThe crime-rate is stored as a decimal as this is required for the calculation to succeed, but ultimately you cannot have half a crime. You can convert the decimal values to integers by creating a new field using the Field Calculator. Instead of ticking the Update existing field box you would now keep the Create a new field box ticked. Name the new field crime-rate-int, make sure the Output field type is set to Whole number (integer), and in the Expression editor enter the following expression: to_int(\"crime-rate\"). You can save these changes by clicking the Save button.\n\n\n\n\n\n\nNow the dataset has been prepared, it is time to make a map. First thing we need to do is symbolise the map with an appropriate colour scheme. Go to Properties and symbolise your map with Graduated colours. You need to decide on an appropriate data classification and colour scheme. For reference, Figure 6 shows an example of a possible classification and colour scheme.\n\n\n\n\n\n\nIf you are looking for inspiration beyond the default colour selection, have a look at the online Colorbrewer 2.0 tool.\n\n\n\n\n\n\n\n\nFigure 6: An example of how you can style your crime rate map.\n\n\n\n\nOnce you are happy with the way your borough data looks, we can move to the Print Layout to turn the symbolised map layer into a publishable map.\n\n\n\n\n\n\nThe Print Layout in QGIS is a dedicated workspace that allows users to create high-quality maps for publication and presentation. It provides tools to add and arrange various elements, such as map layers, legends, scale bars, text labels, and images on a blank canvas.\n\n\n\nFrom the main QGIS window, click on Project -&gt; New Print Layout. In the small box that first appears, call your new print layout: crime-map-borough. A new window should open up that shows a blank canvas.\n\n\n\n\n\nFigure 7: The Print Layout workspace in QGIS.\n\n\n\n\nOn the left-hand side of the window, you will find buttons to add various print layout items, including the current QGIS map canvas, text labels, images, legends, scale bars, basic shapes, arrows, attribute tables, and HTML frames. This toolbar also includes buttons for navigation, allowing you to zoom in on specific areas and pan the view within the layout, as well as tools to select and move layout items.\nOn the right-hand side of the window, there are two sets of panels. The upper panel contains the Items and Undo History panels, while the lower panel includes Layout, Item Properties, and Guides. For our purposes today, we will focus on the bottom panel, as the Layout panel controls the overall appearance of our map, while the Item Properties panel allows us to customise elements such as titles and legends that we may add to our map.\n\n\n\n\n\n\nWorking with maps in the Print Layout is relatively straightforward, but it can become a bit fiddly, especially when creating more complex maps. To effectively manage your layout, it is important to understand features like locking items to prevent accidental changes. If you get stuck, have a look at the training manual or the detailed documentation.\n\n\n\nTo begin creating your map, use the Add Map tool to draw a box where a snapshot of the currently active map displayed in your QGIS main window will be loaded.\n\n\n\n\n\n\nWhen you hover your cursor over the icons in the Print Layout toolbox, the names of the tools will appear after about a second.\n\n\n\n\nClick on the Add Map tool and draw a box on your canvas to to load the map layers that are active in the main QGIS window. You can easily move and resize the map box by clicking on it and dragging the corners, just like you would in Word or similar applications.\n\n\n\n\n\n\nFigure 8: Current active map in the Print Layout.\n\n\n\n\n\nWith your map selected, head to the Items Properties panel and look for the Scale parameter.\n\nHere we can manually edit the scale of our map to find the right zoom level.\nHave a go at entering different values and see what level you thin suits the size of your map.\nIf you need to move the position of the map within the box, look for the Move Item Content tool in the left-hand side toolbar.\n\nClick on the Add Legend tool and draw a box on your canvas where you want the legend to appear. You will notice that the legend automatically generates an entry for every layer in the Layers panel of the main QGIS application.\n\nIn the Item Properties, uncheck Auto update to prevent QGIS from automatically populating your legend, allowing you to customise it instead.\nNext, remove all layers from the legend except for the London-Borough-Crime layer by clicking the - (minus) button.\nFinally, change the label of the legend to Crime Rate (per 10,000 people).\nMove the legend to an appropriate position on the canvas - resize if necessary.\n\n\n\n\n\n\n\n\nYou can further customise the legend’s appearance, including the type, size, and alignment of the font.\n\n\n\n\nNow that we have our two maps ready, we can add the main map elements using the tools in the left-hand toolbar:\n\nAdd a scale bar: Use the Item Properties to adjust the style, number of segments, font, and other settings.\nAdd a north arrow: Use the North Arrow tool to draw a box where you want the arrow to appear, then customise its appearance using the Item Properties panel.\nCreate a title at the top of the page using the Add Label tool. You can format the title as needed.\nFinally add a box detailing the data sources that have been used: Contains data from Office for National Statistics licensed under the Open Government Licence v.3.0. Contains Ordnance Survey data © Crown copyright and database right [2024]. Crime data obtained from data.police.uk (Open Government Licence). You can again use the Add Label tool for this.\n\n\nOnce you have added these map elements, you should have a map canvas that could look something like this:\n\n\n\n\n\nFigure 9: Crime rates across London Boroughs.\n\n\n\n\nThe only remaining step is to export our map as an image. Navigate to Layout -&gt; Export as Image and then save it as London-Borough-2021-crime-rate.png.\n\n\n\n\nThis concludes this week’s tutorial. Now complete the following homework tasks:\n\nLoad last week’s London-LSOA2021.gpkg into QGIS.\nAggregate the London-Crime-2021-BNG.gpkg point dataset to the 2021 London LSOAs using the Count Points in Polygons tool.\nCalculate the crime rate per 1000 people using the Field Calculator.\nDecide on an appropriate data classification and colour scheme.\nCreate a publishable map that includes all essential map elements.\nExport your map as an image.\n\n\n\n\n\n\n\nPaste the exported map into the appendix of your assignment, include a few sentences interpreting the results. Consider how different levels of aggregation that we have used affect your perception of theft rates in London.\n\n\n\n\n\n\nThat is us all done. Remember to save your project. Perhaps time to focus on your final assignment?"
  },
  {
    "objectID": "05-spatial2.html#lecture-slides",
    "href": "05-spatial2.html#lecture-slides",
    "title": "1 Spatial Analysis II",
    "section": "",
    "text": "You can download the slides of this week’s lecture here: [Link]."
  },
  {
    "objectID": "05-spatial2.html#reading-list",
    "href": "05-spatial2.html#reading-list",
    "title": "1 Spatial Analysis II",
    "section": "",
    "text": "Longley, P. et al. 2015. Geographic Information Science & Systems, Chapter 4: Georeferencing, pp. 77-98. [Link]\nLongley, P. et al. 2015. Geographic Information Science & Systems, Chapter 11: Cartography and Map Production, pp. 237-252. [Link]\n\n\n\n\n\nLongley, P. et al. 2015. Geographic Information Science & systems, Chapter 12: Geovisualization, pp. 266-289. [Link]"
  },
  {
    "objectID": "05-spatial2.html#crime-in-london",
    "href": "05-spatial2.html#crime-in-london",
    "title": "1 Spatial Analysis II",
    "section": "",
    "text": "In this week’s practical we will explore the spatial patterns of crime, specifically theft from persons across London boroughs in 2021. For our crime data, we will use data extracted from the Police Data Portal, which provides access to tabular data for crimes recorded by various UK police forces since 2017. You can download the relevant dataset via the link provided below.\nSave the file inside your data folder.\n\n\n\nFile\nType\nLink\n\n\n\n\nTheft from persons in 2021\ncsv\nDownload\n\n\n\n\n\n\n\n\n\nYou may remember that we used this data portal last year during Geography in the Field II.\n\n\n\nIn addition to the crime data, we will also need the spatial boundaries of the London boroughs. To access the spatial boundaries of the London boroughs, you should:\n\nNavigate to the London Datastore: [Link].\nClick on Data in the navigation menu.\nType London Boroughs into the search field.\nDownload the GeoPackage containing the boundaries of each of London’s 33 boroughs.\nRename the file to London-Borough.gpkg and save it in your data folder.\n\n\n\n\n\n\n\nA lot of data about London is collated by the Greater London Authority and made available through the London Datastore. Whereas some of the data is a bit outdated, it is a good place to get some data specific to London.\n\n\n\nWhen studying aggregated point event data, it is important to normalise the data using an appropriate denominator. We therefore also need the population figures for each borough. To get the 2021 borough population counts, you should:\n\nNavigate to the Nomis portal: [Link]\nClick on Query data in the Data Downloads panel.\nClick on Census 2021 -&gt; Topic Summaries.\nClick on TS007A - Age by five-year age bands.\nSelect Geography and set Local authorities: district / unitary (as of April 2023) to Some.\nUnder List areas within select London. Click on Tick all.\nClick on Download data at the left hand side of the screen.\nOnce the data is ready for download, save it to your computer in your data folder as London-Borough-Population.xlsx.\n\n\n\n\n\n\nFigure 1: The downloaded London Borough population dataset.\n\n\n\n\n\n\nTo prepare the borough population data for use in QGIS, you should:\n\nOpen the dataset in your spreadsheet editing software. Locate the columns containing the borough names and their associated population counts. Copy these two columns into a new spreadsheet, and rename the columns to BoroughNames and pop2021 respectively.\nFormat the pop2021 column so that it is recognised as being a numeric column.\nSave the file as a new csv in your data folder as London-Borough-Population.csv.\n\n\n\n\n\n\nFigure 2: The prepared London Borough population dataset.\n\n\n\n\n\n\n\n\n\n\nIf you are struggling with the data preparation of the borough population, refer back to last week’s data cleaning section.\n\n\n\nNow we have our datasets sorted out, we can move to QGIS to analyse our crime dataset.\n\nStart QGIS\nClick on Project -&gt; New. Save your project as w10-crime-analysis. Remember to save your work throughout the practical.\nBefore we get started with adding data, we will first set the Coordinate Reference System of our Project. Click on Project -&gt; Properties – CRS. In the Filter box, type British National Grid. Select OSGB 1936 / British National Grid - EPSG:27700 and click Apply. Click OK.\n\nWe will start by adding the spatial layer containing the London boroughs.\n\nClick on Layer -&gt; Add Layer -&gt; Add Vector Layer.\nWith File select as your source type, click on the small three dots button and navigate to your London-Borough.gpkg file in your data folder. Select the file, then click Add. You may need to close the box after adding the layer.\n\n\n\n\n\n\nFigure 3: The London boroughs.\n\n\n\n\nTo link the spatial data to the population data, we need to join the datasets. Since we covered the steps for joining datasets last week, we will only provide a broad overview of the process:\n\nLoad the London-Borough-Population.csv dataset you just created as a Delimited Text File Layer in QGIS.\nJoin the two datasets together using the Joins tab in the Properties box of the borough layer. Use the BoroughNames field as the unique identifier for the join.\n\n\n\n\n\n\n\nBecause the population file does not include borough codes, we have to use borough names to join the datasets together. While this approach works here (and is easy to manually verify since there are only 33 records), using codes is generally preferred to reduce the risk of errors or incomplete joins. It us often worth taking the time to find these codes, as they ensure more reliable and accurate data linking.\n\n\n\nWe will now load and map our crime data. This will be done using the Delimited Text File Layer option, similar to how you previously loaded the borough population data, but this time we will be adding point coordinates to visualise the crime data as points on the map.\n\nClick on Layer -&gt; Add Layer -&gt; Add Delimited Text File Layer.\n\nClick on the three dots button next to File Name and navigate to your London-Crime-2021 in your data folder.\nYour file format should be set to csv. In Record and Fields Options tick Decimal separator is comma, First record has field names, Detect field types and Discard empty fields.\nUnder Geometry Definition, select Point coordinates and set the X field to Longitude and the Y field to Latitude. The Geometry CRS should be: EPSG:4326 - WGS84. Click Add.\n\n\n\n\n\n\n\n\nAfter clicking Add, you will receive a pop-up in QGIS regarding transformations. Transformations are algorithms used to convert data from one Coordinate Reference System (CRS) to another. In this case, QGIS recognises that the Project CRS is set to the British National Grid, while the Layer you are adding uses the WGS84 CRS. QGIS is asking which transformation it should apply to align the Layer with the Project CRS. One of the key strengths of QGIS is its ability to project data on the fly. This means that once QGIS knows which transformation to use, it will automatically convert all Layers to match the Project CRS, ensuring they are rendered accurately in relation to one another.\n\n\n\n\nClick OK to accept QGIS’ suggested on-the-fly projection. You should now see your crime dataset displayed as points, layered on top of the London borough polygons.\n\n\n\n\n\n\nFigure 4: The London boroughs together with the crime dataset.\n\n\n\n\n\nWe can confirm the temporary nature of the on-the-fly projection by checking the CRS of the London-Crime-2021 layer. To do this, right-click on the layer, then select Properties -&gt; Information. In the information panel, you should see that the CRS of the layer remains as WGS84.\n\nTo ensure that our analysis is both accurate and efficient, it is best to reproject the crime data to the same CRS as our administrative datasets.\n\nBack in the main QGIS window, click on Vector -&gt; Data Management Tools -&gt; Reproject Layer. Fill in the parameters as follows:\n\nInput Layer: London-Crime-2021\nTarget CRS: Project CRS: EPSG: 27700\nReprojected: Click on the three buttons and Save to GeoPackage to create a new data file.\nSave it in your data folder as London-Crime-2021-BNG.gpkg, using London-Crime-2021-BNG as Layer Name.\nClick Run. You should now see the new data layer added to your project. You can now close the Reproject Layer tool.\n\nYou can now also remove the original London-Crime-2021 dataset, only keeping the reprojected version.\n\n\n\n\nThe next step of our analysis involves aggregating the crime events to our administrative geography. We will use the Count Points in Polygons in the Analysis toolset to count how many crimes have occurred in each borough. This will provide us with a count statistic that we can subsequently normalise using population data to create a crime rate statistic.\n\nClick on Vector -&gt; Analysis Tools -&gt; Count Points in Polygons.\nWithin the toolbox, select the parameters as follows:\n\nPolygons: London-Borough\nPoints: London-Crime-2021-BNG [Note how both our data layers state the same CRS.]\nWeight field: leave blank\nClass field: leave blank\nCount field names: theft2021\nClick on the three dot button and select Save to GeoPackage:.\nSave the output as London-Borough-Crime.gpkg, with London-Borough-Crime as Layer Name.\n\nClick Run and Close the dialogue box. You should now see the London-Borough-Crime layer added to your Layers panel\nRight-click on the London-Borough-Crime layer and open the Attribute Table. If all went well, you should see a theft2021 column next to your pop2021 column.\n\nWe can now calculate the crime rate:\n\nWith the Attribute Table of your London-Borough-Crime layer still open, click on the pencil icon in the top left corner to enable Editing mode.\n\n\n\n\n\n\n\nEditing mode allows you to modify both the Attribute Table values and the geometry of your data.\n\n\n\n\nWhilst in the Editing mode, click on New Field button (hotkeys: ctrl + w or cmd + w and fill in the Field Parameters as follows:\n\nName: crime-rate\nComment: leave blank\nType: Decimal number\n\nClick OK. You should now see a new field added to the Attribute Table.\n\nThe new field contains NULL values for each row, so we need to populate the column using the Field Calculator.\n\nWhilst still in the Editing mode, click on the Abacus button (ctrl + i or cmd + i), which is known as the Field Calculator.\nIn the Field Calculator window:\n\nCheck the Update existing field box.\nUse the drop-down to select the crime-rate field.\nIn the Expression editor, add the following expression: ( \"theft2021\" / \"pop2021\" ) * 10000 and click OK.\n\nClick on the Save button to save these edits. Click again on the Pencil button to exit Editing mode.\n\n\n\n\n\n\nFigure 5: The attribute table of the London-Borough-Crime layer after the crime rate calculation.\n\n\n\n\n\n\n\n\n\n\nThe crime-rate is stored as a decimal as this is required for the calculation to succeed, but ultimately you cannot have half a crime. You can convert the decimal values to integers by creating a new field using the Field Calculator. Instead of ticking the Update existing field box you would now keep the Create a new field box ticked. Name the new field crime-rate-int, make sure the Output field type is set to Whole number (integer), and in the Expression editor enter the following expression: to_int(\"crime-rate\"). You can save these changes by clicking the Save button.\n\n\n\n\n\n\nNow the dataset has been prepared, it is time to make a map. First thing we need to do is symbolise the map with an appropriate colour scheme. Go to Properties and symbolise your map with Graduated colours. You need to decide on an appropriate data classification and colour scheme. For reference, Figure 6 shows an example of a possible classification and colour scheme.\n\n\n\n\n\n\nIf you are looking for inspiration beyond the default colour selection, have a look at the online Colorbrewer 2.0 tool.\n\n\n\n\n\n\n\n\nFigure 6: An example of how you can style your crime rate map.\n\n\n\n\nOnce you are happy with the way your borough data looks, we can move to the Print Layout to turn the symbolised map layer into a publishable map.\n\n\n\n\n\n\nThe Print Layout in QGIS is a dedicated workspace that allows users to create high-quality maps for publication and presentation. It provides tools to add and arrange various elements, such as map layers, legends, scale bars, text labels, and images on a blank canvas.\n\n\n\nFrom the main QGIS window, click on Project -&gt; New Print Layout. In the small box that first appears, call your new print layout: crime-map-borough. A new window should open up that shows a blank canvas.\n\n\n\n\n\nFigure 7: The Print Layout workspace in QGIS.\n\n\n\n\nOn the left-hand side of the window, you will find buttons to add various print layout items, including the current QGIS map canvas, text labels, images, legends, scale bars, basic shapes, arrows, attribute tables, and HTML frames. This toolbar also includes buttons for navigation, allowing you to zoom in on specific areas and pan the view within the layout, as well as tools to select and move layout items.\nOn the right-hand side of the window, there are two sets of panels. The upper panel contains the Items and Undo History panels, while the lower panel includes Layout, Item Properties, and Guides. For our purposes today, we will focus on the bottom panel, as the Layout panel controls the overall appearance of our map, while the Item Properties panel allows us to customise elements such as titles and legends that we may add to our map.\n\n\n\n\n\n\nWorking with maps in the Print Layout is relatively straightforward, but it can become a bit fiddly, especially when creating more complex maps. To effectively manage your layout, it is important to understand features like locking items to prevent accidental changes. If you get stuck, have a look at the training manual or the detailed documentation.\n\n\n\nTo begin creating your map, use the Add Map tool to draw a box where a snapshot of the currently active map displayed in your QGIS main window will be loaded.\n\n\n\n\n\n\nWhen you hover your cursor over the icons in the Print Layout toolbox, the names of the tools will appear after about a second.\n\n\n\n\nClick on the Add Map tool and draw a box on your canvas to to load the map layers that are active in the main QGIS window. You can easily move and resize the map box by clicking on it and dragging the corners, just like you would in Word or similar applications.\n\n\n\n\n\n\nFigure 8: Current active map in the Print Layout.\n\n\n\n\n\nWith your map selected, head to the Items Properties panel and look for the Scale parameter.\n\nHere we can manually edit the scale of our map to find the right zoom level.\nHave a go at entering different values and see what level you thin suits the size of your map.\nIf you need to move the position of the map within the box, look for the Move Item Content tool in the left-hand side toolbar.\n\nClick on the Add Legend tool and draw a box on your canvas where you want the legend to appear. You will notice that the legend automatically generates an entry for every layer in the Layers panel of the main QGIS application.\n\nIn the Item Properties, uncheck Auto update to prevent QGIS from automatically populating your legend, allowing you to customise it instead.\nNext, remove all layers from the legend except for the London-Borough-Crime layer by clicking the - (minus) button.\nFinally, change the label of the legend to Crime Rate (per 10,000 people).\nMove the legend to an appropriate position on the canvas - resize if necessary.\n\n\n\n\n\n\n\n\nYou can further customise the legend’s appearance, including the type, size, and alignment of the font.\n\n\n\n\nNow that we have our two maps ready, we can add the main map elements using the tools in the left-hand toolbar:\n\nAdd a scale bar: Use the Item Properties to adjust the style, number of segments, font, and other settings.\nAdd a north arrow: Use the North Arrow tool to draw a box where you want the arrow to appear, then customise its appearance using the Item Properties panel.\nCreate a title at the top of the page using the Add Label tool. You can format the title as needed.\nFinally add a box detailing the data sources that have been used: Contains data from Office for National Statistics licensed under the Open Government Licence v.3.0. Contains Ordnance Survey data © Crown copyright and database right [2024]. Crime data obtained from data.police.uk (Open Government Licence). You can again use the Add Label tool for this.\n\n\nOnce you have added these map elements, you should have a map canvas that could look something like this:\n\n\n\n\n\nFigure 9: Crime rates across London Boroughs.\n\n\n\n\nThe only remaining step is to export our map as an image. Navigate to Layout -&gt; Export as Image and then save it as London-Borough-2021-crime-rate.png."
  },
  {
    "objectID": "05-spatial2.html#homework-task",
    "href": "05-spatial2.html#homework-task",
    "title": "1 Spatial Analysis II",
    "section": "",
    "text": "This concludes this week’s tutorial. Now complete the following homework tasks:\n\nLoad last week’s London-LSOA2021.gpkg into QGIS.\nAggregate the London-Crime-2021-BNG.gpkg point dataset to the 2021 London LSOAs using the Count Points in Polygons tool.\nCalculate the crime rate per 1000 people using the Field Calculator.\nDecide on an appropriate data classification and colour scheme.\nCreate a publishable map that includes all essential map elements.\nExport your map as an image.\n\n\n\n\n\n\n\nPaste the exported map into the appendix of your assignment, include a few sentences interpreting the results. Consider how different levels of aggregation that we have used affect your perception of theft rates in London."
  },
  {
    "objectID": "05-spatial2.html#before-you-leave",
    "href": "05-spatial2.html#before-you-leave",
    "title": "1 Spatial Analysis II",
    "section": "",
    "text": "That is us all done. Remember to save your project. Perhaps time to focus on your final assignment?"
  }
]